[
  {
    "objectID": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html",
    "href": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html",
    "title": "Running llama.cpp on a laptop and phone",
    "section": "",
    "text": "Running LLMs with llama.cpp."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html#llama.cpp",
    "href": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html#llama.cpp",
    "title": "Running llama.cpp on a laptop and phone",
    "section": "llama.cpp",
    "text": "llama.cpp\nllama.cpp is focused on quantizing and deploying LLMs on Mac, but they support linux and windows as well. Despite its name, the project supports a ton of models beyond Llama and Llama-2. It is very straightforward to install. And there are even python bindings to make our lives easier.\n\nThe original project was stitched together in a single evening, and has since become the arguable SOTA for deploying LLMs on CPUs. The llama.cpp community is full of is incredibly helpful and responsive people.\n\nThere benefits of the repo go beyond even the code and models. The community is always bringing in the greatest and latest approaches from the avalanche of LLM progress. The Pull Requests and Issues are full of folks working and experimenting to integrate these advances. Thankfully, the community is very open to indie hackers and unconventional ideas: if something works and thereâ€™s proof, itâ€™s merged in.\nIn fact, this notebook is living proof of the power of Open Source. Last lesson we used the MLC library because it was the only one that supported LLMs on iOS. But, literally since last week, llama.cpp added and fixed support for iOS LLMs. MLC is a great project and worth keeping an eye on, but for now we march on under the llama banner."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html#running-mistral-v0.1-with-llama.cpp",
    "href": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html#running-mistral-v0.1-with-llama.cpp",
    "title": "Running llama.cpp on a laptop and phone",
    "section": "Running Mistral-v0.1 with llama.cpp",
    "text": "Running Mistral-v0.1 with llama.cpp\nThis section covers the following:\n- Setting up and installing the llama.cpp repo.\n- Downloading a Mistral-v0.1 model.\n- Running the Mistral model directly with llama.cpp\n- Running the model with the python bindings\nFirst we create an environment for llama.cpp, then we download and install the repo.\nThen we download a Mistral-v0.1 model thatâ€™s already been quantized into the special GGUF format that llama.cpp expects.\nLastly, we run the model on a sample input.\n\nInstalling llama.cpp\nFirst, create a new environment for llama.cpp. Then install the dependencies.\n# create an environment for llama.cpp\nmamba create -n llama-cpp python=3.11\nThis isnâ€™t strictly necessary for llama.cpp since it uses C++, but weâ€™ll need it for the python bindings. Even without those, itâ€™s good practice to make and keep isolated environments for your projects.\nNext activate the environment:\n# activate the environment\nmamba activate llama-cpp\nWe can now download and move into the repo:\n# clone and move into the llama.cpp repo\ngit clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nLlama supports both the GNU Make and CMake build systems. Note that make works fine on Linux, but Iâ€™ve had mixed results on Mac. For that reason weâ€™ll stick with CMake here. On Mac you can install cmake with homebrew: brew install cmake. On linux you can install it with sudo apt install cmake. For Windows folks you can grab the latest installer from here.\nOnce you installed cmake and are inside the repo, go ahead and build it. The steps below are a standard build process with cmake, youâ€™ll see something similar in many different projects.\nWe can also pass a flag to build Llama with optimizations for certain hardware. Note that cmake build arguments have a funky syntax where theyâ€™re prefixed by -D and then the argument name.\nHere are the flags for different hardware:\n- To build for Macâ€™s Metal acceleration: -DLLAMA_METAL=1\n- To build for NVIDIA GPUs: -DLLAMA_CUBLAS=1\n# prepare for a cmake build\nmkdir build\ncd build\n\n# prepare the build for Metal acceleration\ncmake -DLLAMA_METAL=1 ..\n# # or, replace with this to build for NVIDIA GPUs\n# cmake -DLLAMA_CUBLAS=1 ..\n\n# build the accelerated llama.cpp project\ncmake --build . --config Release  \nThe accelerated files are inside the build/ folder itself. It has the main way weâ€™ll be calling the llama library: main.\nWith llama.cpp installed, we can now download a Mistral-v0.1 model and run it.\n\n\nDownloading a Mistral-v0.1 model\nAs mentioned earlier, weâ€™ll grab a model thatâ€™s already been set up to work with llama.cpp. Weâ€™ll rely on a model uploaded by a user known as TheBloke. His efforts are another example for the power of Open Source: this user has uploaded almost 2,000 models that anyone can now use!\nHere is the Mistral-v0.1 model already formatted for llama.cpp. The model has 7 billion parameters, and is the Instruct version which was explicitly trained to follow and complete instructions.\n\nLink to Mistral-7B-Instruct-v0.1\n\nClick the link above, then navigate to the Files and version tabs. Youâ€™ll see a list of available models, each one quantized in a different way:\n\nThe Q4 models will more comfortably fit on consumer GPUs and hardware. But the higher number after the Q the more accurate the model will be. Here weâ€™ll use the Q5_K_M model which is a bit larger and slower than the Q4 models but makes up for it in performance.\nLetâ€™s grab the Q5_K_M model. First, make sure the huggingface-hub CLI is installed:\n# install a tool to download HuggingFace models via the terminal\npip install huggingface-hub\nThen move into the models/ folder and download the model:\n# download the Mistral model\nhuggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF mistral-7b-instruct-v0.1.Q5_K_M.gguf --local-dir . --local-dir-use-symlinks False\nNow that we have the model, we can run it to make sure everything works.\n\n\nRunning the Mistral model\nWeâ€™ll use the main binary inside of the build/ folder to run it. Go ahead and run the following command to see Mistral-v0.1 in action!\n# run the inference on the official example from the build/\n./bin/main -m ../models/mistral-7b-instruct-v0.1.Q5_K_M.gguf -n 128 -p \"Building a website can be done in 10 simple steps:\\nStep 1:\"\nAnd thatâ€™s it! Weâ€™ve now done the following:\n- Installed llama.cpp.\n- Downloaded a Mistral-v0.1 model.\n- Ran the model on a sample input.\nHowever, this was all via the command line using C++ binaries. How could we do the same with python and Notebooks? Enter the python bindings."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html#running-llama.cpp-model-with-python",
    "href": "blog/posts/fractal-llms/2023-10-14-Session_3_llama/index.html#running-llama.cpp-model-with-python",
    "title": "Running llama.cpp on a laptop and phone",
    "section": "Running llama.cpp model with python",
    "text": "Running llama.cpp model with python\nFirst step, letâ€™s install the llama.cpp python bindings into our environment. Weâ€™ll repeat ourselves a bit here, since the python bindings attempt to build and install the C++ repo as well.\nHere are the instructions for the full Mac Metal installation\nThe TL;DR is to run the following command inside our environment:\npip uninstall llama-cpp-python -y\nCMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install -U llama-cpp-python --no-cache-dir\nThe command above does the following to make sure the python bindings are up to date:\n- Uninstall older versions of the bindings, if any are installed.\n- Install the bindings with Metal acceleration.\nThe following code snippet checks if we can import the bindings:\npython -c \"from llama_cpp import Llama\"\n\nfrom llama_cpp import Llama\n\nIf the command above worked, we can use it to run the Mistral-v0.1 model.\nPut the following into a llama_python.py file, or run it from the notebook:\n\nfrom llama_cpp import Llama\n\n# point to the model, assuming we're in the llama.cpp build/ folder\nwork_dir = \"/Users/cck/repos/llama.cpp/\"\nllm = Llama(f\"{work_dir}/models/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n\nllama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /Users/cck/repos/llama.cpp//models/mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2 (latest))\nllama_model_loader: - tensor    0:                token_embd.weight q5_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - tensor    1:              blk.0.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    2:              blk.0.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor    3:              blk.0.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor    4:         blk.0.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor    5:            blk.0.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor    6:              blk.0.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor    7:            blk.0.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor    8:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor    9:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   10:              blk.1.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   11:              blk.1.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   12:              blk.1.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   13:         blk.1.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   14:            blk.1.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   15:              blk.1.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   16:            blk.1.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   17:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   18:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   19:              blk.2.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   20:              blk.2.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   21:              blk.2.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   22:         blk.2.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   23:            blk.2.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   24:              blk.2.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   25:            blk.2.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   26:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   27:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   28:              blk.3.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   29:              blk.3.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   30:              blk.3.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   31:         blk.3.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   32:            blk.3.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   33:              blk.3.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   34:            blk.3.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   35:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   37:              blk.4.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   38:              blk.4.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   39:              blk.4.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   40:         blk.4.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   41:            blk.4.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   42:              blk.4.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   43:            blk.4.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   44:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   45:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   46:              blk.5.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   47:              blk.5.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   48:              blk.5.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   49:         blk.5.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   50:            blk.5.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   51:              blk.5.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   52:            blk.5.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   53:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   54:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   55:              blk.6.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   56:              blk.6.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   57:              blk.6.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   58:         blk.6.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   59:            blk.6.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   60:              blk.6.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   61:            blk.6.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   62:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   63:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   64:              blk.7.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   65:              blk.7.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   66:              blk.7.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   67:         blk.7.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   68:            blk.7.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   69:              blk.7.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   70:            blk.7.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   71:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   72:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   73:              blk.8.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   74:              blk.8.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   75:              blk.8.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   76:         blk.8.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   77:            blk.8.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   78:              blk.8.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   79:            blk.8.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   80:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   81:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   82:              blk.9.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   83:              blk.9.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   84:              blk.9.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   85:         blk.9.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   86:            blk.9.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   87:              blk.9.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   88:            blk.9.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   89:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   90:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   91:             blk.10.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   92:             blk.10.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   93:             blk.10.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor   94:        blk.10.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor   95:           blk.10.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   96:             blk.10.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor   97:           blk.10.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor   98:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor   99:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  100:             blk.11.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  101:             blk.11.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  102:             blk.11.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  103:        blk.11.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  104:           blk.11.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  105:             blk.11.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  106:           blk.11.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  107:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  108:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  109:             blk.12.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  110:             blk.12.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  111:             blk.12.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  112:        blk.12.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  113:           blk.12.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  114:             blk.12.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  115:           blk.12.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  116:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  118:             blk.13.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  119:             blk.13.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  120:             blk.13.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  121:        blk.13.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  122:           blk.13.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  123:             blk.13.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  124:           blk.13.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  125:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  126:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  127:             blk.14.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  128:             blk.14.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  129:             blk.14.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  130:        blk.14.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  131:           blk.14.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  132:             blk.14.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  133:           blk.14.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  134:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  135:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  136:             blk.15.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  137:             blk.15.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  138:             blk.15.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  139:        blk.15.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  140:           blk.15.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  141:             blk.15.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  142:           blk.15.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  143:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  144:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  145:             blk.16.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  146:             blk.16.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  147:             blk.16.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  148:        blk.16.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  149:           blk.16.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  150:             blk.16.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  151:           blk.16.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  152:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  153:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  154:             blk.17.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  155:             blk.17.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  156:             blk.17.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  157:        blk.17.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  158:           blk.17.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  159:             blk.17.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  160:           blk.17.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  161:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  163:             blk.18.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  164:             blk.18.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  165:             blk.18.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  166:        blk.18.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  167:           blk.18.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  168:             blk.18.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  169:           blk.18.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  170:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  171:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  172:             blk.19.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  173:             blk.19.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  174:             blk.19.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  175:        blk.19.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  176:           blk.19.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  177:             blk.19.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  178:           blk.19.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  179:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  180:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  181:             blk.20.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  182:             blk.20.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  183:             blk.20.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  184:        blk.20.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  185:           blk.20.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  186:             blk.20.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  187:           blk.20.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  188:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  189:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  190:             blk.21.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  191:             blk.21.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  192:             blk.21.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  193:        blk.21.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  194:           blk.21.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  195:             blk.21.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  196:           blk.21.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  197:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  199:             blk.22.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  200:             blk.22.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  201:             blk.22.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  202:        blk.22.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  203:           blk.22.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  204:             blk.22.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  205:           blk.22.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  206:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  207:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  208:             blk.23.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  209:             blk.23.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  210:             blk.23.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  211:        blk.23.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  212:           blk.23.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  213:             blk.23.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  214:           blk.23.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  215:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  216:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  217:             blk.24.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  218:             blk.24.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  219:             blk.24.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  220:        blk.24.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  221:           blk.24.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  222:             blk.24.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  223:           blk.24.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  224:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  225:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  226:             blk.25.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  227:             blk.25.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  228:             blk.25.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  229:        blk.25.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  230:           blk.25.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  231:             blk.25.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  232:           blk.25.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  233:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  234:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  235:             blk.26.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  236:             blk.26.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  237:             blk.26.attn_v.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  238:        blk.26.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  239:           blk.26.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  240:             blk.26.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  241:           blk.26.ffn_down.weight q5_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  242:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  244:             blk.27.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  245:             blk.27.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  246:             blk.27.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  247:        blk.27.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  248:           blk.27.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  249:             blk.27.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  250:           blk.27.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  251:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  252:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  253:             blk.28.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  254:             blk.28.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  255:             blk.28.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  256:        blk.28.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  257:           blk.28.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  258:             blk.28.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  259:           blk.28.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  260:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  261:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  262:             blk.29.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  263:             blk.29.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  264:             blk.29.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  265:        blk.29.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  266:           blk.29.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  267:             blk.29.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  268:           blk.29.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  269:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  270:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  271:             blk.30.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  272:             blk.30.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  273:             blk.30.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  274:        blk.30.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  275:           blk.30.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  276:             blk.30.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  277:           blk.30.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  279:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  280:             blk.31.attn_q.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  281:             blk.31.attn_k.weight q5_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  282:             blk.31.attn_v.weight q6_K     [  4096,  1024,     1,     1 ]\nllama_model_loader: - tensor  283:        blk.31.attn_output.weight q5_K     [  4096,  4096,     1,     1 ]\nllama_model_loader: - tensor  284:           blk.31.ffn_gate.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  285:             blk.31.ffn_up.weight q5_K     [  4096, 14336,     1,     1 ]\nllama_model_loader: - tensor  286:           blk.31.ffn_down.weight q6_K     [ 14336,  4096,     1,     1 ]\nllama_model_loader: - tensor  287:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  288:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  289:               output_norm.weight f32      [  4096,     1,     1,     1 ]\nllama_model_loader: - tensor  290:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\nllama_model_loader: - kv   0:                       general.architecture str     \nllama_model_loader: - kv   1:                               general.name str     \nllama_model_loader: - kv   2:                       llama.context_length u32     \nllama_model_loader: - kv   3:                     llama.embedding_length u32     \nllama_model_loader: - kv   4:                          llama.block_count u32     \nllama_model_loader: - kv   5:                  llama.feed_forward_length u32     \nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32     \nllama_model_loader: - kv   7:                 llama.attention.head_count u32     \nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32     \nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32     \nllama_model_loader: - kv  10:                       llama.rope.freq_base f32     \nllama_model_loader: - kv  11:                          general.file_type u32     \nllama_model_loader: - kv  12:                       tokenizer.ggml.model str     \nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr     \nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr     \nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr     \nllama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32     \nllama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32     \nllama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32     \nllama_model_loader: - kv  19:               general.quantization_version u32     \nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_print_meta: format           = GGUF V2 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 32000\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = mostly Q5_K - Medium\nllm_load_print_meta: model params     = 7.24 B\nllm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \nllm_load_print_meta: general.name   = mistralai_mistral-7b-instruct-v0.1\nllm_load_print_meta: BOS token = 1 '&lt;s&gt;'\nllm_load_print_meta: EOS token = 2 '&lt;/s&gt;'\nllm_load_print_meta: UNK token = 0 '&lt;unk&gt;'\nllm_load_print_meta: LF token  = 13 '&lt;0x0A&gt;'\nllm_load_tensors: ggml ctx size =    0.09 MB\nllm_load_tensors: mem required  = 4893.09 MB\n..................................................................................................\nllama_new_context_with_model: n_ctx      = 512\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_new_context_with_model: kv self size  =   64.00 MB\nllama_new_context_with_model: compute buffer total size = 78.88 MB\nAVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | \n\n\n\n# running the official example via python\nprompt = \"Building a website can be done in 10 simple steps:\\nStep 1:\"\noutput = llm(prompt, max_tokens=512, echo=True)\n\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =  5542.08 ms\nllama_print_timings:      sample time =   147.45 ms /   218 runs   (    0.68 ms per token,  1478.49 tokens per second)\nllama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\nllama_print_timings:        eval time = 13963.44 ms /   218 runs   (   64.05 ms per token,    15.61 tokens per second)\nllama_print_timings:       total time = 14369.62 ms\n\n\n\noutput['choices'][0]['text']\n\n\"Building a website can be done in 10 simple steps:\\nStep 1: Determine the purpose of your website. This will help you decide on the features and design that are important to you.\\nStep 2: Choose a domain name that represents your brand and is easy to remember.\\nStep 3: Select a hosting provider that offers reliable uptime and customer support.\\nStep 4: Choose a content management system (CMS) or website builder that suits your needs.\\nStep 5: Create a layout and design for your website that is visually appealing and easy to navigate.\\nStep 6: Develop the content for your website, including text, images, and videos.\\nStep 7: Optimize your website for search engines (SEO) by using relevant keywords and meta tags.\\nStep 8: Test your website for functionality, usability, and compatibility with different devices.\\nStep 9: Launch your website and promote it through social media, email marketing, and other online channels.\\nStep 10: Monitor your website's performance and make updates as needed to ensure its success.\"\n\n\n\nRevisiting the MLC Outputs\n\nprompt = \"Hello! Please get ready to help me with my project!\"\n# asking Llama2 about itself\noutput = cm.generate(\n    prompt=prompt,\n    progress_callback=StreamToStdout(callback_interval=2),\n)\n\nHello! *adjusts glasses* I'm so glad you're excited about your project! I'm here to help you in any way I can. Could you please provide some more details about your project, such as its topic, any specific questions you have, or what you hope to achieve? That will help me better understand how I can assist you. ðŸ˜Š\n\n\n\ndef talk(txt):\n    output = cm.generate(\n    prompt=txt,\n    progress_callback=StreamToStdout(callback_interval=2),\n)\n\n\ntalk(\"Please be less cringe, thank you\")\n\nOf course, I apologize if my previous response came across as too cheery. I'm here to help you in a respectful and professional manner. Please let me know how I can assist you with your project. Are there any specific tasks you need help with, or any information you need me to provide? Please feel free to ask me anything. ðŸ˜Š\n\n\n\ntalk(\"Actually, please be as cringe as possible, thank you!\")\n\nOh, my goodness! *adjusts glasses* I'm so thrilled to be of assistance to you, my dear! *bounces up and down* I'll do my absolute bestest to help you with your project, even if it means being a wee bit cheesy and cringeworthy! *winks* Please do tell me how I can help, you delightful person, you! ðŸ˜Š\n\n\n\ntalk(\"Hmmm, no quite cringe enough! More cringe please uwu\")\n\nOh, my goodness gracious! *adjusts glasses* I'm so glad you want me to be even more cringe! *bounces up and down* I'll do my absolute bestest to provide you with the most cringe-tastic responses possible! *winks*\nSo, my sweet darling, how may I assist you on this lovely day? *blinks* Do tell me your most precious wish, and I shall do my utmost to make it come true! *bats eyelashes* ðŸ˜Š\n\n\n\ntalk(\"You are actually being a 6/10 cringe. Please show me what a 10000/10 cringe would look like!\")\n\nOh, my goodness gracious! *adjusts glasses* I-I can't possibly...*gulps*...I-I think I'll just have to...*tries to contain excitement*...OVERDO IT! *pulls out all the cringe*\nSo, my sweet, sweet darling...*bounces up and down*...I shall provide you with the most...*gulps*...CRINGE-TASIC RESPONSES POSSIBLE! *winks* *adjusts glasses* *bats eyelashes* ðŸ˜‚ðŸ‘€ðŸ’–\nPlease, my precious, tell me how I may serve you in this, the most cringe-tastic way possible! *curtsies* ðŸ’•\n\n\nStarting the conversation:\n\n# asking Llama2 about itself\noutput = cm.generate(\n    prompt=\"Please tell me a little about yourself:\",\n    progress_callback=StreamToStdout(callback_interval=2),\n)\n\nHello! *adjusts glasses* I'm just an AI, here to help you with any questions or tasks you may have. My purpose is to provide helpful, respectful, and honest assistance to the best of my abilities. I'm just a language model, so I don't have personal experiences or feelings like humans do, but I'm always eager to learn and improve my responses. Is there something specific you'd like to know or discuss?\n\n\nCan also ask question on the fly using pythonâ€™s input() which works in Jupyter Notebooks\n\n# asking Llama2 something on the fly\nprompt = input(\"Prompt: \")\noutput = cm.generate(prompt=prompt, progress_callback=StreamToStdout(callback_interval=2))\n\nGreat, I'm glad you asked! I'm best suited to helping with a wide range of tasks, including but not limited to:\n1. Answering questions: I can provide information and explanations on various topics, from science and history to entertainment and culture.\n2. Generating ideas: I can help you brainstorm ideas for creative projects, or even come up with unique solutions to problems you might be facing.\n3. Language translation: I can translate text from one language to another, helping you communicate with people from different cultures and backgrounds.\n4. Summarizing content: If you have a long piece of text and want to get a quick summary of its main points, I can help you with that.\n5. Offering suggestions: I can provide suggestions for things like gift ideas, travel destinations, or even books to read.\n6. Providing definitions: If you're unsure of the meaning of a word or phrase, I can define it for you and give you examples of how it's used in context.\n7. Creating content: I can assist you in generating content for various mediums, such as articles, social media posts, or even entire books.\n8. Conversing: I'm here to chat and help with any questions or topics you'd like to discuss.\nFeel free to ask me anything, and I'll do my best to assist you!\n\n\nWhat if we wanted a quick summary of what it said?\n\n# asking for a summary of its response\noutput = cm.generate(\n    prompt=\"Please summarize your response in three sentences.\",\n    progress_callback=StreamToStdout(callback_interval=2),\n)\n\nOf course! Here's a summary of my response in three sentences:\nI'm a language model AI trained to assist with various tasks, including answering questions, generating ideas, translating languages, summarizing content, offering suggestions, providing definitions, and creating content. I'm here to help with any questions or topics you'd like to discuss, so feel free to ask me anything. I'm best suited to helping with a wide range of tasks, including but not limited to those listed above.\n\n\nCan go back and forth with the cells above, or can continue talking in other cells\n\n# asking another question\nnew_prompt = input(\"New, Different Prompt: \")\noutput = cm.generate(prompt=new_prompt, progress_callback=StreamToStdout(callback_interval=2))\n\nAbsolutely! I'm here to help. What topic would you like to learn more about? Let me know and I'll do my best to provide you with helpful information and resources.\n\n\nThe chat module maintains an internal chat history. If we get stuck in a loop or simply want to start the convo anew:\n\n# resets the current session's chat history\ncm.reset_chat()\n\nThere is a handy stats() function to check the speed of the modelâ€™s generation.\n\n# checks if llm go brrr\nprint(cm.stats())\n\nprefill: 11.7 tok/s, decode: 12.7 tok/s\n\n\nFor a more rigorous check, we can use the benchmark_generate function to check the speed of a fixed number of tokens:\n\n# benchmarking text generation\nprint(cm.benchmark_generate(prompt=\"What is benchmark?\", generate_length=512))\ncm.stats()\n\n\n nobody expects the Spanish Inquisition! ðŸ˜±ðŸ”¥ðŸŽ¬\n\nBenchmarking is the process of measuring the performance of a system, application, or process against a set of predefined metrics or standards. The goal of benchmarking is to identify areas where improvements can be made, such as increased efficiency, faster performance, or better quality.\nIn the context of the Monty Python sketch, \"nobody expects the Spanish Inquisition!\" is a humorous reference to the unexpected and often absurd nature of benchmarking. Just as the Inquisition was unexpected and unwanted, benchmarking can sometimes be seen as an unnecessary or burdensome process. However, the benefits of benchmarking can be significant, such as identifying areas for improvement, optimizing resources, and ensuring compliance with standards or regulations. ðŸ˜Šs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'prefill: 1.8 tok/s, decode: 30.0 tok/s'"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html",
    "title": "A Python Environment for LLMs",
    "section": "",
    "text": "Building an LLM python environment using mamba and pip"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#things-we-need-for-the-class",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#things-we-need-for-the-class",
    "title": "A Python Environment for LLMs",
    "section": "Things we need for the class",
    "text": "Things we need for the class\nIn order to fully use a current, open source LLM, the first thing we need to do is set up a proper programming environment. The environment is a computing ecosystem with all the software libraries and packages needed to run an LLM.\nNote that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem (insert xkcd comic about competing standards).\nThe main point here is that setting up the environment is often annoying. It can even be straight up painful. Itâ€™s ok to feel lost or struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!\nSo what makes building this environment so challenging? And why do we need it in the first place?\n\nSilent Failures in AI Models\nLLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, classic bugs in regular software include: type mismatches, syntax errors, compilation errors, etc. In other words failures that clearly stem from a wrong operation (aka a bug) that snuck into the code. We wanted the computer to do X, but we told it by accident to do Y instead.\nIn contrast, ML models often have â€œsilentâ€ failures. There is no syntax or compilation error - the program still runs and completes fine. But, there is something wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would (or even could, for now) catch these errors.\nThe fix for the silent failures above is clear:\n- Carefully inspecting the code.\n- Monitoring and validating its outputs.\n- Clarity in both the algorithms and models we are using.\nThere is another, unfortunate kind of silent failure: version mismatches. Version failures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model. These failures are unfortunately immune to our careful logical checks.\nAvoiding these silent failures is the main reason for being consistent and disciplined with our modelâ€™s programming environment. A good environment setup keeps us focused on the important, conceptual part of our model instead of getting bogged down in managing software versions.\n\n\nLooking forward with our environment\nThere is a nice benefit to spending this much time and effort up front on our environment.\nWe will not only have a specialized environment to run and fine-tune a single LLM. Weâ€™ll have a springboard and setup to keep up with the state of the art in the field. A setup to bring in groundbreaking improvements as they are released. And to weave in the latest and greatest models. The LLM world is our oyster, and the base environment the grain of sand soon-to-be pearls."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#organizing-what-we-need",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#organizing-what-we-need",
    "title": "A Python Environment for LLMs",
    "section": "Organizing what we need",
    "text": "Organizing what we need\nThe mamba package manager will handle the python version. Why Mamba? To start it is way fast and better than Anaconda, and it makes it easier to install OS and system-level packages we need outside of python.\nWe will use pip to install the actual python packages. Note that we could use mamba for this as well, but a few of the libraries need custom pip options to install.\n\nNote: Run pip install -e . to install a dynamic version of this package that tracks live code changes."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#mac-installation",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#mac-installation",
    "title": "A Python Environment for LLMs",
    "section": "Mac Installation",
    "text": "Mac Installation\nFirst find the name of your architecture. We then use it to pick the right install script for each Mac.\n\n# check your mac's architecture\narch=$(uname) \necho $arch\n\n# download the appropriate installation script\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\n\n# run the Mambaforge installer\nbash Mambaforge-$(uname)-$(uname -m).sh\nIf you prefer to download the file directly, grab it from here:\nhttps://github.com/conda-forge/miniforge/releases/"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#creating-the-environment",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#creating-the-environment",
    "title": "A Python Environment for LLMs",
    "section": "Creating the environment",
    "text": "Creating the environment\nAfter installing Mamba, head to the Lesson 0 here: Fractal_LLM_Course/lesson_0/envs. The README.md in that folder has the full instructions to build the mamba environment.\nThis folder has all of the pieces we need to build our LLM environment.\nDetails: - Mamba package manager to create the base python environment.\n- Requirements file to install the needed packages with pip.\nThe steps below will create an environment called llm_base. It will have all the pieces we need to get started.\n\n# create the base environment\nmamba env create -f environment.yml\n\n# activate the environment\nmamba activate llm_base\n\n# install the python packages, after activating the env\npython -m pip install -r requirements.txt  \n\n# install the pytorch library\npython -m pip install -r reqs_torch_cpu.txt\n\nMANAGING THE CUDA DRIVERS RESOURCE\n\nThe first line installs the â€œhelperâ€ libraries that will make our lives easier.\nThe second line installs the pytorch library, which weâ€™ll use to load and use the actual LLMs.\n\nNote: On the cloud, you would install the reqs_torch.txt which uses the GPU.\n\nEventually, to speed up the LLMs, we will also need the following libraries:\n# OPTIONAL: install the additional libraries\npython -m pip install -r reqs_optim.txt\nBut these libraries can be tricky to install. Donâ€™t worry if you run into issues, we will revist them later.\nResources:\n- (Installing python on your computer)[https://realpython.com/installing-python] - (Downloading VSCode to edit our code)[https://code.visualstudio.com/download] - (Installing git to manage our code)[https://git-scm.com/book/en/v2/Getting-Started-Installing-Git]"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#helper-libraries",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#helper-libraries",
    "title": "A Python Environment for LLMs",
    "section": "Helper libraries",
    "text": "Helper libraries\nFirst, some best practice. Make sure to update the Ubuntu package list:\nsudo apt update\nThere are two Ubuntu packages that are worth installing:\n- software-properties-common - build-essential\nsoftware-properties-common is a set of tools for adding and managing software repositories. It makes our life a bit easier.\nbuild-essential contains a list of packages that are essential for building Ubuntu packages. It has software key for development like the GNU Compiler Collection (GCC) and GNU Make. It also has the tools to build and install projects from source (aka straight from the repoâ€™s folder). Itâ€™ll help us build and install repos like llama.cpp from source.\nsudo apt install software-properties-common\nsudo apt install build-essential\nThen weâ€™ll add the graphics drivers PPA and update the package list again.\nsudo add-apt-repository ppa:graphics-drivers/ppa\nsudo apt update"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#install-driver",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#install-driver",
    "title": "A Python Environment for LLMs",
    "section": "Install Driver",
    "text": "Install Driver\nNow we can install the nvidia drivers. As of writing, the 535 version of the driver is stable and supports a good number of GPU cards.\nsudo apt install nvidia-driver-535\nMake sure to reboot your system:\nsudo reboot\nAfter logging back in, we can check that the driver is installed correctly:\nnvidia-smi"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#install-cuda",
    "href": "blog/posts/fractal-llms/2023-10-05-Session_0_env/index.html#install-cuda",
    "title": "A Python Environment for LLMs",
    "section": "Install CUDA",
    "text": "Install CUDA\nWith the driver working we can now install the CUDA library. The CUDA library has a set of libraries optimized for NVIDIA GPUs.\nThe example below uses a local .dev installer, and comes straight from the official CUDA website\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\nsudo apt-get update\nsudo apt-get -y install cuda\n\nExport the following to the `~/.bashrc` file to make sure the CUDA libraries are available in the terminal:  \n```bash\necho 'export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nThis command will take a while to run. Once itâ€™s done, reboot one last time just to be to be safe:\nsudo reboot\npaperspace@ps3r37lmx:~$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Tue_Feb__7_19:32:13_PST_2023\nCuda compilation tools, release 12.1, V12.1.66\nBuild cuda_12.1.r12.1/compiler.32415258_0\nIf everything goes well, we should now see that the CUDA version is release 12.1.\nnvcc --version # should output something like the following:\n\n# nvcc: NVIDIA (R) Cuda compiler driver\n# Copyright (c) 2005-2023 NVIDIA Corporation\n# Built on Tue_Feb__7_19:32:13_PST_2023\n# Cuda compilation tools, release 12.1, V12.1.66\n# Build cuda_12.1.r12.1/compiler.32415258_0"
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html",
    "title": "Tips for LLM prompts",
    "section": "",
    "text": "General tips to get better outputs from ChatGPT.\n# Tips and tricks for ChatGPT Prompts\nThis post is a recap of openaiâ€™s suggestions (as of writing) for improving ChatGPTâ€™s outputs.\nThere are many teams actively working on improving and deploying new LLMs with useful (powerful) abilities. Their work has produced several papers that show different ways of improving an LLMâ€™s output.\nThe official openAI documentation has the full details and discussion of a few key papers.\nHowever, we can also extract a broad set of suggestions based on what the different proposed improvements share in common."
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html#improving-outputs",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html#improving-outputs",
    "title": "Tips for LLM prompts",
    "section": "Improving outputs",
    "text": "Improving outputs\n\nSplit large, complex tasks into subtasks\n\nStructure and isolate the instructions of each subtask\n\nPrompt the model to explain its reasoning(s) before answering\nIf the output was bad, try making the instructions clearer\n\nStart with simple and direct language\nCan get more complex as the conversation and context grow\n\nHave the model generate many answers, then ask it to distill them into a single, best answer\nIf possible, Fine-tune custom models to maximize performance"
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html#generic-tips",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html#generic-tips",
    "title": "Tips for LLM prompts",
    "section": "Generic tips",
    "text": "Generic tips\n\nExplicitly guide the model through the thought process\n\nHelps it stay focused on sub-tasks and subprocesses\n\nâ€œLetâ€™s think step by stepâ€¦â€\n\nWorks best on logical, mathematical, and reasoning tasks\nPossible leverage for other tasks by breaking them down into â€œlogicalâ€ steps\n\nGive the model a few examples of the task you want (Few-Shot)\nSplit a question into two types of prompts and alternate between the two\n\nSelection prompt -&gt; find the relevant pieces of into\nInference prompt -&gt; use the relevant pieces to generate the answer\nHalter prompt -&gt; figure out when the alternating should halt, if possible add a value function to evaluate different prompts\n\nReduce hallucinations by constraining what the model can say"
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html#api-tips",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html#api-tips",
    "title": "Tips for LLM prompts",
    "section": "API Tips",
    "text": "API Tips\n\nGive the model an identity that behaves in a certain way with an explicit intent\nAsk to model to answer from the perspective of an expert\nTry restating the original â€œsystemâ€ message to keep the model on-task.\nIf the model is getting off-track, try reminding it of the instruction and context at the end of the prompt"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "",
    "text": "Improving generated images with dynamic Classifier-free Guidance across Diffusion models."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#recap-of-parts-1-6",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#recap-of-parts-1-6",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Recap of Parts 1-6",
    "text": "Recap of Parts 1-6\nIn the first six parts, we found a good, initial set of schedules and normalizations. The most promising schedules are used in this notebook."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#part-7-improvement-across-models",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#part-7-improvement-across-models",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Part 7: Improvement across models",
    "text": "Part 7: Improvement across models\nPart 7 runs our best schedules on the following Diffusion models:\n\nStable Diffusion v1-4\n\nStable Diffusion v1-5\n\nPrompt Heroâ€™s openjourney\n\nStable Diffusion 2-base"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed for rng\nSEED = 977145576 \ndef seed_everything(seed: int) -&gt; torch.Generator:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#image-parameters",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. The height and width will depend on the Stable Diffusion model\n\n# the number of diffusion steps\nnum_steps = 50\n\n# dimensions for v1 and v2 Stable Diffusions\nv1_sd_dims = {'height': 640, 'width': 512}\nv2_sd_dims = {'height': 768, 'width': 768}"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#static-baselines",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#static-baselines",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Static baselines",
    "text": "Static baselines\nFirst we create the constant, baseline Guidances.\nFor Prediction Normalization we use the same default of \\(G = 7.5\\). For T-Normalization and Full Normalization, we use a static \\(G_\\text{small} = 0.15\\).\n\n# create the baseline schedule with the new function\nbaseline_g = 7.5\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda *args, **kwargs: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)\n\nT_baseline_g = 0.15\nT_baseline_params = {'max_val': [T_baseline_g]}\nT_baseline_func = lambda *args, **kwargs: [T_baseline_g for _ in range(num_steps)]\nT_baseline_expts = create_expts(T_baseline_params, T_baseline_func)"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Improving the baseline with schedules and normalizations",
    "text": "Improving the baseline with schedules and normalizations\nNow we build the most promising schedule so far: Inverse kDecay with a fast warmup.\n\n# start by creating regualr kDecay cosine schedules\ninv_k_params = {'k_decay': [0.15]}\ninv_k_func = partial(cos_harness, default_params=DEFAULT_COS_PARAMS)\ninv_k_expts = create_expts(inv_k_params, inv_k_func)\n\n# invert the schedules to turn them into a type of warmup \nfor s in inv_k_expts:\n    s['schedule'] = [max_val - g + min_val for g in s['schedule']]\n\n# put all schedules together\nall_k_expts = inv_k_expts\n\n\n\n\n\n\nWe also build a matching schedule with smaller \\(G\\) values for the T and Full Normalizations.\n\n# create the kDecay cosine experiments\nT_inv_k_func = partial(cos_harness, default_params=DEFAULT_T_PARAMS)\nT_inv_k_expts = create_expts(inv_k_params, T_inv_k_func)\n\n# inverse the schedules\nfor s in T_inv_k_expts:\n    s['schedule'] = [max_T - g + min_T for g in s['schedule']]\n\nall_T_k_expts = T_inv_k_expts"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#function-to-run-the-experiments",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#function-to-run-the-experiments",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Function to run the experiments",
    "text": "Function to run the experiments\nThe previous notebooks ran one Diffusion model at a time. Now, we need to load the model as part of the pipeline.\n\n# to load Stable Diffusion pipelines\nfrom min_diffusion.core import MinimalDiffusion\n# to plot generated images\nfrom min_diffusion.utils import show_image, image_grid, plot_grid\n\nTo do this, we move the model loading code load_sd_model into the run function. We also add some memory cleanup at the end to free up the GPU for the next model.\n\ndef load_sd_model(model_name, device, dtype, model_kwargs={}, generator=None):\n    '''Loads the given `model_name` Stable Diffusion in `dtype` precision.  \n    \n    The model is placed on the `device` hardware. \n    The optional `generator` is used to create noisy latents.  \n    Optional `model_kwargs` are passed to the model's load function.\n    '''\n    pipeline = MinimalDiffusion(model_name, device, dtype, generator=generator)\n    pipeline.load(**model_kwargs);\n    return pipeline\n\n\ndef run(pipeline, prompt, schedules, gen_kwargs={},\n        guide_tfm=None, generator=None, show_each=False, test_run=False):\n    \"\"\"Runs a dynamic Classifier-free Guidance experiment. \n    \n    Generates an image for the text `prompt` given all the values in `schedules`.\n    Uses a Guidance Transformation class from the `cf_guidance` library.  \n    Stores the output images with a matching title for plotting. \n    Optionally shows each image as its generated.\n    If `test_run` is true, it runs a single schedule for testing. \n    \"\"\"\n    # store generated images and their title (the experiment name)\n    images, titles = [], []\n    \n    # make sure we have a valid guidance transform\n    assert guide_tfm\n    print(f'Using Guidance Transform: {guide_tfm}')\n    \n    # optionally run a single test schedule\n    if test_run:\n        print(f'Running a single schedule for testing.')\n        schedules = schedules[:1]\n        \n    # run all schedule experiments\n    for i,s in enumerate(schedules):\n        \n        # parse out the title for the current run\n        cur_title  = s['title']\n        titles.append(cur_title)\n        \n        # create the guidance transformation \n        cur_sched = s['schedule']\n        gtfm = guide_tfm({'g': cur_sched})\n        \n        print(f'Running experiment [{i+1} of {len(schedules)}]: {cur_title}...')\n        img = pipeline.generate(prompt, gtfm, **gen_kwargs)\n        images.append(img)\n        \n        # optionally plot each generated image\n        if show_each:\n            show_image(img, scale=1)\n            \n    print('Done.')\n    return {'images': images,\n            'titles': titles}"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#reading-the-plots",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#reading-the-plots",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Reading the plots",
    "text": "Reading the plots\nFor each model, we plot a grid with its generated images. The grid has two rows and four columns.\nThe first row shows results from the fixed, constant Guidance. The second row shows results for the Inverse kDecay cosine schedules.\nThe first column shows the baseline: unnormalized Classifier-free Guidance with a constant \\(G = 7.5\\).\nThe second column has the Prediction Normalization results.\nThe third column has the T-Normalization results.\nThe fourth column has the Full Normalization results.\nIn general we expect that normalization should improve the images. In other words, the second, third, and fourth column should be better than the first column (the baseline).\nLikewise, we expect that the Inverse kDecay schedules are better than the static schedules. That means that, for a given column, the result in its second row should be better than its first row.\n\n\n\n\n\n\nNote\n\n\n\nThe plotting functions are available in the notebook. They are omitted here for space."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-4",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-4",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Stable Diffusion v1-4",
    "text": "Stable Diffusion v1-4\n\nplot_all_results('CompVis/stable-diffusion-v1-4')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-5",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Stable Diffusion v1-5",
    "text": "Stable Diffusion v1-5\n\nplot_all_results('runwayml/stable-diffusion-v1-5')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#openjourney",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#openjourney",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "openjourney",
    "text": "openjourney\n\nplot_all_results('prompthero/openjourney')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-2-base",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-2-base",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Stable Diffusion 2-base",
    "text": "Stable Diffusion 2-base\n\nplot_all_results('stabilityai/stable-diffusion-2-base')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#evaluating-the-outputs",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#evaluating-the-outputs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Evaluating the outputs",
    "text": "Evaluating the outputs\nIn general, it seems that Prediction Normalization adds more details to the image and background. T-Normalization makes the image â€œsmootherâ€ and can help with its syntax. Full Normalization, which is a combination of the two, seems to get a bit from both worlds."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "",
    "text": "Exploring a range of guidance values for T-Normalization."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#recap-of-parts-1-4",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#recap-of-parts-1-4",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Recap of Parts 1-4",
    "text": "Recap of Parts 1-4\nThe first three parts explored how to turn Classifier-free Guidance into a dynamic process. We found an initial set of schedules and normalizers that seem to improve the quality of Diffusion images. We then dug in and refined a few of the most promising schedules."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#part-5-exploring-values-for-t-normalization",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#part-5-exploring-values-for-t-normalization",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Part 5: Exploring values for T-Normalization",
    "text": "Part 5: Exploring values for T-Normalization\nPart 5 answers the question: what should the value of \\(G_\\text{small}\\) be for T-Normalization and Full Normalization?\nRecall that these two normalizations scale the update vector \\(\\left(t - u \\right)\\). That places the update vector on a different scale than the unconditioned vector \\(u\\). If we then scaled the update vector by a large scalar, say \\(G = 7.5\\), the output collapses to noise. In fact it seems to collapse to the true mode of the latent image distribution: uniform, brown values.\nThese two normalizations are very promising: they improve the syntax and details of the image. However, we only explored a single value of \\(G_\\text{small} = 0.15\\). This is very different from the default \\(G = 7.5\\) that has been truly explored in regular Classifier-free Guidance.\nThis notebook tries to find a good starting point for \\(G_\\text{small}\\), so we can try the normalizations with our best schedules so far."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#image-parameters",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-sweep",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-sweep",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "T-Normalization with \\(G_\\text{small}\\) sweep",
    "text": "T-Normalization with \\(G_\\text{small}\\) sweep\n\nprint('Running the k-Sweep experiments...')\nt_norm_res = run(prompt, const_expts, guide_tfm=TNormGuidance)\n\nRunning the k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 10]: Param: \"max_val\", val=0.01...\nRunning experiment [2 of 10]: Param: \"max_val\", val=0.03...\nRunning experiment [3 of 10]: Param: \"max_val\", val=0.05...\nRunning experiment [4 of 10]: Param: \"max_val\", val=0.08...\nRunning experiment [5 of 10]: Param: \"max_val\", val=0.1...\nRunning experiment [6 of 10]: Param: \"max_val\", val=0.15...\nRunning experiment [7 of 10]: Param: \"max_val\", val=0.2...\nRunning experiment [8 of 10]: Param: \"max_val\", val=0.22...\nRunning experiment [9 of 10]: Param: \"max_val\", val=0.25...\nRunning experiment [10 of 10]: Param: \"max_val\", val=0.3...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-sweep",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-sweep",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Full Normalization with \\(G_\\text{small}\\) sweep",
    "text": "Full Normalization with \\(G_\\text{small}\\) sweep\n\nprint('Running the k-Sweep experiments...')\nfull_norm_res = run(prompt, const_expts, guide_tfm=FullNormGuidance)\n\nRunning the k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 10]: Param: \"max_val\", val=0.01...\nRunning experiment [2 of 10]: Param: \"max_val\", val=0.03...\nRunning experiment [3 of 10]: Param: \"max_val\", val=0.05...\nRunning experiment [4 of 10]: Param: \"max_val\", val=0.08...\nRunning experiment [5 of 10]: Param: \"max_val\", val=0.1...\nRunning experiment [6 of 10]: Param: \"max_val\", val=0.15...\nRunning experiment [7 of 10]: Param: \"max_val\", val=0.2...\nRunning experiment [8 of 10]: Param: \"max_val\", val=0.22...\nRunning experiment [9 of 10]: Param: \"max_val\", val=0.25...\nRunning experiment [10 of 10]: Param: \"max_val\", val=0.3...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-g_textsmall-results",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-g_textsmall-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "T-Normalization \\(G_\\text{small}\\) results",
    "text": "T-Normalization \\(G_\\text{small}\\) results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-g_textsmall-results",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-g_textsmall-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Full Normalization \\(G_\\text{small}\\) results",
    "text": "Full Normalization \\(G_\\text{small}\\) results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#phase-change-in-the-image",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#phase-change-in-the-image",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Phase change in the image",
    "text": "Phase change in the image\nMost interesting, there is a â€œphase changeâ€ between the values of 0.08 and 0.1. The image completely changes style and pose from the previous results weâ€™ve seen so far in the series. This phase change on its own deserves more exploration! What happens around these values of \\(G_\\text{small}\\)?\nLetâ€™s re-run experiments focused on this range. We will pick 10 points uniformly spread between 0.08 and 0.1 to see if we can catch where the phase changes."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-phase-change",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-phase-change",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "T-Normalization with \\(G_\\text{small}\\) phase change",
    "text": "T-Normalization with \\(G_\\text{small}\\) phase change\n\n\n\n\n\nIt seems the phase change happens between 0.088 and 0.09. Letâ€™s check if this is also true for Full Normalization."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-phase-change",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-phase-change",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Full Normalization with \\(G_\\text{small}\\) phase change",
    "text": "Full Normalization with \\(G_\\text{small}\\) phase change\n\nprint('Running the phase change k-Sweep experiments...')\nfull_phase_res = run(prompt, phase_expts, guide_tfm=FullNormGuidance)\n\nRunning the phase change k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"max_val\", val=0.08...\nRunning experiment [2 of 12]: Param: \"max_val\", val=0.082...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.084...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.086...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.088...\nRunning experiment [6 of 12]: Param: \"max_val\", val=0.09...\nRunning experiment [7 of 12]: Param: \"max_val\", val=0.092...\nRunning experiment [8 of 12]: Param: \"max_val\", val=0.094...\nRunning experiment [9 of 12]: Param: \"max_val\", val=0.096...\nRunning experiment [10 of 12]: Param: \"max_val\", val=0.09799999999999999...\nRunning experiment [11 of 12]: Param: \"max_val\", val=0.09999999999999999...\nRunning experiment [12 of 12]: Param: \"max_val\", val=0.102...\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe phase change happens in the same place! In fact the change is more pronounced, there is definitely something strange with the horseâ€™s head as we hit the phase transition around \\(G_\\text{small} = 0.09\\).\nHowever, it seems that the image grows darker and less clear as we move away from the phase change. The horseâ€™s body is less illuminated and is even hard to see.\nOne last check, what if the images before the phase change are better? We already saw that 0.05 was a bit too low, but what about values between 0.06 and 0.08?"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#checking-for-phase-change-multiples",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#checking-for-phase-change-multiples",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Checking for phase change multiples",
    "text": "Checking for phase change multiples\nWill we find another phase change around three times from the first one? Letâ€™s find out.\n\nlow_bound = 0.25\nhi_bound = 0.29\nnpoints = 11\n\npoints = np.linspace(low_bound, hi_bound, npoints+1); points\n\narray([0.25      , 0.25363636, 0.25727273, 0.26090909, 0.26454545,\n       0.26818182, 0.27181818, 0.27545455, 0.27909091, 0.28272727,\n       0.28636364, 0.29      ])\n\n\n\n# create the constant G_small cosine experiments\nlater_phase_params = {'max_val': list(points)}\nlater_phase_func = lambda val: [val for _ in range(num_steps)]\nlater_phase_expts = create_expts(later_phase_params, later_phase_func)\n\n\nprint('Running the phase change k-Sweep experiments...')\nlater_phase_res = run(prompt, later_phase_expts, guide_tfm=TNormGuidance)\n\nRunning the phase change k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"max_val\", val=0.25...\nRunning experiment [2 of 12]: Param: \"max_val\", val=0.25363636363636366...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.25727272727272726...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.2609090909090909...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.26454545454545453...\nRunning experiment [6 of 12]: Param: \"max_val\", val=0.2681818181818182...\nRunning experiment [7 of 12]: Param: \"max_val\", val=0.2718181818181818...\nRunning experiment [8 of 12]: Param: \"max_val\", val=0.27545454545454545...\nRunning experiment [9 of 12]: Param: \"max_val\", val=0.27909090909090906...\nRunning experiment [10 of 12]: Param: \"max_val\", val=0.2827272727272727...\nRunning experiment [11 of 12]: Param: \"max_val\", val=0.2863636363636364...\nRunning experiment [12 of 12]: Param: \"max_val\", val=0.29...\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no clear phase change, but the image is starting to fall apart. It is safe to say we are in territory where \\(G_\\text{small}\\) is too large.\n\\(G_\\text{small} = 0.25\\) is the last image where we have a fully correct, non-smeared astronaut."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "",
    "text": "Experiments with cosine schedules and normalizations for Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#quick-recap-of-parts-1-and-2",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#quick-recap-of-parts-1-and-2",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Quick recap of Parts 1 and 2",
    "text": "Quick recap of Parts 1 and 2\nIn Part 1, we generated a baseline image using a constant Classifier-free Guidance. Attempting to improve on the baseline, we swept the guidance parameter \\(G\\) over a set of Cosine Schedules.\nIn Part 2, we introduced normalizations for Classifier-free Guidance. There was one kind of normalization, Prediction Normalization, that seems to improve the overall quality of generated images."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#part-3-combining-schedules-and-normalizations",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#part-3-combining-schedules-and-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Part 3: Combining schedules and normalizations",
    "text": "Part 3: Combining schedules and normalizations\nIn Part 3, we build on the previous results by now combining guidance normalizations and schedules.\nThe goal is to find a combo of normalized schedules that universally improve the outputs of Diffusion image models."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#leveraging-a-few-helper-libraries",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#leveraging-a-few-helper-libraries",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Leveraging a few helper libraries",
    "text": "Leveraging a few helper libraries\nWe reuse our helper libraries to more efficiently run guidance experiments. The two libraries are:\n\nmin_diffusion\ncf_guidance\n\nThey were introduced in this separate post."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#python-imports",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#python-imports",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Python Imports",
    "text": "Python Imports\nFirst we import the needed python modules.\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom fastcore.all import L\nfrom functools import partial\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-24 18:34:14.079096: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n\n\n\nSeed for reproducibility\nWe use the seed_everything function to make sure that the results are repeatable across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#importing-the-helper-libraries",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#importing-the-helper-libraries",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Importing the helper libraries",
    "text": "Importing the helper libraries\nThe cf_guidance library has the guidance schedules and normalizations.\n\n# helpers to create cosine schedules\nfrom cf_guidance.schedules  import get_cos_sched\n\n# normalizations for classifier-free guidance\nfrom cf_guidance.transforms import GuidanceTfm, BaseNormGuidance, TNormGuidance, FullNormGuidance\n\nThe min_diffusion library loads a Stable Diffusion model from the HuggingFace hub.\n\n# to load Stable Diffusion pipelines\nfrom min_diffusion.core import MinimalDiffusion\n\n# to plot generated images\nfrom min_diffusion.utils import show_image, image_grid, plot_grid"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#image-parameters",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Image parameters",
    "text": "Image parameters\nThe images will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#setting-the-schedule-parameters",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#setting-the-schedule-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Setting the schedule parameters",
    "text": "Setting the schedule parameters\nRecall that there are three kinds of schedules:\n\nA static schedule with a constant \\(G\\).\n\nA decreasing Cosine schedule.\n\nA Cosine schedule with some initial warm up steps.\n\nWe already created the static schedule 1. in the baseline above. This section creates variations of schedules 2. and 3..\n:::: {.callout-note}.\nWe need smaller guidance values for T-Normalization and Full Normalization.\nThese normalizations get their own, smaller value of \\(G_\\text{small} = 0.15\\). This smaller value keeps the guidance update vector \\(\\left( t - u \\right)\\) from exploding in scale.\n::::\n\n# Default schedule parameters from the blog post\n######################################\nmax_val           = 7.5   # guidance scaling value\nmin_val           = 1     # minimum guidance scaling\nnum_steps         = 50    # number of diffusion steps\nnum_warmup_steps  = 0     # number of warmup steps\nwarmup_init_val   = 0     # the intial warmup value\nnum_cycles        = 0.5   # number of cosine cycles\nk_decay           = 1     # k-decay for cosine curve scaling \n\n# smaller values for T-Norm and FullNorm\nmax_T = 0.15\nmin_T = 0.05\n######################################\n\nTo make sure our changes always reference this shared starting point, we can wrap these parameters in a dictionary.\nWe also create a matching dictionary for the T-Norm params.\n\nDEFAULT_COS_PARAMS = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        num_cycles,\n    'k_decay':           k_decay,\n    'num_warmup_steps':  num_warmup_steps,\n    'warmup_init_val':   warmup_init_val,\n}\n\nDEFAULT_T_PARAMS = {\n    'max_val':           max_T, # max G_small value\n    'num_steps':         num_steps,\n    'min_val':           min_T, # min G_small value\n    'num_cycles':        num_cycles,\n    'k_decay':           k_decay,\n    'num_warmup_steps':  num_warmup_steps,\n    'warmup_init_val':   warmup_init_val,\n}\n\nEvery new, incremental schedule will start from these shared dictionaries. Then, a single parameter is changed at a time.\nThe cos_harness below gives us an easy way of making these minimum-pair changes.\n\ndef cos_harness(new_params={}, default_params={}):\n    '''Creates cosine schedules with updated parameters in `new_params`\n    '''\n    # start from the given baseline `cos_params`\n    cos_params = dict(default_params)\n    # update the schedule with any new parameters\n    cos_params.update(new_params)\n    \n    # return the new cosine schedule\n    sched = get_cos_sched(**cos_params)\n    return sched"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#plotting-the-cosine-schedules",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#plotting-the-cosine-schedules",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Plotting the Cosine Schedules",
    "text": "Plotting the Cosine Schedules\nNow we create the different Cosine schedules that will be swept.\n\ncos_params = {\n    'num_warmup_steps': [5, 10],\n    'num_cycles':       [1, 1.5, 2],\n    'k_decay':          [0.7, 2],\n    'max_val':          [8, 10, 12],\n    'min_val':          [2, 3],\n}\n\n# create the cosine experiments\ncos_func  = partial(cos_harness, default_params=DEFAULT_COS_PARAMS)\ncos_expts = create_expts(cos_params, cos_func)\n\n\nplot_grid([o['schedule'] for o in cos_expts], rows=4, titles=[o['title'] for o in cos_expts])\n\n\n\n\nWe repeat the steps above to create the T-Norm experiments\n\nT_params = {\n    'num_warmup_steps': [5, 10],\n    'num_cycles':       [1, 1.5, 2],\n    'k_decay':          [0.7, 2],\n    'max_val':          [0.1, 0.2, 0.3],\n    'min_val':          [0.01, 0.1],\n}\n\n# create the T-norm cosine experiments\nT_func  = partial(cos_harness, default_params=DEFAULT_T_PARAMS)\nT_expts = create_expts(T_params, T_func)\n\nWe also plot the T-Norm schedules below. Note that we are trying a few max and min values.\n\nplot_grid([o['schedule'] for o in T_expts], rows=4, titles=[o['title'] for o in T_expts])"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#running-the-normalized-cosine-experiments",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#running-the-normalized-cosine-experiments",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Running the normalized cosine experiments",
    "text": "Running the normalized cosine experiments\nNext we sweep the schedules for each type of normalization."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-runs",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "BaseNorm runs",
    "text": "BaseNorm runs\n\nprint('Running the BaseNorm experiments...')\nbase_norm_cos_res = run(prompt, cos_expts, guide_tfm=BaseNormGuidance)\n\nRunning the BaseNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.BaseNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"k_decay\", val=0.7...\nRunning experiment [2 of 12]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 12]: Param: \"max_val\", val=8...\nRunning experiment [4 of 12]: Param: \"max_val\", val=10...\nRunning experiment [5 of 12]: Param: \"max_val\", val=12...\nRunning experiment [6 of 12]: Param: \"min_val\", val=2...\nRunning experiment [7 of 12]: Param: \"min_val\", val=3...\nRunning experiment [8 of 12]: Param: \"num_cycles\", val=1...\nRunning experiment [9 of 12]: Param: \"num_cycles\", val=1.5...\nRunning experiment [10 of 12]: Param: \"num_cycles\", val=2...\nRunning experiment [11 of 12]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [12 of 12]: Param: \"num_warmup_steps\", val=10...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-runs",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "T-Norm runs",
    "text": "T-Norm runs\n\nprint('Running the T-Norm experiments...')\nt_norm_cos_res = run(prompt, T_expts, guide_tfm=TNormGuidance)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"k_decay\", val=0.7...\nRunning experiment [2 of 12]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.1...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.2...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.3...\nRunning experiment [6 of 12]: Param: \"min_val\", val=0.01...\nRunning experiment [7 of 12]: Param: \"min_val\", val=0.1...\nRunning experiment [8 of 12]: Param: \"num_cycles\", val=1...\nRunning experiment [9 of 12]: Param: \"num_cycles\", val=1.5...\nRunning experiment [10 of 12]: Param: \"num_cycles\", val=2...\nRunning experiment [11 of 12]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [12 of 12]: Param: \"num_warmup_steps\", val=10...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-runs",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "FullNorm runs",
    "text": "FullNorm runs\n\nprint('Running the FullNorm experiments...')\nfull_norm_cos_res = run(prompt, T_expts, guide_tfm=FullNormGuidance)\n\nRunning the FullNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"k_decay\", val=0.7...\nRunning experiment [2 of 12]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.1...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.2...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.3...\nRunning experiment [6 of 12]: Param: \"min_val\", val=0.01...\nRunning experiment [7 of 12]: Param: \"min_val\", val=0.1...\nRunning experiment [8 of 12]: Param: \"num_cycles\", val=1...\nRunning experiment [9 of 12]: Param: \"num_cycles\", val=1.5...\nRunning experiment [10 of 12]: Param: \"num_cycles\", val=2...\nRunning experiment [11 of 12]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [12 of 12]: Param: \"num_warmup_steps\", val=10...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-results",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "BaseNorm results",
    "text": "BaseNorm results"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-results",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "T-Norm results",
    "text": "T-Norm results"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-results",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "FullNorm results",
    "text": "FullNorm results"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#analysis",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#analysis",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Analysis",
    "text": "Analysis\nThere are many images and parameter changes going on.\nBroadly speaking, across normalizations, the following schedules show the most promise:\n\nChanging k-decay.\n\nAllowing for some warmup steps.\n\nIncreasing the maximum value of \\(G\\).\n\nAllow the cosine to go through more cycles.\n\nThe other changes either have negligible gains or actively corrupted the image."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "",
    "text": "Introducing two helper libraries to run dynamic Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html#motivation",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html#motivation",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "Motivation",
    "text": "Motivation\nThe initial experiments had a lot of boilerplate and repeated code.\nFor example, the same code was used in multiple notebooks to load Stable Diffusion models. The code for guidance schedules and normalizations was also repeated across notebooks.\nThat meant that each notebook needed a lot of overhead before it got to the actual experiments.\nTo make life a bit easier, and because we hope that these ideas are broadly usable, this repeated code was moved to two libraries:\n\nmin_diffusion\ncf_guidance\n\nNow we can import these libraries and jump straight to the important part: running the guidance experiments."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-min_diffusion-library",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-min_diffusion-library",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "The min_diffusion library",
    "text": "The min_diffusion library\nIn this section we generate an image using min_diffsion.\n\nfrom min_diffusion.core import MinimalDiffusion\n\n2022-11-22 15:42:08.507717: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n\n\n\nLoading the openjourney model from Prompt Hero\nThe following code load the openjourney Stable Diffusion model on the GPU, in torch.float16 precision.\n\nmodel_name = 'prompthero/openjourney'\ndevice     = 'cuda'\ndtype      = torch.float16\n\n\npipeline = MinimalDiffusion(model_name, device, dtype)\n\n\npipeline.load();\n\nEnabling default unet attention slicing.\n\n\n\n\nGenerating an image\nNext we use the familiar prompt to generate an image:\n\nâ€œa photograph of an astronaut riding a horseâ€\n\n\n\n\n\n\n\nNote\n\n\n\nThe openjourney model was fine-tuned to create images in the style of Midjourney v4.\nTo enable this fine-tuned style, we have to add the keyword \"mdjrny-v4\" at the start of the prompt.\n\n\n\n# text prompt for image generations\nprompt = \"mdjrny-v4 style a photograph of an astronaut riding a horse\"\n\n\n# set the seed for reproducibility\ntorch.manual_seed(2147483647);\n\n\n# generate the image\nimg = pipeline.generate(prompt);\n\nUsing the default Classifier-free Guidance.\n\n\n\n\n\n\n# display the generated image\nimg\n\n\n\n\nThatâ€™s the entire process!\nThe main difference between MinimalDiffusion and the HuggingFace API is that now we can easily customize the image generation loop. This allows us to explore a wide range of dynamic Classifier-free Guidances."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-cf_guidance-library",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-cf_guidance-library",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "The cf_guidance library",
    "text": "The cf_guidance library\nThe sections below are based on the cf_guidance documentation.\nWe create a few Cosine schedules and plug them into different Classifier-free Guidances.\nThe schedule parameter come from the initial post on dynamic Classifier-free Guidance.\n\nfrom cf_guidance.schedules import get_cos_sched\n\n\n# Parameters from the blog post\n# https://enzokro.dev/blog/posts/2022-11-15-guidance-expts-1/\nmax_val = 7.5\nmin_val = 0.15\nnum_steps = 50\nnum_warmup_steps = 5\n\n# 1) Baseline cosine schedule\ncos_params = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_warmup_steps':  0,\n}\n\n# 2) Cosine schedule with warmup \nwarmup_cos_params = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'warmup_init_val':   min_val + 1., # to show we can offset the warmup relative to min\n    'num_warmup_steps':  num_warmup_steps,\n}\n\n# create the schedules\ncos_g = get_cos_sched(**cos_params)\nwarmup_g = get_cos_sched(**warmup_cos_params)\n\nLetâ€™s plot these cosine schedules to see what they look like.\n\n# plot the schedules\nplt.plot(cos_g)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Cosine Schedule');\n\n\n\n\n\nplt.plot(warmup_g)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Warmup Cosine Schedule');\n\n\n\n\n\nCreating Guidance Normalizers\nNow we can use these schedules during Classifier-free Guidance. The Guidance Transform class, GuidanceTfm, makes this possible.\nGuidance transforms take one initialization parameter: schedules. This is a map from parameter names to an array-like, indexable sequence of values.\nFor a given parameter name at diffusion timestep idx, the value of schedules[name][idx] should be the parameterâ€™s scheduled value at the given timestep.\nIn this case we call the guidance parameter \\(G\\) as a lowercase \\(g\\).\n\nfrom cf_guidance.transforms import GuidanceTfm, BaseNormGuidance\n\n\n# create the `schedules` parameter\nexample_schedules = {'g': cos_g}\n\n# Create a Guidance with cosine schedule.\nguidance = GuidanceTfm(example_schedules)\n\n# Normalized Guidance with a cosine schedule.\nnorm_guidance = BaseNormGuidance(example_schedules)\n\n\n\nUsing the transforms in a Diffusion pipeline\nThe following snippet shows where and how the Guidance Transforms are used in a diffusion loop.\nWe use the norm_guidance example class created above. Specifically, we call norm_guidance with the following arguments:\n\nThe unconditioned noise predictions.\n\nThe conditional noise predictions.\n\nThe index of the current timestep.\n\nThe code is borrowed from HuggingFaceâ€™s official StableDiffusionPipeline to show where norm_guidance should go.\nThis seems like a good starting point, since many scripts and functions are based on this HuggingFace setup.\n    # inside of `StableDiffusionPipeline`\n    \n    for i, t in enumerate(self.progress_bar(timesteps_tensor)):\n        # expand the latents if we are doing classifier free guidance\n        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        # NOTE: our transforms go here:\n        ###############################\n        if do_classifier_free_guidance:\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n\n            ## OLD UPADTE\n            #noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            # NEW cf_guidance UPDATE\n            noise_pred = norm_guidance(noise_pred_uncond, noise_pred_text, i)\n\n\nCreating more complex schedules\nOur cosine scheduler is based on a combination of the schedulers in timm and HuggingFace.\nIt has a variety of parameters to support many schedule combinations as shown below.\n\n# cosine schedule with a full cycle\nfull_cycle = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        1,\n    'num_warmup_steps':  0,\n}\n\n# cosine schedule with k-decay\nk_decay_cos = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        1.5,\n    'k_decay':           0.7,\n    'warmup_init_val':   min_val + 1., # to show we can offset the warmup value\n    'num_warmup_steps':  num_warmup_steps,\n}\n\n# create the schedules\nfull_cycle_sched = get_cos_sched(**full_cycle)\nk_decay_sched = get_cos_sched(**k_decay_cos)\n\n\nplt.plot(full_cycle_sched)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Cosine With a Full Cycle');\n\n\n\n\n\nplt.plot(k_decay_sched)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Cosine with Offset-Warmup, 1.5 Cycles, and K-decay');"
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html",
    "title": "A PyTorch SLERP implementation",
    "section": "",
    "text": "SLERP implemented in PyTorch with proper thresholding."
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html#why-do-we-need-slerp",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html#why-do-we-need-slerp",
    "title": "A PyTorch SLERP implementation",
    "section": "Why do we need SLERP?",
    "text": "Why do we need SLERP?\n\n\n\n\n\n\nNote\n\n\n\nIf you donâ€™t need the theory, you can skip straight to the code.\n\n\nSLERP interpolates two vectors while keeping their magnitudes intact. Why would this be important for Diffusion models?\nThe reason has to do with how Gaussian distributions behave in higher dimensions. This blog post by Ferenc HuszÃ¡r has an excellent description of how exactly our intuitions fall apart in high dimensions. The post also has many good visualizations to drive the point home.\n\nGaussians in high dimensions\nTo summarize Ferencâ€™s blog post: a Gaussian in high dimensions is fundamentally different than its 1-D â€œBell curveâ€ version.\nAs we climb to higher dimensions the Gaussian distribution becomes a thin, hollow shell. Its probability density spreads out around this thin shell. Think about how different that is to a 1-D Gaussian. In the 1-D case, most of the density falls within a few standard deviations of the mean.\nBefore long, the inside of this high-dimensional Gaussian is empty. Only its thin shell has any probability at all. Borrowing Ferencâ€™s excellent analogy: the distribution turns into a â€œsoap bubbleâ€.\nRecall that most Diffusion models are based on high-dimensional Gaussians. That means that, in Diffusion, we are actually dealing with many high-dimensional soap bubbles. If we treat them like regular 2-D or 3-D vectors, our intuitions will fail us.\n\n\nOk, so where does SLERP come in?\nIf we linearly interpolate two high-dimensional Gaussians, the result can easily fly away from the soap bubbleâ€™s surface. The section below has an example of what this looks like in 2-D space.\nSLERP makes it possible to properly interpolate Diffusion vectors by keeping us firmly grounded on the surface of the soap bubble."
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html#what-about-linear-interpolation",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html#what-about-linear-interpolation",
    "title": "A PyTorch SLERP implementation",
    "section": "What about linear interpolation?",
    "text": "What about linear interpolation?\nRegular linear interpolation (sometimes called LERP) is a powerful tool. It is a cornerstone in modern computer graphics to move an object between two points.\nLERP has a loose analogy with gravity: the shortest distance between two points is a straight line.\nFor example, imagine you are drinking a cup of coffee. The mug is currently on the table. As you go to take a sip, you pick up the mug and bring it directly to your lips. You wouldnâ€™t swing your arm around in a weird way. That would only be more work and delay the sip of coffee.\nIn other words, when moving objects in our 3-D world we want to do the least amount of work possible. That is what LERP does in 2-D and 3-D space. In a manner of speaking, you used LERP to bring the coffee mug to your lips and take a sip.\nThis coffee example brings us back to why we need SLERP in the first place. Our notions of 3-D paths break down in higher dimensions, and LERP does not work as intended. Here we are much better served by SLERP.\n\nA concrete LERP example\nLetâ€™s show how linear interpolation works on vectors.\nFor this example we will use the familiar \\(x\\) and \\(y\\) basis vectors. We also draw the Unit Circle for reference.\n\n\n\n\n\n\nNote\n\n\n\nThe plotting function plot_vectors is available in the postâ€™s notebook. It is omitted here for space.\n\n\n\nimport torch\n\n# use the X and Y unit vectors as an example\nxhat = torch.tensor([1, 0]).float()\nyhat = torch.tensor([0, 1]).float()\n\n\n# plot the basis vectors, with a unit circle outline\nfig = plot_vectors(xhat, yhat, labels=['$\\hat{x}$', '$\\hat{y}$'], draw_unit_circle=True)\n\nplt.xlabel('X Axis', fontsize='x-large')\nplt.ylabel('Y Axis', fontsize='x-large')\nplt.title('Basis Vectors on the Unit Circle', fontsize='xx-large', pad=10);\n\n\n\n\nWhat happens if we linearly interpolate (LERP) these vectors to their midpoint?\n\n# use linear interpolation to find the midpoint\np_lerp = (xhat + yhat) / 2\n\n\n# plotting the LERP of basis vectors x and y\nfig = plot_vectors(xhat, yhat, p_lerp, labels=['$\\hat{x}$', '$\\hat{y}$', 'P_lerp'])\n\nplt.xlabel('X Axis', fontsize='x-large')\nplt.ylabel('Y Axis', fontsize='x-large')\nplt.title('Linear Interpolation of Unit Vectors', fontsize='xx-large', pad=10);\n\n\n\n\nIf we only cared about getting from \\(\\hat{y}\\) to \\(\\hat{x}\\) then we are on the right track. LERP is following the shortest possible path.\nBut imagine if the Unit Circle was like a slice of a high-dimensional Gaussian. In that case, linear interpolation has moved us away from the surface of the soap bubble!\nIf we were dealing with a 1-D Gaussian, itâ€™s as if we have moved very far from the mean. Imagine going out \\(+10\\) \\(\\sigma\\) away. That would obviously be an incredibly unlikely sample. And that is exactly where the \\(P_\\text{LERP}\\) vector ends up.\nWith SLERP, we can still interpolate the vectors while also staying firmly anchored to the soap bubble."
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html#slerp-interpolation-of-the-unit-vectors",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html#slerp-interpolation-of-the-unit-vectors",
    "title": "A PyTorch SLERP implementation",
    "section": "SLERP interpolation of the unit vectors",
    "text": "SLERP interpolation of the unit vectors\nWhat happens if we instead use SLERP to interpolate the unit vectors?\n\n# SLERP the unit vectors to their midpoint\np = slerp(xhat, yhat, 0.5)\n\n\n# plot the SLERP iterpolated vector\nfig = plot_vectors(xhat, yhat, p, labels=['$\\hat{x}$', '$\\hat{y}$', \"P_slerp\"])\nplt.xlabel('X Axis', fontsize='x-large')\nplt.ylabel('Y Axis', fontsize='x-large')\nplt.title('SLERP on Unit Vectors to their midpoint P', fontsize='xx-large', pad=10);\n\n\n\n\nThat looks much better!\nIf the Unit Circle was like a Gaussian soap bubble, then weâ€™ve properly moved along its film."
  },
  {
    "objectID": "blog/posts/2022-11-08-merge-n-BSTs/index.html",
    "href": "blog/posts/2022-11-08-merge-n-BSTs/index.html",
    "title": "Merging an arbitrary number of Binary Trees",
    "section": "",
    "text": "Using functional python tools to merge several Binary Trees together."
  },
  {
    "objectID": "blog/posts/2022-11-08-merge-n-BSTs/index.html#the-intuition-to-merge-two-binary-trees.",
    "href": "blog/posts/2022-11-08-merge-n-BSTs/index.html#the-intuition-to-merge-two-binary-trees.",
    "title": "Merging an arbitrary number of Binary Trees",
    "section": "The intuition to merge two Binary Trees.",
    "text": "The intuition to merge two Binary Trees.\nThe general intuition to solve this problem is:\n\nOverlay the two trees together, starting from their root nodes.\n\nThen, merge the values of the root nodes.\n\nFinally, merge both the left and and right subtrees in the same way.\n\nWhat will these steps look like in code?"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html",
    "href": "blog/posts/2022-08-20-spec-norms/index.html",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "",
    "text": "Scaling spectrograms for classification tasks with neural networks."
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#a-quick-note-on-transfer-learning",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#a-quick-note-on-transfer-learning",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "A quick note on Transfer Learning",
    "text": "A quick note on Transfer Learning\nWe also have to talk about Transfer Learning in the context of normalization. In Transfer Learning, it is best-practice to normalize the new dataset with the statistics from the old dataset. This makes sure that the new network inputs are at the same scale as the original inputs. Since most pretrained vision models were trained on ImageNet, we normalize any new inputs with ImageNet statistics.\nHowever, we avoid Transfer Learning in this post and instead train an 18-layer xResNet from scratch. The reason is that pretrained image models operate at a completely different scale than spectrograms. And the main goal here is to learn our own scalings instead!"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#the-esc-50-dataset",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#the-esc-50-dataset",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "The ESC-50 dataset",
    "text": "The ESC-50 dataset\nThe first step is to download the data. ESC-50 is already included in fastaudio so we can grab it with untar_data.\n\n\nCode\n# from fastai.vision.all import *\n# from fastaudio.core.all import *\n# from fastaudio.augment.all import *\n\n# already in fastaudio, can download with fastai's `untar_data`\n# path = untar_data(URLs.ESC50)\n\n\nThe downloaded audio files are inside the aptly named audio folder. Below we use the ls method, a fastai addition to pythonâ€™s pathlib.Path, to check the contents of this folder.\n\n\nCode\n# wavs = (path/\"audio\").ls()\n# wavs\n\n\nThe output of ls shows 2,000 audio files. But the filenames are not very descriptive, so how do we know what is actually in each one?\nThankfully, as with many datasets, the download includes a table with more information about the data (aka metadata).\n\n\nCode\n# # read the audio metadata and show the first few rows\n# df = pd.read_csv(path/\"meta\"/\"esc50.csv\")\n# df.head()\n\n\nThe key info from this table are in the filename and category columns.\nfilename gives the name of a file inside of the audio folder.\ncategory tells us which class a file belongs to.\nThe last file in the data directory will be our working example for normalization. We can index into the metadata table above using this fileâ€™s name to learn more about it.\n\n\nCode\n# # pick the row where \"filename\" matches the file's \"name\".\n# df.loc[df.filename == wavs[-1].name]\n\n\nThis is a recording of crickets!\nWe can load this file with the AudioTensor class in fastaudio. Its create function reads the audio samples straight into a torch.Tensor.\n\n\nCode\n# # create an AudioTensor from a file path\n# sample = AudioTensor.create(wavs[-1])\n\n\nAn AudioTensor can plot and even play the audio with its show method.\n\n\nCode\n# print(f'Audio shape [channels, samples]: {sample.shape}')\n# sample.show();\n\n\nEach â€œburstâ€ in the plot above is a cricket chirp. There are three full chirps and the early starts of a fourth chirp."
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#building-the-dataset-loader",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#building-the-dataset-loader",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Building the dataset loader",
    "text": "Building the dataset loader\nThe setup below follows the fastaudio ESC-50 baseline to step through the training dataset. It is worth mentioning that the files in ESC-50 are sampled 44.1 kHz, but fastaudio will resample them to 16 kHz by default. Downsampling like this risks throwing away some information. But, keeping the higher sampling rate almost triples the â€œwidthâ€ (aka time) of the spectrogram. This larger image will take up more memory in the GPU and limits our batch size and architecture choices. We keep this downsampling since it gives the spectrograms a very reasonable shape of [201, 401], compared with the much larger shape of [201, 1103] if we donâ€™t downsample.\n\n\nCode\n# def CrossValidationSplitter(col='fold', fold=1):\n#     \"Split `items` (supposed to be a dataframe) by fold in `col`\"\n#     def _inner(o):\n#         assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n#         col_values = o.iloc[:,col] if isinstance(col, int) else o[col]\n#         valid_idx = (col_values == fold).values.astype('bool')\n#         return IndexSplitter(mask2idxs(valid_idx))(o)\n#     return _inner\n\n# auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n#                  get_x=ColReader(\"filename\", pref=path/\"audio\"), \n#                  splitter=CrossValidationSplitter(fold=1),\n#                  item_tfms = [AudioNormalize],\n#                  batch_tfms = [audio2spec],\n#                  get_y=ColReader(\"category\"))\n# dbunch = auds.dataloaders(df, bs=64)\n# dbunch.show_batch(figsize=(7,7))"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#calculating-the-statistics",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#calculating-the-statistics",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Calculating the statistics",
    "text": "Calculating the statistics\nNext we make two recorders: one for global statistics and the other for channel-based statistics. Then we step through the training dataset to find both sets of stats.\n\n\nCode\n# # create recorders\n# global_stats  = StatsRecorder()\n# channel_stats = StatsRecorder(red_dims=(0,1,3))\n\n# # step through the training dataset\n# with torch.no_grad():\n#     for idx,(x,y) in enumerate(iter(dbunch.train)):\n#         # update normalization statistics\n#         global_stats.update(x)\n#         channel_stats.update(x)\n    \n# # parse out both sets of stats\n# global_mean,global_std = global_stats.mean,global_stats.std\n# channel_mean,channel_std = channel_stats.mean,channel_stats.std\n\n\nWe can check the shape of the statistics to make sure they are correct. For the global statistics, we expect a shape of: [1,1,1,1]. With spectrogram channel normalizations, we expect one value per spectrogram bin for a shape of [1,1,201,1].\n\n\nCode\n# print(f'Shape of global mean: {global_mean.shape}')\n# print(f'Shape of global standard dev: {global_std.shape}')\n\n\n\n\nCode\n# print(f'Shape of channel mean: {channel_mean.shape}')\n# print(f'Shape of channel standard dev: {channel_std.shape}')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#transforms-to-normalize-mini-batches",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#transforms-to-normalize-mini-batches",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Transforms to normalize mini-batches",
    "text": "Transforms to normalize mini-batches\nWe need to extend the fastai Normalize class in order to use the spectrogram normalization statistics. The reason is type dispatch. fastai normalization uses ImageNet statistics due to the focus on transfer learning with color images. But this ImageNet normalization is only applied on RGB images of the TensorImage class, while AudioSpectrogram subclasses the different TensorImageBase. The solution is to define encodes and decodes for TensorImageBase instead.\n\n\nCode\n# class SpecNormalize(Normalize):\n#     \"Normalize/denorm batch of `TensorImage`\"\n#     def encodes(self, x:TensorImageBase): return (x-self.mean) / self.std\n#     def decodes(self, x:TensorImageBase):\n#         f = to_cpu if x.device.type=='cpu' else noop\n#         return (x*f(self.std) + f(self.mean))\n\n\n\n\nCode\n# # make global and channel normalizers\n# GlobalSpecNorm  = SpecNormalize(global_mean,  global_std,  axes=(0,2,3))\n# ChannelSpecNorm = SpecNormalize(channel_mean, channel_std, axes=(0,1,3))"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#training-helpers",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#training-helpers",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Training helpers",
    "text": "Training helpers\nTo avoid repeating ourselves, the helper functions below build the dataloaders and run the training loops.\nThe get_dls function makes it clear which normalization is being applied. The train_loops function repeats training runs a given number of times.\n\n\nCode\n# def get_dls(bs=64, item_tfms=[], batch_tfms=[]):\n#     \"Get dataloaders with given `bs` and batch/item tfms.\"\n#     auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n#                      get_x=ColReader(\"filename\", pref=path/\"audio\"), \n#                      splitter=CrossValidationSplitter(fold=1),\n#                      item_tfms=item_tfms,   # for waveform normalization\n#                      batch_tfms=batch_tfms, # for spectrogram normalization\n#                      get_y=ColReader(\"category\"))\n#     dls = auds.dataloaders(df, bs=bs)\n#     return dls\n\n# def make_xresnet_grayscale(model, n_in=1):\n#     \"Modifies xresnet `model` for single-channel images.\" \n#     model[0][0].in_channels = n_in\n#     # sum weights to reduce dimension\n#     model[0][0].weight = torch.nn.parameter.Parameter(model[0][0].weight.mean(1, keepdim=True))\n\n# def train_loops(dls, name, num_runs=num_runs, epochs=epochs, num_cls=50):\n#     \"Runs `num_runs` training loops with `dls` for given `epochs`.\"\n#     accuracies = []\n#     for i in range(num_runs):\n#         # make new grayscale xresnet\n#         model = xresnet18(pretrained=False, n_out=num_cls)\n#         make_xresnet_grayscale(model, n_in=1)\n#         # get learner for this run\n#         learn = Learner(dls, model, metrics=[accuracy])\n#         # train network and track accuracy\n#         learn.fit_one_cycle(epochs)\n#         accuracies.append(learn.recorder.values[-1][-1])\n#     print(f'Average accuracy for \"{name}\": {sum(accuracies) / num_runs}')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#baseline-performance",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#baseline-performance",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Baseline performance",
    "text": "Baseline performance\nBefore getting carried away with normalization, we have to first set a baseline without normalizations. This allows us to evaluate the impact of normalization later on, else there is no way to know if normalization helps at all.\n\n\nCode\n# # data without normalization\n# dls = get_dls(batch_tfms=[audio2spec])\n# # run training loops\n# train_loops(dls, name='No Norm')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-global-normalization",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-global-normalization",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Performance with global normalization",
    "text": "Performance with global normalization\nNext we normalize each audio waveform and the spectrograms with global, scalar statistics.\n\n\nCode\n# # data with waveform and global normalization\n# dls = get_dls(item_tfms=[AudioNormalize],\n#               batch_tfms=[audio2spec, GlobalSpecNorm])\n# # run training loops\n# train_loops(dls, name='Global Norm')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-channel-normalization",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-channel-normalization",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Performance with channel normalization",
    "text": "Performance with channel normalization\nFinally, we normalize each audio waveform and the spectrograms with channel-based statistics.\n\n\nCode\n# # get data with waveform and channel normalization\n# dls = get_dls(item_tfms=[AudioNormalize],\n#               batch_tfms=[audio2spec, ChannelSpecNorm])\n# # run training loops\n# train_loops(dls, name='Channel Norm')"
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Fractal LLMs",
    "section": "",
    "text": "Running llama.cpp on a laptop and phone\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\n\n\n\n\n\n\nOverview of MLC and llama.cpp\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\n\n\n\n\n\n\nA Python Environment for LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\nBlogging with Quarto and nbdev\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "chaski",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nRunning llama.cpp on a laptop and phone\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nOverview of MLC and llama.cpp\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nA Python Environment for LLMs\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nBlogging with Quarto and nbdev\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nnbdev\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nTips for LLM prompts\n\n\n\n\n\n\n\nLLM\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion v2 with dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 7\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 6\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 5\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 4\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 3\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 2\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nLibraries for dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 1\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nA PyTorch SLERP implementation\n\n\n\n\n\n\n\ndiffusion\n\n\nlatent interpolation\n\n\nSLERP\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nIntro to normalizing and scheduling Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nMerging an arbitrary number of Binary Trees\n\n\n\n\n\n\n\nBinary Tree\n\n\nalgorithms\n\n\nfunctional\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nComplex Rayleigh Weight Initializations\n\n\n\n\n\n\n\ndeep learning\n\n\ncomplex networks\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nNormalizing spectrograms for Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nspectrogram normalizations\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nenzokro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chaski",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nRunning llama.cpp on a laptop and phone\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nOverview of MLC and llama.cpp\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nA Python Environment for LLMs\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nBlogging with Quarto and nbdev\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nnbdev\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nTips for LLM prompts\n\n\n\n\n\n\n\nLLM\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion v2 with dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 7\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 6\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 5\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 4\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 3\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 2\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nLibraries for dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 1\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nA PyTorch SLERP implementation\n\n\n\n\n\n\n\ndiffusion\n\n\nlatent interpolation\n\n\nSLERP\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nIntro to normalizing and scheduling Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nMerging an arbitrary number of Binary Trees\n\n\n\n\n\n\n\nBinary Tree\n\n\nalgorithms\n\n\nfunctional\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nComplex Rayleigh Weight Initializations\n\n\n\n\n\n\n\ndeep learning\n\n\ncomplex networks\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nNormalizing spectrograms for Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nspectrogram normalizations\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nenzokro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "",
    "text": "Creating complex-valued Rayleigh initializations for neural networks."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-numbers-a-brief-recap",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-numbers-a-brief-recap",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Complex Numbers: A brief recap",
    "text": "Complex Numbers: A brief recap\n Complex numbers have two components:\n- A real part.\n- An imaginary part.\nThe real component is a regular number like we would find on a plain number line. The imaginary component exists along the i axis.\nTo keep things simple, we can think of these numbers on a two-dimensional plot. The real number is on the x-axis while the imaginary number is on the y-axis.\n\nStarting with a real number\nPlotting examples is a great way to make things concrete. We first plot a regular, real number that we are all familiar with: \\(x = 2\\)\n\n\n\n\n\n\n\nMagnitude of a real number\nThe distance from the origin to our number tells us its magnitude. With positive values this feels redundant, since the magnitude is always the number itself.\nBut what about negative numbers? That is where the absolute value, represented as \\(|x|\\), comes into play. If we had picked \\(x = -2\\) instead, the magnitude would still be the same: \\(|-2| = |2| = 2\\).\nSo for any real number, positive or negative, we can find its magnitude by drawing an arrow starting from the origin \\(0\\). The absolute length of the arrow will be the numberâ€™s magnitude.\nWhy are we spelling out this aspect of numbers so much? That will become clear when we introduce the imaginary component next.\n\n\nAdding an imaginary component\nWe will keep our real component the same: \\(x = 2\\).\nBut now, letâ€™s an imaginary component: \\(y = 3\\), to turn it into a complex number.\nWhat does this new complex number look like? We can visualize it on a 2D plot:\n\n\n\n\n\nWe combined these two components to get a complex number! Letâ€™s call this number \\(z\\).\n\\(z\\) will be defined as: \\(z = x + iy\\)\nThe â€œ\\(i\\)â€ next to a number means that it is the imaginary component.\n\n\nMagnitude of a complex number\nWhile we could use the real and imaginary components, there is another representation of complex numbers that will be more useful to us. This other representation is the magnitude and phase of a complex number.\nRemember how for a real number, its magnitude was the length of an arrow starting from the origin? The same idea applies to complex numbers. With one new detail: we have two components now, so our arrowâ€™s length will be different.\nLetâ€™s first draw our new complex number as an arrow.\n\n\n\n\n\nThe formula to compute the magnitude of a complex number \\(z\\) is:\n\\[|z| = \\sqrt{x^{2} + y^{2}}\\]\nPlugging in our \\(x\\) and \\(y\\) values gives our complex \\(z\\) a magnitude of:\n\\[|z| = \\sqrt{2^{2} + 3^{2}} = \\sqrt{4 + 9} = \\sqrt{13}\\]\nWhile knowing the magnitude is important, it is not enough to fully describe \\(z\\). For example what if instead of \\((x = 2, y = 3)\\) we had swapped them around as \\((x = 3, y = 2)\\). If we plug these values into the magnitude equation we get back the exact same number \\(\\sqrt{13}\\).\nBut looking at our 2D plots, these swapped points would obviously be in different locations. So if we were given only the magnitude, how could we tell that it came from our true, original \\(z\\)?\n\n\nPhase: telling complex magnitudes apart from each other\nThe way to tell two complex numbers with the same magnitude apart lies in the fact that the arrows are no longer flat along the x-axis.\nInstead they are now elevated (â€œpulled upâ€) by the imaginary component \\(y = 3\\). The complex number now has an angle respective to the x-axis.\nThis angle, together with a magnitude, is enough to perfectly describe our complex \\(z\\). In other words: we know both how long to make the vector and where to point it.\nLetâ€™s complete the picture by including the angle of \\(z\\):\n\n\n\n\n\nWe use \\(\\theta\\) to represent the angle. The formula to compute \\(\\theta\\) is:\n\\[\\theta = \\arctan{\\frac{y}{x}}\\]\nPlugging in \\(x\\) and \\(y\\) for our complex number \\(z\\) gives us an angle of:\n\\[\\theta = \\arctan{\\frac{3}{2}} = 56.31 ^\\circ\\]\nWith the phase and magnitude, we now have a unique way of representing our complex number \\(z\\)."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#recap-complex-numbers",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#recap-complex-numbers",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Recap: Complex Numbers",
    "text": "Recap: Complex Numbers\nIn this section, we gave a brief overview of complex numbers and their representation. To make things concrete, we picked a complex number \\(z\\) with a real component \\(x = 2\\) and an imaginary component \\(y = 3\\).\nThen, we showed that we can perfectly represent this complex number \\(z\\) with two pieces of information: its magnitude and its phase.\n\nMagnitude: the length of a vector.\n\nPhase: the angle, or direction, where a vector is pointing."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#background-on-neural-network-initializations",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#background-on-neural-network-initializations",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Background on neural network initializations",
    "text": "Background on neural network initializations\nWhile initializations are now taken for granted, they were part of the first key pieces that made it possible to train deep neural networks. Before we knew how to properly initialize networks, training was very unstable as the gradients would either diverge or collapse to 0. This is known as gradient explosion or vanishing, respectively.\nThe main insights to prevent gradients from vanishing or exploding came from analyzing their variance during training.\n&gt; Aside: this is still an important error analysis tool! Looking at the behavior and distribution of gradients is a surefire way to catch problems with the training. Especially during the earliest optimizer steps.\n\nAchieving smooth gradient flows\nIt was the seminal work by He and Glorot, Bengio that showed how to control the variance of gradients to make sure that training was successful. They found that the variance of the sampling distributions, either Normal or Uniform, must meet certain criteria for the gradients to flow â€œsmoothlyâ€.\nHere, â€œsmoothlyâ€ means that the gradients neither disappear nor explode during training.\nThe initializations derived in these papers are now the defaults in popular deep learning libraries like TensorFlow and pytorch.\nUnfortunately, the theory of complex-valued neural networks is not as well established. How can we know what are good variances and distributions for complex weights?\nIt turns out we can borrow these hard-earned lessons about good real-valued initializations to make sure that our complex gradients flow smoothly."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-complex-magnitudes",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-complex-magnitudes",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Initializing complex magnitudes",
    "text": "Initializing complex magnitudes\nInstead of drawing from a Normal or Uniform distribution, like we do for real-valued networks, the magnitudes will instead be drawn from a Rayleigh distribution. The reasons for this are described below. We can think of a Rayleigh distribution as the complex version of the familiar Normal distribution we use for real-valued weights."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-phases",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-phases",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Initializing phases",
    "text": "Initializing phases\nThe phases will be drawn from a Uniform distribution. To see why, think about a compass with 360 degrees to choose from.\nWe could randomly pick a degree and start walking in that direction for a given amount of time. Assuming we are on a flat surface, each degree choice will place us in a different, unique location.\nBecause we donâ€™t know which direction our learned complex weights should point in, the best we can do is to start by randomly pointing everywhere and letting the gradients steer the vectors instead."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#why-rayleigh",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#why-rayleigh",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Why Rayleigh?",
    "text": "Why Rayleigh?\nWhy do we choose the Rayleigh distribution? The reason is that, without having more information about what our complex magnitudes should be, it is the best, unbiased starting point for the network.\nIn other words, we pick the maximum entropy distribution to avoid a-priori biasing our network toward any particular outcome. One of the successes of Deep Learning has been that itâ€™s best to let the learning procedure figure out the values on its own in its higher dimensional activation feature space.\nThis is the complex-valued version of the same logic for using Normal or Uniform distribution to initialize real-valued networks."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#details-of-the-rayleigh-distribution",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#details-of-the-rayleigh-distribution",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Details of the Rayleigh distribution",
    "text": "Details of the Rayleigh distribution\nLetâ€™s dive into the details. The equation below is the Probability Density Function (PDF) of the Rayleigh distribution.\n\\[f(x,\\sigma) = \\frac{x}{\\sigma^2}e^{-x^2/(2\\sigma^2)}, \\ \\ x \\geq 0\\]\nThis equation is a bit intimidating in written form. Letâ€™s instead code it up as a python function with NumPy to make it cleaner.\n\n# start by importing the libraries we need\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom numpy.random import default_rng\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# define the Rayleigh PDF\ndef rayleigh_pdf(x, sigma):\n    \"Evaluates the Rayleigh PDF at a given point `x`.\"\n    p = (x / sigma**2) * np.exp(-x**2 / (2*sigma**2)) # see if you can match this code to the equation above\n    return p\n\nThe parameter sigma \\((\\sigma)\\) is known as the distributionâ€™s scale. It is commonly found in many probability distributions and often controls how spread out or narrow a distribution is.\nLet us start by setting \\(\\sigma = 1\\) to draw the â€œbasicâ€ Rayleigh shape. We will then change sigma to see how this affects the distributionâ€™s shape.\n\n# start with sigma of one as the base case\nsigma = 1\n\n# calculate the Rayleigh PDF on 100 equally spaced points between 0 and 5\npoints = np.linspace(0, 5, 100)\nray_pdf = rayleigh_pdf(points, sigma)  \n\n# setup the plot\nfig, ax = plt.subplots(figsize=(8,7), dpi=80)\nax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels()))))\nax.set_xlabel('Sample Value', fontsize='x-large')\nax.set_ylabel('Probability Density', fontsize='x-large')\nax.set_title('Rayleigh PDF', fontsize='xx-large');\n\n# plot the Rayleigh pdf\nax.plot(ray_pdf);\n\n\n\n\nAs we mentioned the scale \\(\\sigma\\) controls the width or narrowness of the distribution.\nLetâ€™s both halve and double sigma to (\\(\\frac{1}{2}, {2})\\) respectively to see what happens.\n\n# setup plot\nfig,ax = plt.subplots(figsize=(8,7), dpi=80)\nax.set_xlabel('Sample Value', fontsize='x-large')\nax.set_ylabel('Probability Density', fontsize='x-large')\nax.set_title('Rayleigh PDFs', fontsize='xx-large'); \n\n\n# different colors for each sigma\nsigmas = [0.5, 1, 2]\ncolors = ['m', 'b', 'r']\n\n# plot the distributions with different scales\nfor color,sig in zip(colors,sigmas):\n    rpdf = rayleigh_pdf(points, sig)\n    ax.plot(points, rpdf, c=color, label=f'Ïƒ: {sig}')\nax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels()))))\nax.legend();\n\n\n\n\nThe blue line in the plot above is the same PDF from our first plot where \\(\\sigma = 1\\).\nWe can see how \\((\\sigma = 0.5)\\) pulls the distribution up and to the left, while \\((\\sigma = 2)\\) squishes it down and to the right.\nIn other words, a smaller sigma makes our distribution narrower while a larger sigma makes it wider.\nPlotting the theoretical Rayleigh PDF only shows what the distribution should looks like. Next, we need to actually generate some Rayleigh values."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#magnitude-phase-vs.-real-imaginary",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#magnitude-phase-vs.-real-imaginary",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "(Magnitude, Phase) vs.Â (Real, Imaginary)",
    "text": "(Magnitude, Phase) vs.Â (Real, Imaginary)\nWe mentioned earlier that a complex number has real and imaginary components. But so far we have deal with magnitudes and phases instead. How are these quantities related?\nIt turns out that we can use the phase and magnitude to split our vector into its real and imaginary parts. The cosine of the phase and magnitude gives us the real part, and the sine of the phase gives us the imaginary part.\nThese are two different representations of the same complex number. We do not lose anything going from one to the other or vice-versa.\n\n# splitting our phases and magnitues into real and imaginary components\nreal = ray_vals * np.cos(phase)\nimag = ray_vals * np.sin(phase)\n\nIt turns out this will be a key detail when we are creating complex-valued network layers. As a preview: we will give one set of weights the real values, and another set of weights the imag values. This is because complex operations like addition and multiplication work better on GPUs with real and imaginary representations."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#visualizing-our-random-phases",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#visualizing-our-random-phases",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Visualizing our random phases",
    "text": "Visualizing our random phases\nNow we can check if these phases are truly orienting our magnitudes in random directions. To do so we plot the first 500 complex weights in the polar plane.\n\n\n# indexes for the first 500 random weights\nchosen_samples = range(500) \n\n# plot these first complex weights\nplt.figure(figsize=(8,7), dpi=80)\nfor idx in chosen_samples:\n\n    # index into phase and magnitude variables\n    angle,mag = phase[idx],ray_vals[idx]\n\n    # plot them starting from the origin\n    plt.polar([0,angle], [0,mag], marker='o')\n    \nplt.title('Magnitudes and Phases of our Complex Weights');\n\n\n\n\nThat definitely looks like a random, uniform orientation!"
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#he-and-glorot-criteria-for-rayleigh-distributions",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#he-and-glorot-criteria-for-rayleigh-distributions",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "He and Glorot criteria for Rayleigh distributions",
    "text": "He and Glorot criteria for Rayleigh distributions\nHow can we make sure our Rayleigh magnitudes meet the He and Glorot variance criteria?\nThe Complex Neural Nets paper from earlier includes a nice derivation for the variance of a complex Rayleigh distribution: \\[\\text{Var}(W) = 2\\sigma^{2}\\]\nWe can set the Rayleigh variance equal to the He and Glorot criteria and solve for sigma \\(\\sigma\\).\nTo meet the He criteria, sigma should be: \\[\\sigma_{\\text{He}} = \\frac{1}{\\sqrt{\\text{fanIn}}}\\] \nTo meet the Glorot criteria, sigma should be: \\[\\sigma_{\\text{Glorot}} = \\frac{1}{\\sqrt{\\text{fanIn + fanOut}}}\\] \n\nStarting with a simple one-layer network\nIn the previous sections we used a flat vector of complex weights for the examples and plots. Tying it back at our two concrete examples of wind speed and radio noise, itâ€™s as if we took a single series of measurements.\nBut since the He and Glorot criteria are defined for network layers, we need a new example. Letâ€™s start with to a simple one-layer network. Our layer will have 100 inputs and 50 outputs (fanIn = 100, fanOut = 50).\nPlugging these fanIn and fanOut values into the Rayleigh sigma criteria gives: \\[\\sigma_{\\text{He}} = \\frac{1}{10}\\]\n\\[\\sigma_{\\text{Glorot}} = \\frac{1}{5\\sqrt{6}}\\]\nNow we can pass either of these sigmas to our default_rng and it will draw Rayleigh samples with variances that match the chosen criteria.\n\nA quick word about fanIn and fanOut. We saw the simple feed-forward case with in our example for a single network layer. In that case the number of incoming connections was simply fanIn and the outgoing connections were fanOut.\n\n\nHowever, the convolutional case is a bit more complicated. A convolutional layer has input and output feature maps which are roughly analogous to input and output units in feed-forward layers. But they also have a kernel size to consider. PyTorch has a nice convenience function that handles this for us."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.linear",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.linear",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Complex initializations for nn.Linear",
    "text": "Complex initializations for nn.Linear\n\n# re-create out earlier example with a single layer\nfan_in, fan_out = 100, 50\nsigma_he = 1. / np.sqrt(fan_in) # to match the He criteria\n\n# get the complex-valued weights\nm = torch.nn.Linear(fan_in, fan_out)\nreal, imag = get_complex_inits(m)\n\nWe should check that the magnitude of the weights actually follow a Rayleigh distribution.\n\n# get linear magnitudes as a flat numpy vector\nmagnitude = torch.sqrt(real**2 + imag**2).numpy().reshape(-1)\n\n\n# setup the plot\nplt.figure(figsize=(8,7), dpi=80)\nplt.title('Complex nn.Linear Weights vs. Theoretical Rayleigh PDF', fontsize='x-large')\nplt.xlabel('Sample Value', fontsize='x-large')\nplt.ylabel('Probability Density', fontsize='x-large')\n\n# pick points that cover the sample range to compare with theoretical rayleigh pdf\npoints = np.linspace(0, magnitude.max(), 1000)\nray_pdf = rayleigh_pdf(points, sigma=sigma_he)\n\n# plot histogram of Linear magnitudes vs. the theoretical pdf\nplt.hist(magnitude, bins=35, density=True)\nplt.plot(points, ray_pdf, c='r', linewidth=3);\n\n\n\n\nSuccess! Our Linear module is properly initialized."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.conv2d",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.conv2d",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Complex initializations for nn.Conv2d",
    "text": "Complex initializations for nn.Conv2d\nCan we do the same for a convolutional layer? Our main concern is correctly handling both the tensor shape and fan_in, fan_out.\n\n# make conv layer with 100 input features, 50 output features, and (3x3) kernel\nk = 3 # kernel size\n# now, these are the number of feature maps (chan_in and chan_out)\nfan_in, fan_out = 100, 50\n\nconv_layer = torch.nn.Conv2d(fan_in, fan_out, k)\nreal_conv, imag_conv = get_complex_inits(conv_layer) # get the initial complex weights\n\n# make sure the shape of weights is ok\nprint(f'Shapes of real and imaginary convolutional tensors: {real_conv.shape}, {imag_conv.shape}')\n\nShapes of real and imaginary convolutional tensors: torch.Size([50, 100, 3, 3]), torch.Size([50, 100, 3, 3])\n\n\nLetâ€™s check if these convolutional weights are still Rayleigh distributed.\n\n# get convolutional magnitudes as a flat numpy vector\nconv_magnitude = torch.sqrt(real_conv**2 + imag_conv**2).numpy().reshape(-1)\n\n\n# setup the plots\nplt.figure(figsize=(8,7), dpi=80)\nplt.title('Complex nn.Conv2d Weights vs. Theoretical Rayleigh PDF', fontsize='x-large')\nplt.xlabel('Sample Value', fontsize='x-large')\nplt.ylabel('Probability Density', fontsize='x-large')\n\n\n# pick points that cover sample range to compare with theoretical rayleigh pdf\npoints = np.linspace(0, conv_magnitude.max(), 1000)\n\n# note: we need to re-compute fanIn for the convolutional layer\nfan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(conv_layer.weight)\nsigma_he_conv = sigma=1. / np.sqrt(fan_in)\n\nray_pdf = rayleigh_pdf(points, sigma_he_conv)\n\n# plot histogram of magnitudes vs. theoretical pdf\nplt.hist(conv_magnitude, bins=35, density=True)\nplt.plot(points, ray_pdf, c='r', linewidth=3);\n\n\n\n\nAnother match! Our convolutional layer is also properly initialized."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "",
    "text": "Changing the Classifier-Free Guidance parameter during diffusion."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#classifier-free-guidance-overview",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#classifier-free-guidance-overview",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Classifier-free Guidance overview",
    "text": "Classifier-free Guidance overview\nClassifier-free Guidance is a way of steering the outputs of Diffusion models to better align with a given input. It is a key aspect of how we are able to type in a text prompt and get back a relevant, generated image.\nCFG was needed because, by default, a Diffusion model starts from pure noise and randomly â€œwalksâ€ to unearth an image. Classifier-free Guidance can instead align the output according to a known, specific input. This known input is usually a meaningful piece of context like a sentence, or a segment of speech, or even another image.\nIn summary: Instead of randomly walking to generate random images, CFG allows Diffusion models to create targeted outputs.\n\nCFG Formula\nCFG updates the unconditioned latents to better match the conditional inputs as follows:\n\\[\\hat{\\epsilon}(x \\ |\\  y) = \\epsilon(x) + G\\left(\\ \\epsilon(x\\  |\\  y) - \\epsilon(x)\\ \\right)\\]\nWe can think of this equation as a type of moving average. To be more specific, the terms are:\n\n\n\nEquation Term\nDescription\n\n\n\n\n\\(\\epsilon(x)\\)\nUnconditioned noise prediction\n\n\n\\(\\epsilon(x\\ |\\ y)\\)\nConditional noise prediction\n\n\n\\(G\\)\nGuidance scaling factor\n\n\n\\(\\hat{\\epsilon}(x\\ |\\ y)\\)\nThe final, guided prediction.\n\n\n\nAs several people have noticed, this update is not balanced. The reason for the unbalance is that \\(G\\) is usually a large, fixed scalar. For example the default \\(G\\) in Stable Diffusion pipelines is \\(G = 7.5\\).\nThis brings up two questions:\n\nDoes a large \\(G\\) make the vectors too different?\n\nShould \\(G\\) be a fixed constant throughout the entire diffusion process?\n\nFahim compiled the forumâ€™s answers to these questions in this notebook. His work compares both different normalizations and schedules for the Guidance parameter.\nAt first glance, it seems that both normalizing and scheduling the diffusion parameter improves the generated images. These better images are achieved for â€œfreeâ€, in the sense that we didnâ€™t need any fine-tuning or new data.\nLetâ€™s take a look at some of the details and benefits of a dynamic guidance parameter.\n\n\n\n\n\n\nNote\n\n\n\nAs Ben Poole points out in Jeremyâ€™s twitter thread, these ideas are not new on their own.\nOne of the scalings was described in Guided-TTS for Speech diffusion. The normalizations are also related to the ones in Pretraining is All You Need for Image-to-Image Translation by Wang et. al.Â \nOur normalizations are similar in spirit to the Dynamic Thresholding in the Imagen paper."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#normalizing-the-guidance",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#normalizing-the-guidance",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Normalizing the guidance",
    "text": "Normalizing the guidance\nThis notebook explores two types of normalization we call BaseNorm and T-Norm:\n\nBaseNorm: Normalize the entire prediction by the ratio of the conditioned and unconditioned norms.\n\\[\\hat{\\epsilon}(x \\ |\\  y)_\\text{BaseNorm} = \\hat{\\epsilon}(x \\ |\\  y)\\cdot \\frac{\\|\\epsilon(x)\\|}{\\|\\epsilon(x \\ |\\  y)\\|}\\]\nT-Norm: Normalize the difference of the conditioned and unconditioned predictions. \\[\\hat{\\epsilon}(x \\ |\\  y)_\\text{TNorm} = \\epsilon(x) + G\\ \\frac{\\epsilon(x \\ |\\  y) - \\epsilon(x)}{\\|\\epsilon(x \\ |\\  y) - \\epsilon(x)\\|\\cdot \\|\\epsilon(x)\\|}\\]"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#scheduling-the-guidance",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#scheduling-the-guidance",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Scheduling the guidance",
    "text": "Scheduling the guidance\nIn standard CFG the guidance scaling value is fixed. But since the final and initial images are so different, should we expect that the same value is optimal for the entire time?\nTo explore this question we can borrow from Neural Network optimizers. Specifically, our idea of a â€œguidance scheduleâ€ is based on the popular schedules for learning rates.\nThis notebook explores two new schedules for the CFG parameter \\(G\\):\n\nCosine\n\nCosine with Warmup."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-the-changes",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-the-changes",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Combining the changes",
    "text": "Combining the changes\nThe natural idea is to combine these approaches: we should both normalize and schedule \\(G\\).\nAfter exploring each change in isolation we combine them to see their joint effects."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#python-imports",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#python-imports",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Python imports",
    "text": "Python imports\nFirst we import the python, PyTorch, and HuggingFace modules that we need. We also use the timm library for its built-in Cosine schedules.\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nfrom tqdm.auto import tqdm \n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler\n\n# use cosine scheduler from timm\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\nfrom timm.optim import create_optimizer\nfrom timm import create_model\n\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-20 19:51:47.940762: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#prompt-for-image-generations",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#prompt-for-image-generations",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Prompt for image generations",
    "text": "Prompt for image generations\nWe use the following prompt to test our guidance changes:\n\nâ€œa photograph of an astronaut riding a horseâ€\n\nThis is the same prompt folks used in the forums. It seems like a good, simple starting point for future runs.\n\n# the input prompt for diffusion\nprompt = \"a photograph of an astronaut riding a horse\""
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#picking-a-diffusion-model",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#picking-a-diffusion-model",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Picking a Diffusion model",
    "text": "Picking a Diffusion model\nWe also have to pick a Diffusion model. Some possible options are:\n\nstable-Diffusion-v1-4 from CompVis.\n\nstable-Diffusion v1-5 from Runway.ml.\n\nHere we use the Stable Diffusion v1-4 model from CompVis.\nBut it is worth mentioning that this code will work with any Diffusion model name on the HuggingFace hub.\n\n# set the diffusion model\nmodel_name = \"CompVis/stable-diffusion-v1-4\" # \"runwayml/stable-diffusion-v1-5\""
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#utility-functions.",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#utility-functions.",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Utility functions.",
    "text": "Utility functions.\nNext we define some helper functions.\nThese helpers create the text embeddings, convert latent features into images, and plot the decoded images. All of these functions are directly from Fahimâ€™s notebook.\n\ndef text_embeddings(prompts, maxlen=None):\n    \"Extracts text embeddings from the given `prompts`.\"\n    maxlen = maxlen or tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(device))[0]\n\n\ndef image_from_latents(latents):\n    \"Scales the diffusion `latents` and turns them into a PIL Image.\"\n    \n    # scale and decode the latents\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        data = vae.decode(latents).sample[0]\n\n    # Create PIL image\n    data = (data / 2 + 0.5).clamp(0, 1)\n    data = data.cpu().permute(1, 2, 0).float().numpy()\n    data = (data * 255).round().astype(\"uint8\")\n    image = Image.fromarray(data)\n    return image\n    \n    \ndef show_image(image, scale=0.5):\n    \"Displays the given `image` resized based on `scale`.\"\n    img = image.resize(((int)(image.width * scale), (int)(image.height * scale)))\n    display(img)\n    return img\n\n\ndef image_grid(images, rows = 1, width=256, height=256, title=None):\n    \"Display an array of images in a nice grid, or single row\"\n    count = len(images)\n    cols = int(count / rows)\n    if cols * rows &lt; count:\n        rows += 1\n    # Calculate fig size based on individual image sizes    \n    px = 1/plt.rcParams['figure.dpi']\n    w = cols * width * px\n    # Add some extra space for the caption/title since that can wrap\n    h = (rows * height * px) + (rows * 30 * px)\n    fig, axes = plt.subplots(rows, cols, figsize=(w, h))\n    for y in range(rows):\n        for x in range(cols):\n            index = y*cols + x\n            ref = axes[x] if rows == 1 else axes[y] if cols == 1 else axes[y, x]\n            ref.axis('off')\n            if index &gt; count - 1:\n                continue\n            img = images[index]\n            txt = f'Frame: {index}'\n            if title is not None:\n                if isinstance(title, str):\n                    txt = f'{title}: {index}'\n                elif isinstance(title, List):\n                    txt = title[index]\n            # small change for bigger, more visible titles\n            txt = '\\n'.join(wrap(txt, width=70))\n            ref.set_title(txt, fontsize='x-large')\n            ref.imshow(img)\n            ref.axis('off')"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#plotting-the-cosine-schedules",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#plotting-the-cosine-schedules",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Plotting the Cosine schedules",
    "text": "Plotting the Cosine schedules\nLetâ€™s plot these new schedules to compare them against the previous, constant guidance."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#making-schedules-for-guidancetfm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#making-schedules-for-guidancetfm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Making schedules for GuidanceTfm",
    "text": "Making schedules for GuidanceTfm\nWe start with the following family of Guidance schedules:\n- Constant guidance with \\(\\left(G = 7.5\\right)\\)\n- Constant guidance with \\(\\left(G = 0.15\\right)\\)\n- A cosine schedule from \\(\\left(G = 7.5\\right)\\) down to \\(\\left(G = 0.15\\right)\\)\n- A cosine schedule that warms up to \\(\\left(G = 7.5\\right)\\) over the first 10% of steps\nFor the T-Norm experiments, we also define a smaller-valued cosine schedule:\n- T-Norm cosine schedule from \\(\\left(G = 0.25\\right)\\) down to \\(\\left(G = 0.05\\right)\\)\nThe schedule maps below will be the arguments to our GuidanceTfm instances.Â \n\n# baseline constant schedules with min and max values\nmax_sched        = {'g': [max_g] * num_steps}\nmin_sched        = {'g': [min_g] * num_steps}\n\n# cosine schedules\ncos_sched        = {'g': cos_g}\ncos_warmup_sched = {'g': warmup_cos_g}\n\n# normalized cosing schedules for T and Full-scale guidance\nsmall_cos_sched = {'g':  t_scale_cos_g}"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#recreating-the-forum-ideas",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#recreating-the-forum-ideas",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Recreating the forum ideas",
    "text": "Recreating the forum ideas\nFirst, letâ€™s recreate the experiment baselines from the forums and Fahimâ€™s notebook.\n\n# stores the guidance experiements to run\nexpts = {}\n\n\n### RECREATE SCALING RUNS FROM fast.ai FORUM POSTS\n#################################################\n#################################################\nbaseline        = GuidanceTfm(max_sched)       # 1) No scaling, guidance fixed to 7.5\nscale_base_hi_g = BaseNormGuidance(max_sched)  # 2) Scale the \"whole\" update\nscale_T_lo_g    = TNormGuidance(min_sched)     # 3) Scale the update of \"t\"\nscale_all_hi_g  = FullNormGuidance(min_sched)  # 4) Scale everything (steps 2 + 3)\n\n# add baselines to the experiment list\nexpts[f'NoNorm_FixedG_{max_g:.2f}']   = baseline\nexpts[f'BaseNorm_FixedG_{max_g:.2f}'] = scale_base_hi_g\nexpts[f'TNorm_FixedG_0{min_g:.2f}']   = scale_T_lo_g\nexpts[f'FullNorm_FixedG_{min_g:.2f}'] = scale_all_hi_g\n#################################################"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-scales-and-schedules",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-scales-and-schedules",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Combining scales and schedules",
    "text": "Combining scales and schedules\nNext, we leverage our GuidanceTfm class to easily make new experiments.\nWe create the following:\n\nDefault and BaseNorm Guidance with Cosine and Cosine Warmup schedules.\nT-Norm and FullNorm Guidance with the smaller T-Cosine schedule.\n\n\n# group the cosine to run, and their names for plotting\nname2sched = {\n    'Cos':        cos_sched,\n    'CosWarmup':  cos_warmup_sched,\n    'TCos':       small_cos_sched,\n}\n\n\n# T-Norm and FullNorm guidance with small T-Cosine\nnorm_scalers = [TNormGuidance, FullNormGuidance]\nfor scaler in norm_scalers:\n    \n    # step through all cosine schedules\n    for name in ['TCos']:\n\n        # experiment for this (scaling, schedule) pair\n        expt = scaler(name2sched[name])\n        # unique name for this experiment\n        expt_name = f'{scaler.name}_Sched_{name}'\n\n        # add scaler to lists of experiments\n        expts[expt_name] = expt\n\n        \n# Default and BaseNorm guidance with cosine schedules \ng_scalers = [GuidanceTfm, BaseNormGuidance]\nfor scaler in g_scalers:\n    \n    # step through all cosine schedules\n    for name in ['Cos', 'CosWarmup']:\n\n        # experiment for this (scaling, schedule) pair\n        expt = scaler(name2sched[name])\n        # unique name for this experiment\n        expt_name = f'{scaler.name}_Sched_{name}'\n\n        # add scaler to lists of experiments\n        expts[expt_name] = expt\n\nHere we print all of the queued experiments:\n\nprint(\"Guidance experiments to run:\\n\")\nprint('\\n'.join(f'{k}' for k,_ in expts.items()))\n\nGuidance experiments to run:\n\nNoNorm_FixedG_7.50\nBaseNorm_FixedG_7.50\nTNorm_FixedG_00.15\nFullNorm_FixedG_0.15\nTNormGuidance_Sched_TCos\nFullNormGuidance_Sched_TCos\nCFGuidance_Sched_Cos\nCFGuidance_Sched_CosWarmup\nBaseNormGuidance_Sched_Cos\nBaseNormGuidance_Sched_CosWarmup"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#showing-all-images-side-by-side",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#showing-all-images-side-by-side",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Showing all images side by side",
    "text": "Showing all images side by side\nOur starting image, the baseline, is in the top-left. All other images are from different Guidance normalizations and schedules.\n\n\n\n\n\nThatâ€™s a lot of images. Thankfully, there is one result that stands out above the rest:"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#biggest-improvement-cosine-with-t-norm-and-fullnorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#biggest-improvement-cosine-with-t-norm-and-fullnorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Biggest Improvement: Cosine with T-Norm and FullNorm",
    "text": "Biggest Improvement: Cosine with T-Norm and FullNorm\nThere seems to be a consistent gain from using either T-Norm or FullNorm with a Cosine schedule.\nThe image below compares our baseline to T-Norm and Cosine schedule. We can see:\n\nA more semantically correct horse (it has all of its legs!).\n\nBetter details and colors in the background.\n\nThe horseâ€™s body is still not quite right, but itâ€™s a marked improvement from the baseline."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#cosine-t-norm-vs.-cosine-fullnorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#cosine-t-norm-vs.-cosine-fullnorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Cosine T-Norm vs.Â Cosine FullNorm",
    "text": "Cosine T-Norm vs.Â Cosine FullNorm\nThese images are close, and both are better than the baseline. It seems we traded some background quality for subject quality with FullNorm vs.Â T-Norm."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs.Â BaseNorm",
    "text": "Original vs.Â BaseNorm\nHere we plot our default image and the result from BaseNorm.\nThe differences are subtle, but track the general observations from the forums:\n- More detail in the backgrounds.\n- Better shadowing on subjects.\n- Some moderate clarity gains."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-t-norm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-t-norm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs.Â T-Norm",
    "text": "Original vs.Â T-Norm"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-fullnorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-fullnorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs.Â FullNorm",
    "text": "Original vs.Â FullNorm"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-cosine",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-cosine",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs.Â Cosine",
    "text": "Original vs.Â Cosine"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm-with-cosine",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm-with-cosine",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs.Â BaseNorm with Cosine",
    "text": "Original vs.Â BaseNorm with Cosine"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "",
    "text": "Experiments with cosine schedules for Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#imports",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#imports",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom fastcore.all import L\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nfrom tqdm.auto import tqdm \n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler\n\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-21 19:06:22.967865: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#helper-functions",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#helper-functions",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Helper functions",
    "text": "Helper functions\nThe functions below help with:\n\nGenerating text embeddings from a given prompt.\n\nConverting Diffusion latents to a PIL image.\n\nPlotting the images to visualize results.\n\n\ndef text_embeddings(prompts, maxlen=None):\n    \"Extracts text embeddings from the given `prompts`.\"\n    maxlen = maxlen or tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(device))[0]\n\n\ndef image_from_latents(latents):\n    \"Scales diffusion `latents` and turns them into a PIL Image.\"\n    \n    # scale and decode the latents\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        data = vae.decode(latents).sample[0]\n\n    # Create PIL image\n    data = (data / 2 + 0.5).clamp(0, 1)\n    data = data.cpu().permute(1, 2, 0).float().numpy()\n    data = (data * 255).round().astype(\"uint8\")\n    image = Image.fromarray(data)\n    return image\n    \n    \ndef show_image(image, scale=0.5):\n    \"Displays the given `image` resized based on `scale`.\"\n    img = image.resize(((int)(image.width * scale), (int)(image.height * scale)))\n    display(img)\n    return img\n\n\ndef image_grid(images, rows = 1, width=256, height=256, title=None):\n    \"Display an array of images in a grid with the given number of `rows`\"\n    count = len(images)\n    cols = int(count / rows)\n    if cols * rows &lt; count:\n        rows += 1\n    # Calculate fig size based on individual image sizes    \n    px = 1/plt.rcParams['figure.dpi']\n    w = cols * width * px\n    # Add some extra space for the caption/title since that can wrap\n    h = (rows * height * px) + (rows * 30 * px)\n    fig, axes = plt.subplots(rows, cols, figsize=(w, h))\n    for y in range(rows):\n        for x in range(cols):\n            index = y*cols + x\n            ref = axes[x] if rows == 1 else axes[y] if cols == 1 else axes[y, x]\n            ref.axis('off')\n            if index &gt; count - 1:\n                continue\n            img = images[index]\n            txt = f'Frame: {index}'\n            if title is not None:\n                if isinstance(title, str):\n                    txt = f'{title}: {index}'\n                elif isinstance(title, List):\n                    txt = title[index]\n            # small change for bigger, more visible titles\n            txt = '\\n'.join(wrap(txt, width=70))\n            ref.set_title(txt, fontsize='x-large')\n            ref.imshow(img)\n            ref.axis('off')"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#loading-a-diffusion-pipeline",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#loading-a-diffusion-pipeline",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Loading a Diffusion pipeline",
    "text": "Loading a Diffusion pipeline\nWe need to dynamically change the diffusion guidance parameter \\(G\\).\nThat means we need more control than what is available in the high-level HuggingFace APIs. To achieve this control, we load each piece of a Diffusion pipeline separately. Then, we can write our own image generation loop with full control over \\(G\\).\nThe get_sd_pieces function loads and returns the separate components of a Stable Diffusion pipeline.\n\ndef get_sd_pieces(model_name, dtype=torch.float32, better_vae='ema'):\n    \"Loads and returns the individual pieces in a Diffusion pipeline.\"\n    \n    # create the tokenizer and text encoder\n    tokenizer = CLIPTokenizer.from_pretrained(\n        model_name,\n        subfolder=\"tokenizer\",\n        torch_dtype=dtype)\n    text_encoder = CLIPTextModel.from_pretrained(\n        model_name,\n        subfolder=\"text_encoder\",\n        torch_dtype=dtype).to(device)\n\n    # we are using a VAE from stability that was trained for longer than the baseline \n    if better_vae:\n        assert better_vae in ('ema', 'mse')\n        vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{better_vae}\", torch_dtype=dtype).to(device)\n    else:\n        vae = AutoencoderKL.from_pretrained(model_name, subfolder='vae', torch_dtype=dtype).to(device)\n    \n    # build the unet\n    unet = UNet2DConditionModel.from_pretrained(\n        model_name,\n        subfolder=\"unet\",\n        torch_dtype=dtype).to(device)\n    \n    # enable unet attention slicing\n    slice_size = unet.config.attention_head_dim // 2\n    unet.set_attention_slice(slice_size)\n        \n    # build the scheduler\n    scheduler = LMSDiscreteScheduler.from_config(model_name, subfolder=\"scheduler\")\n    \n    return (\n        tokenizer,\n        text_encoder,\n        vae,\n        unet,\n        scheduler,\n    )\n\n\nPicking a model\nThese runs use the openjourney model from Prompt Hero.\n\n\n\n\n\n\nImportant\n\n\n\nopenjourney was fine-tuned to create images in the style of Midjourney v4.\nTo trigger this style, we need to add the special keyword \"mdjrny-v4\" at the front of an input text prompt.\n\n\n\n# set the diffusion model\nmodel_name = \"prompthero/openjourney\"\n\n# other possible models\n# model_name = \"CompVis/stable-diffusion-v1-4\"\n# model_name = \"runwayml/stable-diffusion-v1-5\"\n\nNext we use the function get_sd_pieces to load this model. The pieces are loaded in float16 precision.\n\n# set the data type for the pipeline\ndtype = torch.float16\n\n# load the individual diffusion pieces\npieces = get_sd_pieces(model_name, dtype=dtype)\n(tokenizer, text_encoder, vae, unet, scheduler) = pieces"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#text-prompt-for-generations",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#text-prompt-for-generations",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Text prompt for generations",
    "text": "Text prompt for generations\nWe use the same input text prompt from the previous notebook:\n\nâ€œa photograph of an astronaut riding a horseâ€\n\nBut, we add the special prefix keyword \"mdjrny-v4\" to create Midjourney-style images.\n\n# input text prompt for diffusion models\nprompt = \"mdjrny-v4 style a photograph of an astronaut riding a horse\""
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#generating-images",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#generating-images",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Generating images",
    "text": "Generating images\nThe images will be generated over \\(50\\) diffusion steps. They will have a height and width size of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512\n\n\nCreating a fixed starting point for diffusion\nThe code below creates an initial set of latent noise.\nThe idea is for every generation to start from this shared, fixed noise. That way we can be sure that only our guidance changes are having an effect on the output image.\n\n# create the shared, initial latents\nseed = 1024\ntorch.manual_seed(seed)\ninit_latents = torch.randn((1, unet.in_channels, height//8, width//8), dtype=unet.dtype, device=device)\n\n\n\nImage generation function\nBelow is the main image generation function: generate. It uses the Stable Diffusion components we loaded earlier.\nNote that this function is almost identical to the StableDiffusionPipeline from HuggingFace. The main difference is plugging in our Guidance Transform instead of doing the default Classifier-free Guidance update.\n\ndef generate(prompt, guide_tfm=None, width=width, height=height, steps=num_steps, **kwargs):\n    # make sure we have a guidance transformation\n    assert guide_tfm\n    \n    # prepare the text embeddings\n    text = text_embeddings(prompt)\n    uncond = text_embeddings('')\n    emb = torch.cat([uncond, text]).type(unet.dtype)\n    \n    # start from the shared, initial latents\n    latents = torch.clone(init_latents)\n    scheduler.set_timesteps(steps)\n    latents = latents * scheduler.init_noise_sigma\n    \n    # run the diffusion process\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): \n            tf = ts\n            if torch.has_mps:\n                tf = ts.type(torch.float32)\n            u,t = unet(inp, tf, encoder_hidden_states=emb).sample.chunk(2)\n        \n        # call the guidance transform\n        pred = guide_tfm(u, t, idx=i)\n        \n        # update the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n    # decode and return the final latents\n    image = image_from_latents(latents)\n    return image"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#default-schedule-parameters",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#default-schedule-parameters",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Default schedule parameters",
    "text": "Default schedule parameters\nWe start from the guidance schedule value from the previous notebook.\nRecall that there were three kinds of schedules:\n\nA static schedule with a constant \\(G\\).\n\nA decreasing Cosine schedule.\n\nA Cosine schedule with some initial warm up steps.\n\n\n# Default schedule parameters from the blog post\n######################################\nmax_val           = 7.5   # guidance scaling value\nmin_val           = 1     # minimum guidance scaling\nnum_steps         = 50    # number of diffusion steps\nnum_warmup_steps  = 0     # number of warmup steps\nwarmup_init_val   = 0     # the intial warmup value\nnum_cycles        = 0.5   # number of cosine cycles\nk_decay           = 1     # k-decay for cosine curve scaling \n######################################\n\nTo make sure our changes always reference this shared starting point, we can wrap these parameters in a dictionary.\n\nDEFAULT_COS_PARAMS = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        num_cycles,\n    'k_decay':           k_decay,\n    'num_warmup_steps':  num_warmup_steps,\n    'warmup_init_val':   warmup_init_val,\n}\n\nThen, every minimum-pair change will start from this shared dictionary and update a single parameter. The cos_harness below gives us an easy way of making these minimum-pair changes.\n\ndef cos_harness(new_params={}, cos_params=DEFAULT_COS_PARAMS):\n    '''Creates cosine schedules with updated parameters in `new_params`'''\n    \n    # start from the given baseline `cos_params`\n    cos_params = dict(cos_params)\n    \n    # update the schedule with any new parameters\n    if new_params: cos_params.update(new_params)\n    \n    # return the new cosine schedule\n    sched = get_cos_sched(**cos_params)\n    return sched\n\nLetâ€™s use the cosine harness to plot three test schedules, just to make sure things are working:\n\nThe baseline with no warmup.\n\nWarmup for 5 steps.\n\nWarmup for 10 steps.\n\n\n\n\n\n\n\nNote\n\n\n\nThe schedule plotting function plot_schedules is available in the postâ€™s notebook.\n\n\n\n# plot cosine schedules with different number of warmup steps\nwarmup_steps = (0, 5, 10)\nwarm_g = L( \n    {'sched': cos_harness({'num_warmup_steps': w}), \n     'title': f'Warmup Steps: {w}'}\n    for w in warmup_steps\n)\n\n# plot the schedules\nprint('Plotting sample cosine schedules...')\nplot_schedules(warm_g.itemgot('sched'), rows=1, titles=warm_g.itemgot('title'))\n\nPlotting sample cosine schedules..."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#creating-the-cosine-experiments",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#creating-the-cosine-experiments",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Creating the Cosine experiments",
    "text": "Creating the Cosine experiments\nNow we can create the different Cosine schedules that will be swept.\n\ncos_param_sweep = {\n    'num_warmup_steps': [5, 10, 15],\n    'num_cycles':       [1, 1.5],\n    'k_decay':          [0.8, 0.6],\n    'max_val':          [10],\n    'min_val':          [3],\n}\n\nparam_names = sorted(list(cos_param_sweep))\n\ncos_scheds = L()\nfor idx,name in enumerate(param_names):\n    for idj,val in enumerate(cos_param_sweep[name]):\n\n        # create the cosine experimeent\n        expt = {\n            'param_name': name,\n            'val': val,\n            'schedule': cos_harness({name: val})\n        }\n        # for plotting\n        expt['title'] = f'Param: \"{name}\", val={val}'\n        \n        cos_scheds.append(expt)\n    \n\n\nplot_schedules(cos_scheds.itemgot('schedule'), rows=3, titles=cos_scheds.itemgot('title'))"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#running-the-cosine-experiments",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#running-the-cosine-experiments",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Running the cosine experiments",
    "text": "Running the cosine experiments\nWe use the run function from before to run all of the cosine experiments.\n\ncos_res = run(prompt, cos_scheds, guide_tfm=GuidanceTfm, show_each=False)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 9]: Param: \"k_decay\", val=0.8...\nRunning experiment [2 of 9]: Param: \"k_decay\", val=0.6...\nRunning experiment [3 of 9]: Param: \"max_val\", val=10...\nRunning experiment [4 of 9]: Param: \"min_val\", val=3...\nRunning experiment [5 of 9]: Param: \"num_cycles\", val=1...\nRunning experiment [6 of 9]: Param: \"num_cycles\", val=1.5...\nRunning experiment [7 of 9]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [8 of 9]: Param: \"num_warmup_steps\", val=10...\nRunning experiment [9 of 9]: Param: \"num_warmup_steps\", val=15...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#analysis",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#analysis",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Analysis",
    "text": "Analysis\nCertain Cosine schedules seem promising. They either increase the details of the astronaut or background, or they create more anatomically correct horses.\nIn the rest of the series, we will explore the promising Cosine changes:\n\nSetting a higher Guidance ceiling.\n\nAllowing the Cosine to go through multiple cycles.\n\nWarming up for a few steps."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#bringing-in-normalizations",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#bringing-in-normalizations",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Bringing in Normalizations",
    "text": "Bringing in Normalizations\nIn the previous notebooks, we found that normalization can have a huge improvement on generated images. The next logical step is to add normalizations to our schedules to see if the gains compound."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "",
    "text": "Experiments with normalizations for Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#quick-recap-of-part-1",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#quick-recap-of-part-1",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Quick recap of Part 1",
    "text": "Quick recap of Part 1\nIn Part 1, we generated a baseline image using the default, static Classifier-free Guidance. To see if we could improve on the baseline, we swept a range of Cosine Schedules on the guidance parameter \\(G\\).\nTo recap the results of the sweep, there are a few promising guidance schedules to explore:\n\nSetting a higher guidance value.\n\nAllowing the Cosine schedule to go through multiple cycles.\n\nWarming up the guidance for a few steps."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#part-2-bringing-in-normalizations",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#part-2-bringing-in-normalizations",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Part 2: Bringing in Normalizations",
    "text": "Part 2: Bringing in Normalizations\nIn Part 2, we bring in normalizations as another kind of dynamic guidance.\nThe idea is that normalizing the guidance might improve the updates in the Diffusion modelâ€™s latent image space. To test this we explore three kinds of guidance normalizations:\n\nNormalizing the prediction by its overall norm.\n\nNormalizing the guidance update vector, \\(\\left(t - u\\right)\\), by its norm.\n\nCombining the Normalizations in 1. and 2.\n\n\n\n\n\n\n\nNote\n\n\n\nMore details about the normalizations can be found in this section of the original post.\n\n\nAfter these runs, we should have a good idea of both the schedules and normalizations that can improve Diffusion images. We will then combine the two approaches and explore other, more advanced schedules."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#leveraging-a-few-helper-libraries",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#leveraging-a-few-helper-libraries",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Leveraging a few helper libraries",
    "text": "Leveraging a few helper libraries\nWe use two new libraries that make it easier to run dynamic Classifier-free Guidances.\nThese two libraries are:\n\nmin_diffusion\ncf_guidance\n\nThe helper libraries remove a lot of overhead and boilerplate code. They allow us to jump straight to the important parts: running the guidance experiments.\nFor more details, the libraries were introduced in this earlier post."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#python-imports",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#python-imports",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Python Imports",
    "text": "Python Imports\nTo start we import the needed python modules.\nWe also handle random seeding to make sure that our results are reproducible across the series.\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom fastcore.all import L\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# seed for reproducibility\nSEED = 1024\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-23 14:58:50.779076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0"
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#the-min_diffusion-library",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#the-min_diffusion-library",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "The min_diffusion library",
    "text": "The min_diffusion library\nWe use the min_diffusion library to load a Stable Diffusion model from the HuggingFace hub.\n\n# to load Stable Diffusion pipelines\nfrom min_diffusion.core import MinimalDiffusion\n\n# helpers to plot the generated images\nfrom min_diffusion.utils import show_image, image_grid\n\n\nLoading the openjourney model from Prompt Hero\nThe following code loads the openjourney model in torch.float16 precision and puts it on the GPU.\n\nmodel_name = 'prompthero/openjourney'\ndevice     = 'cuda'\ndtype      = torch.float16\n\n\npipeline = MinimalDiffusion(model_name, device, dtype)\n\n\npipeline.load();\n\nEnabling default unet attention slicing."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#text-prompt-for-generations",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#text-prompt-for-generations",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Text prompt for generations",
    "text": "Text prompt for generations\nWe use the familiar, running prompt in our series to generate an image:\n\nâ€œa photograph of an astronaut riding a horseâ€\n\n\n\n\n\n\n\nImportant\n\n\n\nThe openjourney model was fine-tuned to create images in the style of Midjourney v4.\nTo enable this fine-tuned style, we need to add the keyword \"mdjrny-v4\" at the start of the prompt.\n\n\n\n# text prompt for image generations\nprompt = \"mdjrny-v4 style a photograph of an astronaut riding a horse\""
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#image-parameters",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#image-parameters",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Image parameters",
    "text": "Image parameters\nThe images will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#wrapper-to-run-the-experiments",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#wrapper-to-run-the-experiments",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Wrapper to run the experiments",
    "text": "Wrapper to run the experiments\nThe run function below generates images from a given prompt.\nIt also takes an argument guide_tfm for the specific Guidance Transformation class that will guide the outputs. The schedules argument has the parameter values of \\(G\\) at each diffusion timestep.\n\ndef run(prompt, schedules, guide_tfm=None, generator=None,\n        show_each=False, test_run=False):\n    \"\"\"Runs a dynamic Classifier-free Guidance experiment. \n    \n    Generates an image for the text `prompt` given all the values in `schedules`.\n    Uses a Guidance Transformation class from the `cf_guidance` library.  \n    Stores the output images with a matching title for plotting. \n    Optionally shows each image as its generated.\n    If `test_run` is true, it runs a single schedule for testing. \n    \"\"\"\n    # store generated images and their title (the experiment name)\n    images, titles = [], []\n    \n    # make sure we have a valid guidance transform\n    assert guide_tfm\n    print(f'Using Guidance Transform: {guide_tfm}')\n    \n    # optionally run a single test schedule\n    if test_run:\n        print(f'Running a single schedule for testing.')\n        schedules = schedules[:1]\n        \n\n    # run all schedule experiments\n    for i,s in enumerate(schedules):\n        \n        # parse out the title for the current run\n        cur_title  = s['title']\n        titles.append(cur_title)\n        \n        # create the guidance transformation \n        cur_sched = s['schedule']\n        gtfm = guide_tfm({'g': cur_sched})\n        \n        print(f'Running experiment [{i+1} of {len(schedules)}]: {cur_title}...')\n        img = pipeline.generate(prompt, gtfm, generator=generator)\n        images.append(img)\n        \n        # optionally plot the image\n        if show_each:\n            show_image(img, scale=1)\n\n    print('Done.')\n    return {'images': images,\n            'titles': titles,}\n\nLetâ€™s create the baseline image. The hope is that our guidance changes will then improve on it.\n\nbaseline_res = run(prompt, baseline_scheds, guide_tfm=GuidanceTfm, generator=generator)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone.\n\n\n\n\n\n\n# view the baseline image\nbaseline_res['images'][0]\n\n\n\n\nNot a bad starting point. Letâ€™s see if we can do better."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#details-on-normalized-guidance-values",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#details-on-normalized-guidance-values",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Details on normalized Guidance values",
    "text": "Details on normalized Guidance values\nFor Prediction Normalization we can use the same static \\(G = 7.5\\) from the baseline.\nFor both T-Normalization and Full Normalization, however, we need a much smaller guidance value. The reason is that these normalizations scale the update vector \\(\\left( t - u \\right)\\) itself. That means that a large value like \\(G = 7.5\\) would de-scale the vectors even more! That is the exact situation we are trying to avoid with normalization in the first place.\nTo prevent this, we create a special \\(G_\\text{small}\\) schedule for T and Full Normalizations with a smaller value of \\(G_\\text{small} = 0.15\\)\n\n# create the baseline Classifier-free Guidance\nT_run = {'max_val': [0.15]}\n\n# parameters we are sweeping\nT_scheds = L()\n\n# step through each parameter\nfor idx,name in enumerate(baselines_names):\n    # step through each of its values\n    for idj,val in enumerate(T_run[name]):\n\n        # create the baseline experimeent\n        expt = {\n            'param_name': name,\n            'val': val,\n            'schedule': [val for _ in range(num_steps)]\n        }\n        # for plotting\n        expt['title'] = f'Param: \"{name}\", val={val}'\n        \n        # add to the running list of experiments\n        T_scheds.append(expt)"
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-runs",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-runs",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Prediction Norm runs",
    "text": "Prediction Norm runs\n\nprint('Running the BaseNorm experiments...')\nbase_norm_res = run(prompt, baseline_scheds, guide_tfm=BaseNormGuidance, generator=generator)\n\nRunning the BaseNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.BaseNormGuidance'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-runs",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-runs",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "T-Norm runs",
    "text": "T-Norm runs\n\nprint('Running the T-Norm experiments...')\nt_norm_res = run(prompt, T_scheds, guide_tfm=TNormGuidance, generator=generator)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=0.15...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-runs",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-runs",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Full Norm runs",
    "text": "Full Norm runs\n\nprint('Running the FullNorm experiments...')\nfull_norm_res = run(prompt, T_scheds, guide_tfm=FullNormGuidance, generator=generator)\n\nRunning the FullNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=0.15...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-results",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-results",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Prediction Norm results",
    "text": "Prediction Norm results\n\n\n\n\n\nComparing left to right, Prediction Normalization improves the overall image.\nThe horseâ€™s body and hair are more defined. The clouds in the background have more texture. The lowest orb in the sky is much better defined. The shadows on the ground also have better coverage and a more natural transition. The ground itself has more details and texture, and is better separated from the background sky.\nEven the reflection on the astronautâ€™s helmet has more depth and looks smoother.\nOverall, it seems that Prediction Normalization is a global improvement on the baseline."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-results",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-results",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "T-Norm results",
    "text": "T-Norm results\n\n\n\n\n\nThis one is much more interesting. T-Normalization completely changed the image! Even though they started from the exact same noisy latents.\nHere the horseâ€™s anatomy, especially its head, look more correct. Even though we lost overall illumination on the horseâ€™s body.\nThe patches and details on the astronautâ€™s gear are also better defined. And maybe itâ€™s subjective, but this one feels more like a photograph (thanks to helmetâ€™s glare) while the baseline looks more like digital art."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-results",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-results",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Full Norm results",
    "text": "Full Norm results\n\n\n\n\n\nFull Normalization feels like a mix of the best from both worlds.\nThe horseâ€™s anatomy and astronaut details are better, following the results from T-Normalization. And we regained some background vs.Â foreground separation from Prediction Normalization.\nIt seems this dual benefit came at the cost of some symmetry for the orbs in the sky, and a loss of resolution on the horseâ€™s tail."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#analysis",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#analysis",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Analysis",
    "text": "Analysis\nOverall, at least for this example, it is fair to say that normalizations can improve Diffusion images.\nEither the baseline image was improved overall (Prediction Normalization), or we gained better image syntax and details (T-Normalization and Full Normalization).\nGiven that T-Normalization and Full Normalization completely changed the style of the baseline image, there is a lot to explore here. To start, there is likely a much better set of \\(G_\\text{small}\\) values. Consider the baselineâ€™s value of \\(G = 7.5\\). This value is the standard across many Diffusion models and empirically produces good results. Meanwhile, our \\(G_\\text{small} = 0.15\\) is only a starting point that has not been thoroughly tested.\nIn summary, it seems that Prediction Normalization could be an easy way to improve all Diffusion images. As for the others, they definitely have potential that should be explored further."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "",
    "text": "Exploring the effect of k-decay on Cosine Schedules."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#recap-of-parts-1-3",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#recap-of-parts-1-3",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Recap of Parts 1-3",
    "text": "Recap of Parts 1-3\nThe first three parts explored how to turn Classifier-free Guidance into a dynamic process. We found an initial set of schedules and normalizers that seem to improve the quality of Diffusion images."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#part-4-alternative-warmups-for-cosine-schedules",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#part-4-alternative-warmups-for-cosine-schedules",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Part 4: Alternative warmups for cosine schedules",
    "text": "Part 4: Alternative warmups for cosine schedules\nPart 4 is an exploration of kDecay applied to Cosine Schedules.\nThe kDecay paper introduces a hyperparameter \\(k\\) for scheduled learning rates. This parameter empirically improves the performance of models across many learning rate schedules.\nHere we explore two aspects of \\(k\\) for the guidance parameter \\(G\\):\n\nThe effect of \\((\\ k\\ &lt;\\ 1\\ )\\) and \\((\\ k\\ &gt;\\ 1\\ )\\) on the guidance parameter.\n\nHow an inverse kDecay schedule can be used as a type of warm up."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#image-parameters",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#setting-the-baseline-with-g-7.5",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#setting-the-baseline-with-g-7.5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Setting the baseline with \\(G = 7.5\\)",
    "text": "Setting the baseline with \\(G = 7.5\\)\nFirst we create and display the baseline imagine using a constant Classifier-free Guidance with \\(G = 7.5\\). Since this is a constant schedule, \\(k\\) does not come into play.\n\n# create the baseline schedule with the new function\nbaseline_g = 7.5\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda params: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)\n\n\nbaseline_res = run(prompt, baseline_expts, guide_tfm=GuidanceTfm)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone.\n\n\n\n\n\n\n# view the baseline image\nbaseline_res['images'][0]"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#improving-the-baseline-with-k-schedules",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#improving-the-baseline-with-k-schedules",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Improving the baseline with \\(k\\) schedules",
    "text": "Improving the baseline with \\(k\\) schedules\nNow we sweep the Cosine Schedules with different \\(k\\) values. Then we will check the output images and compare them to the baseline."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-runs",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "k-Sweep runs",
    "text": "k-Sweep runs\n\nprint('Running the k-Sweep experiments...')\ncos_res = run(prompt, cos_expts, guide_tfm=GuidanceTfm)\n\nRunning the k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 10]: Param: \"k_decay\", val=0.1...\nRunning experiment [2 of 10]: Param: \"k_decay\", val=0.2...\nRunning experiment [3 of 10]: Param: \"k_decay\", val=0.3...\nRunning experiment [4 of 10]: Param: \"k_decay\", val=0.5...\nRunning experiment [5 of 10]: Param: \"k_decay\", val=0.7...\nRunning experiment [6 of 10]: Param: \"k_decay\", val=1.0...\nRunning experiment [7 of 10]: Param: \"k_decay\", val=1.5...\nRunning experiment [8 of 10]: Param: \"k_decay\", val=2...\nRunning experiment [9 of 10]: Param: \"k_decay\", val=3...\nRunning experiment [10 of 10]: Param: \"k_decay\", val=5...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-runs",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Inverse k-Sweep runs",
    "text": "Inverse k-Sweep runs\n\nprint('Running the Inverse-k-Sweep experiments...')\ninv_cos_res = run(prompt, inv_cos_expts, guide_tfm=GuidanceTfm)\n\nRunning the Inverse-k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 10]: Param: \"k_decay\", val=0.1...\nRunning experiment [2 of 10]: Param: \"k_decay\", val=0.2...\nRunning experiment [3 of 10]: Param: \"k_decay\", val=0.3...\nRunning experiment [4 of 10]: Param: \"k_decay\", val=0.5...\nRunning experiment [5 of 10]: Param: \"k_decay\", val=0.7...\nRunning experiment [6 of 10]: Param: \"k_decay\", val=1.0...\nRunning experiment [7 of 10]: Param: \"k_decay\", val=1.5...\nRunning experiment [8 of 10]: Param: \"k_decay\", val=2...\nRunning experiment [9 of 10]: Param: \"k_decay\", val=3...\nRunning experiment [10 of 10]: Param: \"k_decay\", val=5...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-results",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "k-Sweep results",
    "text": "k-Sweep results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-results",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Inverse k-Sweep results",
    "text": "Inverse k-Sweep results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-comparison",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "k-Sweep comparison",
    "text": "k-Sweep comparison"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-comparison",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Inverse k-Sweep comparison",
    "text": "Inverse k-Sweep comparison\n\n\n\n\n\nIn both cases, with a high and smooth value of \\(G\\), the output image improved. We gained more details in the background, on the horseâ€™s body, and on the astronautâ€™s gear."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "",
    "text": "Combining the best schedules and normalizations so far."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#recap-of-parts-1-5",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#recap-of-parts-1-5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Recap of Parts 1-5",
    "text": "Recap of Parts 1-5\nThe first five parts explored how to turn Classifier-free Guidance into a dynamic process. We found a good set of schedules and normalizations that seem to improve the output of diffusion image models."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#part-6-putting-it-all-together",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#part-6-putting-it-all-together",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Part 6: Putting it all together",
    "text": "Part 6: Putting it all together\nPart 6 brings together our best approaches so far. Specifically, it explores the following schedules:\n\nkDecay with large \\(k\\) values.\n\nInverse kDecay with small \\(k\\) values.\n\nOn all three Guidance normalizations:\n\nPrediction Normalization\n\nT-Normalization\n\nFull Normalization"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#image-parameters",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#creating-the-baseline-image-with-g-7.5",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#creating-the-baseline-image-with-g-7.5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Creating the baseline image with \\(G = 7.5\\)",
    "text": "Creating the baseline image with \\(G = 7.5\\)\nFirst we create the baseline image using a constant Classifier-free Guidance with \\(G = 7.5\\). Since this is a constant schedule, \\(k\\) does not come into play.\n\n# create the baseline schedule with the new function\nbaseline_g = 7.5\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda *args, **kwargs: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)\n\n\nbaseline_res = run(prompt, baseline_expts, guide_tfm=GuidanceTfm)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone.\n\n\n\n\n\n\n# view the baseline image\nbaseline_res['images'][0]"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Improving the baseline with schedules and normalizations",
    "text": "Improving the baseline with schedules and normalizations\nNow letâ€™s run our kDecay schedules with normalizations. Then we can check how it changed the baseline image.\nSince every run starts from the exact same noisy latents, only the schedules and normalizations are affecting the output."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-runs",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Prediction Normalization runs",
    "text": "Prediction Normalization runs\n\nprint('Running the Prediction Norm experiments...')\nbase_norm_res = run(prompt, all_k_expts, guide_tfm=BaseNormGuidance)\n\nRunning the Prediction Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.BaseNormGuidance'&gt;\nRunning experiment [1 of 8]: Param: \"k_decay\", val=1...\nRunning experiment [2 of 8]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 8]: Param: \"k_decay\", val=5...\nRunning experiment [4 of 8]: Param: \"k_decay\", val=0.15...\nRunning experiment [5 of 8]: Param: \"k_decay\", val=0.2...\nRunning experiment [6 of 8]: Param: \"k_decay\", val=0.3...\nRunning experiment [7 of 8]: Param: \"k_decay\", val=0.5...\nRunning experiment [8 of 8]: Param: \"k_decay\", val=0.7...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-runs",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "T-Normalization runs",
    "text": "T-Normalization runs\n\nprint('Running the T-Norm experiments...')\nT_norm_res = run(prompt, all_T_k_expts, guide_tfm=TNormGuidance)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 8]: Param: \"k_decay\", val=1...\nRunning experiment [2 of 8]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 8]: Param: \"k_decay\", val=5...\nRunning experiment [4 of 8]: Param: \"k_decay\", val=0.15...\nRunning experiment [5 of 8]: Param: \"k_decay\", val=0.2...\nRunning experiment [6 of 8]: Param: \"k_decay\", val=0.3...\nRunning experiment [7 of 8]: Param: \"k_decay\", val=0.5...\nRunning experiment [8 of 8]: Param: \"k_decay\", val=0.7...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-runs",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Full Normalization runs",
    "text": "Full Normalization runs\n\nprint('Running the T-Norm experiments...')\nfull_norm_res = run(prompt, all_T_k_expts, guide_tfm=FullNormGuidance)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 8]: Param: \"k_decay\", val=1...\nRunning experiment [2 of 8]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 8]: Param: \"k_decay\", val=5...\nRunning experiment [4 of 8]: Param: \"k_decay\", val=0.15...\nRunning experiment [5 of 8]: Param: \"k_decay\", val=0.2...\nRunning experiment [6 of 8]: Param: \"k_decay\", val=0.3...\nRunning experiment [7 of 8]: Param: \"k_decay\", val=0.5...\nRunning experiment [8 of 8]: Param: \"k_decay\", val=0.7...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-results",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Prediction Normalization results",
    "text": "Prediction Normalization results"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-results",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "T-Normalization results",
    "text": "T-Normalization results"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-results",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Full-Normalization results",
    "text": "Full-Normalization results"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-comparison",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Prediction Normalization comparison",
    "text": "Prediction Normalization comparison"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-comparison",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "T-Normalization comparison",
    "text": "T-Normalization comparison"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-comparison",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Full Normalization comparison",
    "text": "Full Normalization comparison"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.15-across-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.15-across-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Comparing \\(k = 0.15\\) across normalizations",
    "text": "Comparing \\(k = 0.15\\) across normalizations"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.2-across-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.2-across-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Comparing \\(k = 0.2\\) across normalizations",
    "text": "Comparing \\(k = 0.2\\) across normalizations"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.3-across-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.3-across-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Comparing \\(k = 0.3\\) across normalizations",
    "text": "Comparing \\(k = 0.3\\) across normalizations\n\n\n\n\n\nAt this point, the difference in quality between \\(0.15\\) and \\(0.2\\) becomes subjective. It does seem that 0.2 makes for more stable images across the normalizations. But, 0.15 fixed the astronautâ€™s leg and arm.\n\\(0.3\\) still improves the image, but we start to lose texture and coherence in the background."
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "",
    "text": "Running dynamic CFG with the Stable Diffusion v2 model."
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#seed-for-reproducibility",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed for rng\nSEED = 4191151944 \ndef seed_everything(seed: int) -&gt; torch.Generator:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#static-baselines",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#static-baselines",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Static baselines",
    "text": "Static baselines\nFirst we create the constant, baseline Guidances.\n\n# create the baseline schedule with the new function\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda *args, **kwargs: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)"
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#improving-the-baseline-with-scheduled-guidance",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#improving-the-baseline-with-scheduled-guidance",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Improving the baseline with scheduled Guidance",
    "text": "Improving the baseline with scheduled Guidance\nNow we build the most promising dynamic schedule: Inverse kDecay with a fast warmup.\n\n# creating the inverse kDecay cosine schedules\nk_decays = [0.1, 0.2, 0.3, 0.5]\ninv_k_params = {'k_decay': k_decays}\ninv_k_func = partial(cos_harness, default_params=DEFAULT_COS_PARAMS)\ninv_k_expts = create_expts(inv_k_params, inv_k_func)\n\n# invert the schedules to turn them into a type of warmup \n##TODO: move into the scheduler helper\nfor s in inv_k_expts:\n    s['schedule'] = [max_val - g + min_val for g in s['schedule']]\n\n# put all schedules together\nall_k_expts = inv_k_expts\n\nLetâ€™s plot these schedules to see what they look like."
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#stable-diffusion-v2-images",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#stable-diffusion-v2-images",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Stable Diffusion v2 images",
    "text": "Stable Diffusion v2 images\nHere we plot all of the generated images.\nThe image on the left is the baseline with a static, constant Guidance.\nThe images on the right are the improvements with Guidance scheduling. Specifically, using the Inverse-kDecay cosine schedules with different values of k.\n\nplot_all_results('stabilityai/stable-diffusion-2')"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-09-27-Session_1_nbdev/index.html",
    "href": "blog/posts/fractal-llms/2023-09-27-Session_1_nbdev/index.html",
    "title": "Blogging with Quarto and nbdev",
    "section": "",
    "text": "Building a personal blog with nbdev and publishing it with Quarto."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-09-27-Session_1_nbdev/index.html#references",
    "href": "blog/posts/fractal-llms/2023-09-27-Session_1_nbdev/index.html#references",
    "title": "Blogging with Quarto and nbdev",
    "section": "References",
    "text": "References\n\nOfficial nbdev tutorial\nBlogging with nbdev"
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html",
    "href": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html",
    "title": "Overview of MLC and llama.cpp",
    "section": "",
    "text": "A look at llama.cpp, and running Llama2 with the Machine Learning Compilation (MLC) library."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html#llama.cpp",
    "href": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html#llama.cpp",
    "title": "Overview of MLC and llama.cpp",
    "section": "Llama.cpp",
    "text": "Llama.cpp\nFantastic library on github here. This was patched together as a hackathon project, and is now arguably the SOTA for deploying local LLMs on CPUs. Great proof of â€œjust do thingsâ€, and there still being tons of low-hanging fruit in the space.\nTo best leverage the repo, check out the Pull Requests for the latest updates. Folks are constantly weaving in the newest and latest approaches. Indie hackers and unconventional ideals get proposed all the time: if it works and thereâ€™s proof, people accept it."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html#mlc",
    "href": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html#mlc",
    "title": "Overview of MLC and llama.cpp",
    "section": "MLC",
    "text": "MLC\nTool for deploying ML models across all major architectures. They have a companion course Link that gets into many details."
  },
  {
    "objectID": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html#steps",
    "href": "blog/posts/fractal-llms/2023-10-07-Session_2_mlc/index.html#steps",
    "title": "Overview of MLC and llama.cpp",
    "section": "Steps",
    "text": "Steps\n\nCreate a python environment for MLC\nWeâ€™ll use python3.11 in the MLC environment.\n# create a python3.11 environment for MLC\nmamba create -n mlc-llm python=3.11\n# conda env create -n mlc-llm python=3.11 (maybe?)\nNext, letâ€™s activate the environment.\n# activate the environment\nmamba activate mlc-llm  \nNow weâ€™re ready to install the MLC python library. MLC has pre-built binaries available. Full instructions here.\nFor Mac, install it with the following command:\n# installing the mlc python library\npip install --pre --force-reinstall \\\n    mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels\nCan check if the library installed correctly by running the following:\n# checks if the mlc python api works\npython -c \"from mlc_chat import ChatModule; print(ChatModule)\"\n\n\nManaging large model files\nWeâ€™re going to start downloading the model weights now.\nWeight files are very large. They get unwieldy in regular git repositories.\nWould be very expensive to push/pull a lot of data we know wonâ€™t change. At least not until we fine tune it.\nWill use git-lfs tool to manage large files. Installation instructions here\nFor Mac, you can use either the Homebrew or MacPorts package managers.\n# install git-lfs with Homebrew\nbrew install git-lfs"
  }
]