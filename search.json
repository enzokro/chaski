[
  {
    "objectID": "blog/posts/fractal-llms/05-rag/index.html",
    "href": "blog/posts/fractal-llms/05-rag/index.html",
    "title": "Lesson 5: Processing text documents for LLMs",
    "section": "",
    "text": "Preparing text data for LLM applications."
  },
  {
    "objectID": "blog/posts/fractal-llms/05-rag/index.html#converting-.rst-to-.txt-files",
    "href": "blog/posts/fractal-llms/05-rag/index.html#converting-.rst-to-.txt-files",
    "title": "Lesson 5: Processing text documents for LLMs",
    "section": "Converting .rst to .txt files",
    "text": "Converting .rst to .txt files\nIt is rare that files come in exactly the right format for our ML algorithms. Pre-processing the input is one of the most important steps in ML that often gets overlooked. However, it is a great place to follow one of the Golden Rules of ML: *always* look at your data.\nAll too often, folks jump right into the code and start training models. This a fun step, to be sure, but we can learn so much about both our problem and the domain itself by first looking at the data. Without carefully inspecting data, you are basically flying blind. It is only the sheer and overwhelming power of ML and LLMs that let us get away with it (sometimes), but that doesn’t mean we should.\nWith that said, here we only have to do a little bit of pre-processing. We need to convert the Diátaxis .rst files into .txt files, then clean up the text a bit.\n\n\n\n\n\n\nNote\n\n\n\nMake sure you are inside of the llm-env virtual environment.\n\n\nRun the cell below to install the rst processing libraries.\n\n# installing the rst to txt converter and writer\npip install rst2txt docutils\n\nNext we can modify the example in the rst2txt documentation to write a function that turns an .rst file into a .txt file.\n\nfrom docutils.core import publish_file\nimport rst2txt\n\ndef convert_rst_to_txt(filename):\n    \"\"\"\n    Turns an rst file to a txt file with the same name.\n    \"\"\"\n    with open(filename, 'r') as source:\n        publish_file(\n            source=source,\n            destination_path=filename.replace(\".rst\", \".txt\"),\n            writer=rst2txt.Writer()\n        )\n\nNext up, let’s grab all of the .rst files in the Diátaxis repository and convert them into .txt files.\n\nimport os\n\n# NOTE: replace with your path to the Diátaxis repo\npath_to_diataxis = '/Users/cck/repos/diataxis-documentation-framework'\n\n# find all rst files in the docs repo\nrst_files = [o for o in os.listdir(path_to_diataxis) if o.endswith('.rst')]\n\n# convert all rst files to txt files\nfor rst in rst_files:\n    convert_rst_to_txt(f'{path_to_diataxis}/{rst}')\n\nThe following subset are the docs with relevant information an LLM would need to write notebooks in the Diaxtaxis style.\n\n# files with important content about writing docs\nvalid_files = [\n    'compass.txt',\n    'complex-hierarchies.txt',\n    'explanation.txt',\n    'how-to-guides.txt',\n    'how-to-use-diataxis.txt',\n    'needs.txt',\n    'quality.txt',\n    'reference-explanation.txt',\n    'reference.txt',\n    'tutorials-how-to.txt',\n    'tutorials.txt',\n]\n\nLet’s read in these text files and store them into a data dictionary.\n\n# stores the text data\ndata = {}\n\n# read in the relevant files\nfor f in valid_files:\n    with open(f'{path_to_diataxis}/{f}', 'r') as file:\n        data[f] = str(file.read())\n\nIn data, file name are the keys and the values are the text in the files. This is a pretty standard pattern when loading ML data: features are loaded into a map (dictionary), indexed by some unique identifier."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html",
    "title": "Overview of MLC and llama.cpp",
    "section": "",
    "text": "A look at llama.cpp, and running Llama2 with the Machine Learning Compilation (MLC) library."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html#llama.cpp",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html#llama.cpp",
    "title": "Overview of MLC and llama.cpp",
    "section": "Llama.cpp",
    "text": "Llama.cpp\nFantastic library on github here. This was patched together as a hackathon project, and is now arguably the SOTA for deploying local LLMs on CPUs. Great proof of “just do things”, and there still being tons of low-hanging fruit in the space.\nTo best leverage the repo, check out the Pull Requests for the latest updates. Folks are constantly weaving in the newest and latest approaches. Indie hackers and unconventional ideals get proposed all the time: if it works and there’s proof, people accept it."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html#mlc",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html#mlc",
    "title": "Overview of MLC and llama.cpp",
    "section": "MLC",
    "text": "MLC\nTool for deploying ML models across all major architectures. They have a companion course Link that gets into many details."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html#steps",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/old_mlc_nb.html#steps",
    "title": "Overview of MLC and llama.cpp",
    "section": "Steps",
    "text": "Steps\n\nCreate a python environment for MLC\nWe’ll use python3.11 in the MLC environment.\n# create a python3.11 environment for MLC\nmamba create -n mlc-llm python=3.11\n# conda env create -n mlc-llm python=3.11 (maybe?)\nNext, let’s activate the environment.\n# activate the environment\nmamba activate mlc-llm  \nNow we’re ready to install the MLC python library. MLC has pre-built binaries available. Full instructions here.\nFor Mac, install it with the following command:\n# installing the mlc python library\npip install --pre --force-reinstall \\\n    mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels\nCan check if the library installed correctly by running the following:\n# checks if the mlc python api works\npython -c \"from mlc_chat import ChatModule; print(ChatModule)\"\n\n\nManaging large model files\nWe’re going to start downloading the model weights now.\nWeight files are very large. They get unwieldy in regular git repositories.\nWould be very expensive to push/pull a lot of data we know won’t change. At least not until we fine tune it.\nWill use git-lfs tool to manage large files. Installation instructions here\nFor Mac, you can use either the Homebrew or MacPorts package managers.\n# install git-lfs with Homebrew\nbrew install git-lfs"
  },
  {
    "objectID": "blog/posts/fractal-llms/02-nbdev/index.html",
    "href": "blog/posts/fractal-llms/02-nbdev/index.html",
    "title": "Lessong 2: Blogging with nbdev",
    "section": "",
    "text": "Making a personal blog for the course using nbdev."
  },
  {
    "objectID": "blog/posts/fractal-llms/02-nbdev/index.html#literate-programming",
    "href": "blog/posts/fractal-llms/02-nbdev/index.html#literate-programming",
    "title": "Lessong 2: Blogging with nbdev",
    "section": "Literate Programming",
    "text": "Literate Programming\nIn Literate Programming, notes and tests are woven directly into a project’s source code. Each piece is crucial to the overall approach and makes for better, more readable code. The original idea was proposed by Donald Knuth in 1984.\nThe code, documentation, and tests are all first-class citizens in Literate Programming. This is very different from the typical approach to software where tests and documentation live in a separate set of files.\nIn nbdev, a Jupyter Notebook is the single source of truth for code, documentation, and tests. Instead of managing three independent groups of files, everything is defined and happens in the Notebook. If the Notebook runs, then you know your code will run."
  },
  {
    "objectID": "blog/posts/fractal-llms/02-nbdev/index.html#exploratory-programming",
    "href": "blog/posts/fractal-llms/02-nbdev/index.html#exploratory-programming",
    "title": "Lessong 2: Blogging with nbdev",
    "section": "Exploratory Programming",
    "text": "Exploratory Programming\nExploratory Programming is an open-ended approach for tackling new problems and unknown domains. It’s very useful at the start of a project when details are still being finalized. And it can also bring out different angles to a known, established approach by showing it under a new light.\nNotebooks are both dynamic and interactive which makes them perfect for Exploratory Programming. They make the barrier for trying new things extremely low. And they’re downright fun!\nThe video below is an incredible talk by Bret Victor about the power of Exploratory Programming, with stunning examples throughout to drive the point home:"
  },
  {
    "objectID": "blog/posts/fractal-llms/02-nbdev/index.html#combining-literate-and-exploratory-programming",
    "href": "blog/posts/fractal-llms/02-nbdev/index.html#combining-literate-and-exploratory-programming",
    "title": "Lessong 2: Blogging with nbdev",
    "section": "Combining Literate and Exploratory Programming",
    "text": "Combining Literate and Exploratory Programming\nnbdev combines these two ideas in its workflow. We covered why this is a powerful combination in the Course Philosophy section of the syllabus. Please take a moment to re-read with the new context from above.\nWith this combo, we can interactively learn about and explore how something works. Iterations are fast and cheap so it’s easy to follow any hit of curiosity. And we can now develop and test code in a much more dynamic way than usual.\nThese ideas can be doled out on the fly. For example, at the start of a project we could lean Exploratory to map out the problem space. Then, as the idea matures, we can pivot to Literate to refine and crystallize our approach.\nAnd if anything breaks, we can always restart the Notebook and try again.\nNext let’s look at how nbdev can turn Notebooks into blog posts."
  },
  {
    "objectID": "blog/posts/fractal-llms/02-nbdev/index.html#references",
    "href": "blog/posts/fractal-llms/02-nbdev/index.html#references",
    "title": "Lessong 2: Blogging with nbdev",
    "section": "References",
    "text": "References\n\nOfficial nbdev tutorial\nBlogging with nbdev"
  },
  {
    "objectID": "blog/posts/fractal-llms/00-Overview/index.html",
    "href": "blog/posts/fractal-llms/00-Overview/index.html",
    "title": "Syllabus: Fractal-U Course on LLMs",
    "section": "",
    "text": "Overview and motivations for the Fractal-U LLM Course."
  },
  {
    "objectID": "blog/posts/fractal-llms/00-Overview/index.html#realistic-assistants",
    "href": "blog/posts/fractal-llms/00-Overview/index.html#realistic-assistants",
    "title": "Syllabus: Fractal-U Course on LLMs",
    "section": "Realistic Assistants",
    "text": "Realistic Assistants\nDespite the breakneck speed of LLM progress, AI Assistants as powerful as those are still a ways off. The exact timelines are hard, if not impossible, to predict. For now it is safe to say that Assistants of that caliber won’t be here anytime “soon”. But, barring some force majeure, they will exist at some point.\nThe gap, then, is that folks are promising advanced Assistants while on the ground we still deal with LLM hallucinations and prohibitive compute requirements."
  },
  {
    "objectID": "blog/posts/fractal-llms/00-Overview/index.html#building-an-ai-assistant",
    "href": "blog/posts/fractal-llms/00-Overview/index.html#building-an-ai-assistant",
    "title": "Syllabus: Fractal-U Course on LLMs",
    "section": "Building an AI Assistant",
    "text": "Building an AI Assistant\nWhere does that leave us? Well, as a recent announcement from OpenAI shows, fine-tuning a GPT-3.5 model on small, clean datasets surpasses GPT-4 on certain tasks.\nThat’s what we are aiming for. In other words, we already have the ability to develop outrageously powerful tools by fine-tuning LLMs on small, clean datasets.\nSo while we won’t make Iron Man’s Jarvis, we are aiming for far more than a simple chatbot. Think of our Assistant like a smart Rubber Duck. A Rubber Duck is anything (actual yellow rubber duckie optional) that you keep around your desk and talk to about your work. It is a physical tool for thought, since it’s so often helpful to speak out loud the swirl of thoughts in our head.\nOur simple Assistant will be a Rubber Duck that we can talk with. When we ask it a question about our work, it will respond given what it knows about the project as a whole. Or, if we are simply verbalizing a thought to untangle it, the Assistant can give us feedback or suggest other approaches.\nIf we can be so bold: our Assistant will be a mini-Jarvis laser-focused on a specific task. Then, as both the tools and field progresses, we will have a full development stack to unlock even more capabilities from this smart Rubber Duck.\n\nCourse Summary: We will fine-tune and augment an LLM on a small, clean dataset to build an Intelligent Rubber Duck."
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "",
    "text": "Running dynamic CFG with the Stable Diffusion v2 model."
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#seed-for-reproducibility",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed for rng\nSEED = 4191151944 \ndef seed_everything(seed: int) -&gt; torch.Generator:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#static-baselines",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#static-baselines",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Static baselines",
    "text": "Static baselines\nFirst we create the constant, baseline Guidances.\n\n# create the baseline schedule with the new function\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda *args, **kwargs: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)"
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#improving-the-baseline-with-scheduled-guidance",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#improving-the-baseline-with-scheduled-guidance",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Improving the baseline with scheduled Guidance",
    "text": "Improving the baseline with scheduled Guidance\nNow we build the most promising dynamic schedule: Inverse kDecay with a fast warmup.\n\n# creating the inverse kDecay cosine schedules\nk_decays = [0.1, 0.2, 0.3, 0.5]\ninv_k_params = {'k_decay': k_decays}\ninv_k_func = partial(cos_harness, default_params=DEFAULT_COS_PARAMS)\ninv_k_expts = create_expts(inv_k_params, inv_k_func)\n\n# invert the schedules to turn them into a type of warmup \n##TODO: move into the scheduler helper\nfor s in inv_k_expts:\n    s['schedule'] = [max_val - g + min_val for g in s['schedule']]\n\n# put all schedules together\nall_k_expts = inv_k_expts\n\nLet’s plot these schedules to see what they look like."
  },
  {
    "objectID": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#stable-diffusion-v2-images",
    "href": "blog/posts/2022-11-28-sd-v2-schedules-1/index.html#stable-diffusion-v2-images",
    "title": "Stable Diffusion v2 with dynamic Classifier-free Guidance",
    "section": "Stable Diffusion v2 images",
    "text": "Stable Diffusion v2 images\nHere we plot all of the generated images.\nThe image on the left is the baseline with a static, constant Guidance.\nThe images on the right are the improvements with Guidance scheduling. Specifically, using the Inverse-kDecay cosine schedules with different values of k.\n\nplot_all_results('stabilityai/stable-diffusion-2')"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "",
    "text": "Combining the best schedules and normalizations so far."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#recap-of-parts-1-5",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#recap-of-parts-1-5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Recap of Parts 1-5",
    "text": "Recap of Parts 1-5\nThe first five parts explored how to turn Classifier-free Guidance into a dynamic process. We found a good set of schedules and normalizations that seem to improve the output of diffusion image models."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#part-6-putting-it-all-together",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#part-6-putting-it-all-together",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Part 6: Putting it all together",
    "text": "Part 6: Putting it all together\nPart 6 brings together our best approaches so far. Specifically, it explores the following schedules:\n\nkDecay with large \\(k\\) values.\n\nInverse kDecay with small \\(k\\) values.\n\nOn all three Guidance normalizations:\n\nPrediction Normalization\n\nT-Normalization\n\nFull Normalization"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#image-parameters",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#creating-the-baseline-image-with-g-7.5",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#creating-the-baseline-image-with-g-7.5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Creating the baseline image with \\(G = 7.5\\)",
    "text": "Creating the baseline image with \\(G = 7.5\\)\nFirst we create the baseline image using a constant Classifier-free Guidance with \\(G = 7.5\\). Since this is a constant schedule, \\(k\\) does not come into play.\n\n# create the baseline schedule with the new function\nbaseline_g = 7.5\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda *args, **kwargs: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)\n\n\nbaseline_res = run(prompt, baseline_expts, guide_tfm=GuidanceTfm)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone.\n\n\n\n\n\n\n# view the baseline image\nbaseline_res['images'][0]"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Improving the baseline with schedules and normalizations",
    "text": "Improving the baseline with schedules and normalizations\nNow let’s run our kDecay schedules with normalizations. Then we can check how it changed the baseline image.\nSince every run starts from the exact same noisy latents, only the schedules and normalizations are affecting the output."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-runs",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Prediction Normalization runs",
    "text": "Prediction Normalization runs\n\nprint('Running the Prediction Norm experiments...')\nbase_norm_res = run(prompt, all_k_expts, guide_tfm=BaseNormGuidance)\n\nRunning the Prediction Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.BaseNormGuidance'&gt;\nRunning experiment [1 of 8]: Param: \"k_decay\", val=1...\nRunning experiment [2 of 8]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 8]: Param: \"k_decay\", val=5...\nRunning experiment [4 of 8]: Param: \"k_decay\", val=0.15...\nRunning experiment [5 of 8]: Param: \"k_decay\", val=0.2...\nRunning experiment [6 of 8]: Param: \"k_decay\", val=0.3...\nRunning experiment [7 of 8]: Param: \"k_decay\", val=0.5...\nRunning experiment [8 of 8]: Param: \"k_decay\", val=0.7...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-runs",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "T-Normalization runs",
    "text": "T-Normalization runs\n\nprint('Running the T-Norm experiments...')\nT_norm_res = run(prompt, all_T_k_expts, guide_tfm=TNormGuidance)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 8]: Param: \"k_decay\", val=1...\nRunning experiment [2 of 8]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 8]: Param: \"k_decay\", val=5...\nRunning experiment [4 of 8]: Param: \"k_decay\", val=0.15...\nRunning experiment [5 of 8]: Param: \"k_decay\", val=0.2...\nRunning experiment [6 of 8]: Param: \"k_decay\", val=0.3...\nRunning experiment [7 of 8]: Param: \"k_decay\", val=0.5...\nRunning experiment [8 of 8]: Param: \"k_decay\", val=0.7...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-runs",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Full Normalization runs",
    "text": "Full Normalization runs\n\nprint('Running the T-Norm experiments...')\nfull_norm_res = run(prompt, all_T_k_expts, guide_tfm=FullNormGuidance)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 8]: Param: \"k_decay\", val=1...\nRunning experiment [2 of 8]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 8]: Param: \"k_decay\", val=5...\nRunning experiment [4 of 8]: Param: \"k_decay\", val=0.15...\nRunning experiment [5 of 8]: Param: \"k_decay\", val=0.2...\nRunning experiment [6 of 8]: Param: \"k_decay\", val=0.3...\nRunning experiment [7 of 8]: Param: \"k_decay\", val=0.5...\nRunning experiment [8 of 8]: Param: \"k_decay\", val=0.7...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-results",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Prediction Normalization results",
    "text": "Prediction Normalization results"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-results",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "T-Normalization results",
    "text": "T-Normalization results"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-results",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Full-Normalization results",
    "text": "Full-Normalization results"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-comparison",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#prediction-normalization-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Prediction Normalization comparison",
    "text": "Prediction Normalization comparison"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-comparison",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#t-normalization-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "T-Normalization comparison",
    "text": "T-Normalization comparison"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-comparison",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#full-normalization-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Full Normalization comparison",
    "text": "Full Normalization comparison"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.15-across-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.15-across-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Comparing \\(k = 0.15\\) across normalizations",
    "text": "Comparing \\(k = 0.15\\) across normalizations"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.2-across-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.2-across-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Comparing \\(k = 0.2\\) across normalizations",
    "text": "Comparing \\(k = 0.2\\) across normalizations"
  },
  {
    "objectID": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.3-across-normalizations",
    "href": "blog/posts/2022-11-25-guidance-expts-7/index.html#comparing-k-0.3-across-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 6",
    "section": "Comparing \\(k = 0.3\\) across normalizations",
    "text": "Comparing \\(k = 0.3\\) across normalizations\n\n\n\n\n\nAt this point, the difference in quality between \\(0.15\\) and \\(0.2\\) becomes subjective. It does seem that 0.2 makes for more stable images across the normalizations. But, 0.15 fixed the astronaut’s leg and arm.\n\\(0.3\\) still improves the image, but we start to lose texture and coherence in the background."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "",
    "text": "Exploring the effect of k-decay on Cosine Schedules."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#recap-of-parts-1-3",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#recap-of-parts-1-3",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Recap of Parts 1-3",
    "text": "Recap of Parts 1-3\nThe first three parts explored how to turn Classifier-free Guidance into a dynamic process. We found an initial set of schedules and normalizers that seem to improve the quality of Diffusion images."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#part-4-alternative-warmups-for-cosine-schedules",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#part-4-alternative-warmups-for-cosine-schedules",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Part 4: Alternative warmups for cosine schedules",
    "text": "Part 4: Alternative warmups for cosine schedules\nPart 4 is an exploration of kDecay applied to Cosine Schedules.\nThe kDecay paper introduces a hyperparameter \\(k\\) for scheduled learning rates. This parameter empirically improves the performance of models across many learning rate schedules.\nHere we explore two aspects of \\(k\\) for the guidance parameter \\(G\\):\n\nThe effect of \\((\\ k\\ &lt;\\ 1\\ )\\) and \\((\\ k\\ &gt;\\ 1\\ )\\) on the guidance parameter.\n\nHow an inverse kDecay schedule can be used as a type of warm up."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#image-parameters",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#setting-the-baseline-with-g-7.5",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#setting-the-baseline-with-g-7.5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Setting the baseline with \\(G = 7.5\\)",
    "text": "Setting the baseline with \\(G = 7.5\\)\nFirst we create and display the baseline imagine using a constant Classifier-free Guidance with \\(G = 7.5\\). Since this is a constant schedule, \\(k\\) does not come into play.\n\n# create the baseline schedule with the new function\nbaseline_g = 7.5\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda params: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)\n\n\nbaseline_res = run(prompt, baseline_expts, guide_tfm=GuidanceTfm)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone.\n\n\n\n\n\n\n# view the baseline image\nbaseline_res['images'][0]"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#improving-the-baseline-with-k-schedules",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#improving-the-baseline-with-k-schedules",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Improving the baseline with \\(k\\) schedules",
    "text": "Improving the baseline with \\(k\\) schedules\nNow we sweep the Cosine Schedules with different \\(k\\) values. Then we will check the output images and compare them to the baseline."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-runs",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "k-Sweep runs",
    "text": "k-Sweep runs\n\nprint('Running the k-Sweep experiments...')\ncos_res = run(prompt, cos_expts, guide_tfm=GuidanceTfm)\n\nRunning the k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 10]: Param: \"k_decay\", val=0.1...\nRunning experiment [2 of 10]: Param: \"k_decay\", val=0.2...\nRunning experiment [3 of 10]: Param: \"k_decay\", val=0.3...\nRunning experiment [4 of 10]: Param: \"k_decay\", val=0.5...\nRunning experiment [5 of 10]: Param: \"k_decay\", val=0.7...\nRunning experiment [6 of 10]: Param: \"k_decay\", val=1.0...\nRunning experiment [7 of 10]: Param: \"k_decay\", val=1.5...\nRunning experiment [8 of 10]: Param: \"k_decay\", val=2...\nRunning experiment [9 of 10]: Param: \"k_decay\", val=3...\nRunning experiment [10 of 10]: Param: \"k_decay\", val=5...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-runs",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Inverse k-Sweep runs",
    "text": "Inverse k-Sweep runs\n\nprint('Running the Inverse-k-Sweep experiments...')\ninv_cos_res = run(prompt, inv_cos_expts, guide_tfm=GuidanceTfm)\n\nRunning the Inverse-k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 10]: Param: \"k_decay\", val=0.1...\nRunning experiment [2 of 10]: Param: \"k_decay\", val=0.2...\nRunning experiment [3 of 10]: Param: \"k_decay\", val=0.3...\nRunning experiment [4 of 10]: Param: \"k_decay\", val=0.5...\nRunning experiment [5 of 10]: Param: \"k_decay\", val=0.7...\nRunning experiment [6 of 10]: Param: \"k_decay\", val=1.0...\nRunning experiment [7 of 10]: Param: \"k_decay\", val=1.5...\nRunning experiment [8 of 10]: Param: \"k_decay\", val=2...\nRunning experiment [9 of 10]: Param: \"k_decay\", val=3...\nRunning experiment [10 of 10]: Param: \"k_decay\", val=5...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-results",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "k-Sweep results",
    "text": "k-Sweep results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-results",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Inverse k-Sweep results",
    "text": "Inverse k-Sweep results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-comparison",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#k-sweep-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "k-Sweep comparison",
    "text": "k-Sweep comparison"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-comparison",
    "href": "blog/posts/2022-11-24-guidance-expts-5/index.html#inverse-k-sweep-comparison",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 4",
    "section": "Inverse k-Sweep comparison",
    "text": "Inverse k-Sweep comparison\n\n\n\n\n\nIn both cases, with a high and smooth value of \\(G\\), the output image improved. We gained more details in the background, on the horse’s body, and on the astronaut’s gear."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "",
    "text": "Experiments with normalizations for Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#quick-recap-of-part-1",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#quick-recap-of-part-1",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Quick recap of Part 1",
    "text": "Quick recap of Part 1\nIn Part 1, we generated a baseline image using the default, static Classifier-free Guidance. To see if we could improve on the baseline, we swept a range of Cosine Schedules on the guidance parameter \\(G\\).\nTo recap the results of the sweep, there are a few promising guidance schedules to explore:\n\nSetting a higher guidance value.\n\nAllowing the Cosine schedule to go through multiple cycles.\n\nWarming up the guidance for a few steps."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#part-2-bringing-in-normalizations",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#part-2-bringing-in-normalizations",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Part 2: Bringing in Normalizations",
    "text": "Part 2: Bringing in Normalizations\nIn Part 2, we bring in normalizations as another kind of dynamic guidance.\nThe idea is that normalizing the guidance might improve the updates in the Diffusion model’s latent image space. To test this we explore three kinds of guidance normalizations:\n\nNormalizing the prediction by its overall norm.\n\nNormalizing the guidance update vector, \\(\\left(t - u\\right)\\), by its norm.\n\nCombining the Normalizations in 1. and 2.\n\n\n\n\n\n\n\nNote\n\n\n\nMore details about the normalizations can be found in this section of the original post.\n\n\nAfter these runs, we should have a good idea of both the schedules and normalizations that can improve Diffusion images. We will then combine the two approaches and explore other, more advanced schedules."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#leveraging-a-few-helper-libraries",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#leveraging-a-few-helper-libraries",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Leveraging a few helper libraries",
    "text": "Leveraging a few helper libraries\nWe use two new libraries that make it easier to run dynamic Classifier-free Guidances.\nThese two libraries are:\n\nmin_diffusion\ncf_guidance\n\nThe helper libraries remove a lot of overhead and boilerplate code. They allow us to jump straight to the important parts: running the guidance experiments.\nFor more details, the libraries were introduced in this earlier post."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#python-imports",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#python-imports",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Python Imports",
    "text": "Python Imports\nTo start we import the needed python modules.\nWe also handle random seeding to make sure that our results are reproducible across the series.\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom fastcore.all import L\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# seed for reproducibility\nSEED = 1024\ndef seed_everything(seed: int):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-23 14:58:50.779076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0"
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#the-min_diffusion-library",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#the-min_diffusion-library",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "The min_diffusion library",
    "text": "The min_diffusion library\nWe use the min_diffusion library to load a Stable Diffusion model from the HuggingFace hub.\n\n# to load Stable Diffusion pipelines\nfrom min_diffusion.core import MinimalDiffusion\n\n# helpers to plot the generated images\nfrom min_diffusion.utils import show_image, image_grid\n\n\nLoading the openjourney model from Prompt Hero\nThe following code loads the openjourney model in torch.float16 precision and puts it on the GPU.\n\nmodel_name = 'prompthero/openjourney'\ndevice     = 'cuda'\ndtype      = torch.float16\n\n\npipeline = MinimalDiffusion(model_name, device, dtype)\n\n\npipeline.load();\n\nEnabling default unet attention slicing."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#text-prompt-for-generations",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#text-prompt-for-generations",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Text prompt for generations",
    "text": "Text prompt for generations\nWe use the familiar, running prompt in our series to generate an image:\n\n“a photograph of an astronaut riding a horse”\n\n\n\n\n\n\n\nImportant\n\n\n\nThe openjourney model was fine-tuned to create images in the style of Midjourney v4.\nTo enable this fine-tuned style, we need to add the keyword \"mdjrny-v4\" at the start of the prompt.\n\n\n\n# text prompt for image generations\nprompt = \"mdjrny-v4 style a photograph of an astronaut riding a horse\""
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#image-parameters",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#image-parameters",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Image parameters",
    "text": "Image parameters\nThe images will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#wrapper-to-run-the-experiments",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#wrapper-to-run-the-experiments",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Wrapper to run the experiments",
    "text": "Wrapper to run the experiments\nThe run function below generates images from a given prompt.\nIt also takes an argument guide_tfm for the specific Guidance Transformation class that will guide the outputs. The schedules argument has the parameter values of \\(G\\) at each diffusion timestep.\n\ndef run(prompt, schedules, guide_tfm=None, generator=None,\n        show_each=False, test_run=False):\n    \"\"\"Runs a dynamic Classifier-free Guidance experiment. \n    \n    Generates an image for the text `prompt` given all the values in `schedules`.\n    Uses a Guidance Transformation class from the `cf_guidance` library.  \n    Stores the output images with a matching title for plotting. \n    Optionally shows each image as its generated.\n    If `test_run` is true, it runs a single schedule for testing. \n    \"\"\"\n    # store generated images and their title (the experiment name)\n    images, titles = [], []\n    \n    # make sure we have a valid guidance transform\n    assert guide_tfm\n    print(f'Using Guidance Transform: {guide_tfm}')\n    \n    # optionally run a single test schedule\n    if test_run:\n        print(f'Running a single schedule for testing.')\n        schedules = schedules[:1]\n        \n\n    # run all schedule experiments\n    for i,s in enumerate(schedules):\n        \n        # parse out the title for the current run\n        cur_title  = s['title']\n        titles.append(cur_title)\n        \n        # create the guidance transformation \n        cur_sched = s['schedule']\n        gtfm = guide_tfm({'g': cur_sched})\n        \n        print(f'Running experiment [{i+1} of {len(schedules)}]: {cur_title}...')\n        img = pipeline.generate(prompt, gtfm, generator=generator)\n        images.append(img)\n        \n        # optionally plot the image\n        if show_each:\n            show_image(img, scale=1)\n\n    print('Done.')\n    return {'images': images,\n            'titles': titles,}\n\nLet’s create the baseline image. The hope is that our guidance changes will then improve on it.\n\nbaseline_res = run(prompt, baseline_scheds, guide_tfm=GuidanceTfm, generator=generator)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone.\n\n\n\n\n\n\n# view the baseline image\nbaseline_res['images'][0]\n\n\n\n\nNot a bad starting point. Let’s see if we can do better."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#details-on-normalized-guidance-values",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#details-on-normalized-guidance-values",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Details on normalized Guidance values",
    "text": "Details on normalized Guidance values\nFor Prediction Normalization we can use the same static \\(G = 7.5\\) from the baseline.\nFor both T-Normalization and Full Normalization, however, we need a much smaller guidance value. The reason is that these normalizations scale the update vector \\(\\left( t - u \\right)\\) itself. That means that a large value like \\(G = 7.5\\) would de-scale the vectors even more! That is the exact situation we are trying to avoid with normalization in the first place.\nTo prevent this, we create a special \\(G_\\text{small}\\) schedule for T and Full Normalizations with a smaller value of \\(G_\\text{small} = 0.15\\)\n\n# create the baseline Classifier-free Guidance\nT_run = {'max_val': [0.15]}\n\n# parameters we are sweeping\nT_scheds = L()\n\n# step through each parameter\nfor idx,name in enumerate(baselines_names):\n    # step through each of its values\n    for idj,val in enumerate(T_run[name]):\n\n        # create the baseline experimeent\n        expt = {\n            'param_name': name,\n            'val': val,\n            'schedule': [val for _ in range(num_steps)]\n        }\n        # for plotting\n        expt['title'] = f'Param: \"{name}\", val={val}'\n        \n        # add to the running list of experiments\n        T_scheds.append(expt)"
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-runs",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-runs",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Prediction Norm runs",
    "text": "Prediction Norm runs\n\nprint('Running the BaseNorm experiments...')\nbase_norm_res = run(prompt, baseline_scheds, guide_tfm=BaseNormGuidance, generator=generator)\n\nRunning the BaseNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.BaseNormGuidance'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=7.5...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-runs",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-runs",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "T-Norm runs",
    "text": "T-Norm runs\n\nprint('Running the T-Norm experiments...')\nt_norm_res = run(prompt, T_scheds, guide_tfm=TNormGuidance, generator=generator)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=0.15...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-runs",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-runs",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Full Norm runs",
    "text": "Full Norm runs\n\nprint('Running the FullNorm experiments...')\nfull_norm_res = run(prompt, T_scheds, guide_tfm=FullNormGuidance, generator=generator)\n\nRunning the FullNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 1]: Param: \"max_val\", val=0.15...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-results",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#prediction-norm-results",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Prediction Norm results",
    "text": "Prediction Norm results\n\n\n\n\n\nComparing left to right, Prediction Normalization improves the overall image.\nThe horse’s body and hair are more defined. The clouds in the background have more texture. The lowest orb in the sky is much better defined. The shadows on the ground also have better coverage and a more natural transition. The ground itself has more details and texture, and is better separated from the background sky.\nEven the reflection on the astronaut’s helmet has more depth and looks smoother.\nOverall, it seems that Prediction Normalization is a global improvement on the baseline."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-results",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#t-norm-results",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "T-Norm results",
    "text": "T-Norm results\n\n\n\n\n\nThis one is much more interesting. T-Normalization completely changed the image! Even though they started from the exact same noisy latents.\nHere the horse’s anatomy, especially its head, look more correct. Even though we lost overall illumination on the horse’s body.\nThe patches and details on the astronaut’s gear are also better defined. And maybe it’s subjective, but this one feels more like a photograph (thanks to helmet’s glare) while the baseline looks more like digital art."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-results",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#full-norm-results",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Full Norm results",
    "text": "Full Norm results\n\n\n\n\n\nFull Normalization feels like a mix of the best from both worlds.\nThe horse’s anatomy and astronaut details are better, following the results from T-Normalization. And we regained some background vs. foreground separation from Prediction Normalization.\nIt seems this dual benefit came at the cost of some symmetry for the orbs in the sky, and a loss of resolution on the horse’s tail."
  },
  {
    "objectID": "blog/posts/2022-11-21-guidance-expts-3/index.html#analysis",
    "href": "blog/posts/2022-11-21-guidance-expts-3/index.html#analysis",
    "title": "Dynamic Classifier-free Guidance Pt. 2",
    "section": "Analysis",
    "text": "Analysis\nOverall, at least for this example, it is fair to say that normalizations can improve Diffusion images.\nEither the baseline image was improved overall (Prediction Normalization), or we gained better image syntax and details (T-Normalization and Full Normalization).\nGiven that T-Normalization and Full Normalization completely changed the style of the baseline image, there is a lot to explore here. To start, there is likely a much better set of \\(G_\\text{small}\\) values. Consider the baseline’s value of \\(G = 7.5\\). This value is the standard across many Diffusion models and empirically produces good results. Meanwhile, our \\(G_\\text{small} = 0.15\\) is only a starting point that has not been thoroughly tested.\nIn summary, it seems that Prediction Normalization could be an easy way to improve all Diffusion images. As for the others, they definitely have potential that should be explored further."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "",
    "text": "Experiments with cosine schedules for Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#imports",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#imports",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom fastcore.all import L\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nfrom tqdm.auto import tqdm \n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler\n\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-21 19:06:22.967865: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#helper-functions",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#helper-functions",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Helper functions",
    "text": "Helper functions\nThe functions below help with:\n\nGenerating text embeddings from a given prompt.\n\nConverting Diffusion latents to a PIL image.\n\nPlotting the images to visualize results.\n\n\ndef text_embeddings(prompts, maxlen=None):\n    \"Extracts text embeddings from the given `prompts`.\"\n    maxlen = maxlen or tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(device))[0]\n\n\ndef image_from_latents(latents):\n    \"Scales diffusion `latents` and turns them into a PIL Image.\"\n    \n    # scale and decode the latents\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        data = vae.decode(latents).sample[0]\n\n    # Create PIL image\n    data = (data / 2 + 0.5).clamp(0, 1)\n    data = data.cpu().permute(1, 2, 0).float().numpy()\n    data = (data * 255).round().astype(\"uint8\")\n    image = Image.fromarray(data)\n    return image\n    \n    \ndef show_image(image, scale=0.5):\n    \"Displays the given `image` resized based on `scale`.\"\n    img = image.resize(((int)(image.width * scale), (int)(image.height * scale)))\n    display(img)\n    return img\n\n\ndef image_grid(images, rows = 1, width=256, height=256, title=None):\n    \"Display an array of images in a grid with the given number of `rows`\"\n    count = len(images)\n    cols = int(count / rows)\n    if cols * rows &lt; count:\n        rows += 1\n    # Calculate fig size based on individual image sizes    \n    px = 1/plt.rcParams['figure.dpi']\n    w = cols * width * px\n    # Add some extra space for the caption/title since that can wrap\n    h = (rows * height * px) + (rows * 30 * px)\n    fig, axes = plt.subplots(rows, cols, figsize=(w, h))\n    for y in range(rows):\n        for x in range(cols):\n            index = y*cols + x\n            ref = axes[x] if rows == 1 else axes[y] if cols == 1 else axes[y, x]\n            ref.axis('off')\n            if index &gt; count - 1:\n                continue\n            img = images[index]\n            txt = f'Frame: {index}'\n            if title is not None:\n                if isinstance(title, str):\n                    txt = f'{title}: {index}'\n                elif isinstance(title, List):\n                    txt = title[index]\n            # small change for bigger, more visible titles\n            txt = '\\n'.join(wrap(txt, width=70))\n            ref.set_title(txt, fontsize='x-large')\n            ref.imshow(img)\n            ref.axis('off')"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#loading-a-diffusion-pipeline",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#loading-a-diffusion-pipeline",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Loading a Diffusion pipeline",
    "text": "Loading a Diffusion pipeline\nWe need to dynamically change the diffusion guidance parameter \\(G\\).\nThat means we need more control than what is available in the high-level HuggingFace APIs. To achieve this control, we load each piece of a Diffusion pipeline separately. Then, we can write our own image generation loop with full control over \\(G\\).\nThe get_sd_pieces function loads and returns the separate components of a Stable Diffusion pipeline.\n\ndef get_sd_pieces(model_name, dtype=torch.float32, better_vae='ema'):\n    \"Loads and returns the individual pieces in a Diffusion pipeline.\"\n    \n    # create the tokenizer and text encoder\n    tokenizer = CLIPTokenizer.from_pretrained(\n        model_name,\n        subfolder=\"tokenizer\",\n        torch_dtype=dtype)\n    text_encoder = CLIPTextModel.from_pretrained(\n        model_name,\n        subfolder=\"text_encoder\",\n        torch_dtype=dtype).to(device)\n\n    # we are using a VAE from stability that was trained for longer than the baseline \n    if better_vae:\n        assert better_vae in ('ema', 'mse')\n        vae = AutoencoderKL.from_pretrained(f\"stabilityai/sd-vae-ft-{better_vae}\", torch_dtype=dtype).to(device)\n    else:\n        vae = AutoencoderKL.from_pretrained(model_name, subfolder='vae', torch_dtype=dtype).to(device)\n    \n    # build the unet\n    unet = UNet2DConditionModel.from_pretrained(\n        model_name,\n        subfolder=\"unet\",\n        torch_dtype=dtype).to(device)\n    \n    # enable unet attention slicing\n    slice_size = unet.config.attention_head_dim // 2\n    unet.set_attention_slice(slice_size)\n        \n    # build the scheduler\n    scheduler = LMSDiscreteScheduler.from_config(model_name, subfolder=\"scheduler\")\n    \n    return (\n        tokenizer,\n        text_encoder,\n        vae,\n        unet,\n        scheduler,\n    )\n\n\nPicking a model\nThese runs use the openjourney model from Prompt Hero.\n\n\n\n\n\n\nImportant\n\n\n\nopenjourney was fine-tuned to create images in the style of Midjourney v4.\nTo trigger this style, we need to add the special keyword \"mdjrny-v4\" at the front of an input text prompt.\n\n\n\n# set the diffusion model\nmodel_name = \"prompthero/openjourney\"\n\n# other possible models\n# model_name = \"CompVis/stable-diffusion-v1-4\"\n# model_name = \"runwayml/stable-diffusion-v1-5\"\n\nNext we use the function get_sd_pieces to load this model. The pieces are loaded in float16 precision.\n\n# set the data type for the pipeline\ndtype = torch.float16\n\n# load the individual diffusion pieces\npieces = get_sd_pieces(model_name, dtype=dtype)\n(tokenizer, text_encoder, vae, unet, scheduler) = pieces"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#text-prompt-for-generations",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#text-prompt-for-generations",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Text prompt for generations",
    "text": "Text prompt for generations\nWe use the same input text prompt from the previous notebook:\n\n“a photograph of an astronaut riding a horse”\n\nBut, we add the special prefix keyword \"mdjrny-v4\" to create Midjourney-style images.\n\n# input text prompt for diffusion models\nprompt = \"mdjrny-v4 style a photograph of an astronaut riding a horse\""
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#generating-images",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#generating-images",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Generating images",
    "text": "Generating images\nThe images will be generated over \\(50\\) diffusion steps. They will have a height and width size of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512\n\n\nCreating a fixed starting point for diffusion\nThe code below creates an initial set of latent noise.\nThe idea is for every generation to start from this shared, fixed noise. That way we can be sure that only our guidance changes are having an effect on the output image.\n\n# create the shared, initial latents\nseed = 1024\ntorch.manual_seed(seed)\ninit_latents = torch.randn((1, unet.in_channels, height//8, width//8), dtype=unet.dtype, device=device)\n\n\n\nImage generation function\nBelow is the main image generation function: generate. It uses the Stable Diffusion components we loaded earlier.\nNote that this function is almost identical to the StableDiffusionPipeline from HuggingFace. The main difference is plugging in our Guidance Transform instead of doing the default Classifier-free Guidance update.\n\ndef generate(prompt, guide_tfm=None, width=width, height=height, steps=num_steps, **kwargs):\n    # make sure we have a guidance transformation\n    assert guide_tfm\n    \n    # prepare the text embeddings\n    text = text_embeddings(prompt)\n    uncond = text_embeddings('')\n    emb = torch.cat([uncond, text]).type(unet.dtype)\n    \n    # start from the shared, initial latents\n    latents = torch.clone(init_latents)\n    scheduler.set_timesteps(steps)\n    latents = latents * scheduler.init_noise_sigma\n    \n    # run the diffusion process\n    for i,ts in enumerate(tqdm(scheduler.timesteps)):\n        inp = scheduler.scale_model_input(torch.cat([latents] * 2), ts)\n        with torch.no_grad(): \n            tf = ts\n            if torch.has_mps:\n                tf = ts.type(torch.float32)\n            u,t = unet(inp, tf, encoder_hidden_states=emb).sample.chunk(2)\n        \n        # call the guidance transform\n        pred = guide_tfm(u, t, idx=i)\n        \n        # update the latents\n        latents = scheduler.step(pred, ts, latents).prev_sample\n        \n    # decode and return the final latents\n    image = image_from_latents(latents)\n    return image"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#default-schedule-parameters",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#default-schedule-parameters",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Default schedule parameters",
    "text": "Default schedule parameters\nWe start from the guidance schedule value from the previous notebook.\nRecall that there were three kinds of schedules:\n\nA static schedule with a constant \\(G\\).\n\nA decreasing Cosine schedule.\n\nA Cosine schedule with some initial warm up steps.\n\n\n# Default schedule parameters from the blog post\n######################################\nmax_val           = 7.5   # guidance scaling value\nmin_val           = 1     # minimum guidance scaling\nnum_steps         = 50    # number of diffusion steps\nnum_warmup_steps  = 0     # number of warmup steps\nwarmup_init_val   = 0     # the intial warmup value\nnum_cycles        = 0.5   # number of cosine cycles\nk_decay           = 1     # k-decay for cosine curve scaling \n######################################\n\nTo make sure our changes always reference this shared starting point, we can wrap these parameters in a dictionary.\n\nDEFAULT_COS_PARAMS = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        num_cycles,\n    'k_decay':           k_decay,\n    'num_warmup_steps':  num_warmup_steps,\n    'warmup_init_val':   warmup_init_val,\n}\n\nThen, every minimum-pair change will start from this shared dictionary and update a single parameter. The cos_harness below gives us an easy way of making these minimum-pair changes.\n\ndef cos_harness(new_params={}, cos_params=DEFAULT_COS_PARAMS):\n    '''Creates cosine schedules with updated parameters in `new_params`'''\n    \n    # start from the given baseline `cos_params`\n    cos_params = dict(cos_params)\n    \n    # update the schedule with any new parameters\n    if new_params: cos_params.update(new_params)\n    \n    # return the new cosine schedule\n    sched = get_cos_sched(**cos_params)\n    return sched\n\nLet’s use the cosine harness to plot three test schedules, just to make sure things are working:\n\nThe baseline with no warmup.\n\nWarmup for 5 steps.\n\nWarmup for 10 steps.\n\n\n\n\n\n\n\nNote\n\n\n\nThe schedule plotting function plot_schedules is available in the post’s notebook.\n\n\n\n# plot cosine schedules with different number of warmup steps\nwarmup_steps = (0, 5, 10)\nwarm_g = L( \n    {'sched': cos_harness({'num_warmup_steps': w}), \n     'title': f'Warmup Steps: {w}'}\n    for w in warmup_steps\n)\n\n# plot the schedules\nprint('Plotting sample cosine schedules...')\nplot_schedules(warm_g.itemgot('sched'), rows=1, titles=warm_g.itemgot('title'))\n\nPlotting sample cosine schedules..."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#creating-the-cosine-experiments",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#creating-the-cosine-experiments",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Creating the Cosine experiments",
    "text": "Creating the Cosine experiments\nNow we can create the different Cosine schedules that will be swept.\n\ncos_param_sweep = {\n    'num_warmup_steps': [5, 10, 15],\n    'num_cycles':       [1, 1.5],\n    'k_decay':          [0.8, 0.6],\n    'max_val':          [10],\n    'min_val':          [3],\n}\n\nparam_names = sorted(list(cos_param_sweep))\n\ncos_scheds = L()\nfor idx,name in enumerate(param_names):\n    for idj,val in enumerate(cos_param_sweep[name]):\n\n        # create the cosine experimeent\n        expt = {\n            'param_name': name,\n            'val': val,\n            'schedule': cos_harness({name: val})\n        }\n        # for plotting\n        expt['title'] = f'Param: \"{name}\", val={val}'\n        \n        cos_scheds.append(expt)\n    \n\n\nplot_schedules(cos_scheds.itemgot('schedule'), rows=3, titles=cos_scheds.itemgot('title'))"
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#running-the-cosine-experiments",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#running-the-cosine-experiments",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Running the cosine experiments",
    "text": "Running the cosine experiments\nWe use the run function from before to run all of the cosine experiments.\n\ncos_res = run(prompt, cos_scheds, guide_tfm=GuidanceTfm, show_each=False)\n\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.GuidanceTfm'&gt;\nRunning experiment [1 of 9]: Param: \"k_decay\", val=0.8...\nRunning experiment [2 of 9]: Param: \"k_decay\", val=0.6...\nRunning experiment [3 of 9]: Param: \"max_val\", val=10...\nRunning experiment [4 of 9]: Param: \"min_val\", val=3...\nRunning experiment [5 of 9]: Param: \"num_cycles\", val=1...\nRunning experiment [6 of 9]: Param: \"num_cycles\", val=1.5...\nRunning experiment [7 of 9]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [8 of 9]: Param: \"num_warmup_steps\", val=10...\nRunning experiment [9 of 9]: Param: \"num_warmup_steps\", val=15...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#analysis",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#analysis",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Analysis",
    "text": "Analysis\nCertain Cosine schedules seem promising. They either increase the details of the astronaut or background, or they create more anatomically correct horses.\nIn the rest of the series, we will explore the promising Cosine changes:\n\nSetting a higher Guidance ceiling.\n\nAllowing the Cosine to go through multiple cycles.\n\nWarming up for a few steps."
  },
  {
    "objectID": "blog/posts/2022-11-20-guidance-expts-2/index.html#bringing-in-normalizations",
    "href": "blog/posts/2022-11-20-guidance-expts-2/index.html#bringing-in-normalizations",
    "title": "Dynamic Classifier-free Guidance Pt. 1",
    "section": "Bringing in Normalizations",
    "text": "Bringing in Normalizations\nIn the previous notebooks, we found that normalization can have a huge improvement on generated images. The next logical step is to add normalizations to our schedules to see if the gains compound."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "",
    "text": "Changing the Classifier-Free Guidance parameter during diffusion."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#classifier-free-guidance-overview",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#classifier-free-guidance-overview",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Classifier-free Guidance overview",
    "text": "Classifier-free Guidance overview\nClassifier-free Guidance is a way of steering the outputs of Diffusion models to better align with a given input. It is a key aspect of how we are able to type in a text prompt and get back a relevant, generated image.\nCFG was needed because, by default, a Diffusion model starts from pure noise and randomly “walks” to unearth an image. Classifier-free Guidance can instead align the output according to a known, specific input. This known input is usually a meaningful piece of context like a sentence, or a segment of speech, or even another image.\nIn summary: Instead of randomly walking to generate random images, CFG allows Diffusion models to create targeted outputs.\n\nCFG Formula\nCFG updates the unconditioned latents to better match the conditional inputs as follows:\n\\[\\hat{\\epsilon}(x \\ |\\  y) = \\epsilon(x) + G\\left(\\ \\epsilon(x\\  |\\  y) - \\epsilon(x)\\ \\right)\\]\nWe can think of this equation as a type of moving average. To be more specific, the terms are:\n\n\n\nEquation Term\nDescription\n\n\n\n\n\\(\\epsilon(x)\\)\nUnconditioned noise prediction\n\n\n\\(\\epsilon(x\\ |\\ y)\\)\nConditional noise prediction\n\n\n\\(G\\)\nGuidance scaling factor\n\n\n\\(\\hat{\\epsilon}(x\\ |\\ y)\\)\nThe final, guided prediction.\n\n\n\nAs several people have noticed, this update is not balanced. The reason for the unbalance is that \\(G\\) is usually a large, fixed scalar. For example the default \\(G\\) in Stable Diffusion pipelines is \\(G = 7.5\\).\nThis brings up two questions:\n\nDoes a large \\(G\\) make the vectors too different?\n\nShould \\(G\\) be a fixed constant throughout the entire diffusion process?\n\nFahim compiled the forum’s answers to these questions in this notebook. His work compares both different normalizations and schedules for the Guidance parameter.\nAt first glance, it seems that both normalizing and scheduling the diffusion parameter improves the generated images. These better images are achieved for “free”, in the sense that we didn’t need any fine-tuning or new data.\nLet’s take a look at some of the details and benefits of a dynamic guidance parameter.\n\n\n\n\n\n\nNote\n\n\n\nAs Ben Poole points out in Jeremy’s twitter thread, these ideas are not new on their own.\nOne of the scalings was described in Guided-TTS for Speech diffusion. The normalizations are also related to the ones in Pretraining is All You Need for Image-to-Image Translation by Wang et. al. \nOur normalizations are similar in spirit to the Dynamic Thresholding in the Imagen paper."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#normalizing-the-guidance",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#normalizing-the-guidance",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Normalizing the guidance",
    "text": "Normalizing the guidance\nThis notebook explores two types of normalization we call BaseNorm and T-Norm:\n\nBaseNorm: Normalize the entire prediction by the ratio of the conditioned and unconditioned norms.\n\\[\\hat{\\epsilon}(x \\ |\\  y)_\\text{BaseNorm} = \\hat{\\epsilon}(x \\ |\\  y)\\cdot \\frac{\\|\\epsilon(x)\\|}{\\|\\epsilon(x \\ |\\  y)\\|}\\]\nT-Norm: Normalize the difference of the conditioned and unconditioned predictions. \\[\\hat{\\epsilon}(x \\ |\\  y)_\\text{TNorm} = \\epsilon(x) + G\\ \\frac{\\epsilon(x \\ |\\  y) - \\epsilon(x)}{\\|\\epsilon(x \\ |\\  y) - \\epsilon(x)\\|\\cdot \\|\\epsilon(x)\\|}\\]"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#scheduling-the-guidance",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#scheduling-the-guidance",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Scheduling the guidance",
    "text": "Scheduling the guidance\nIn standard CFG the guidance scaling value is fixed. But since the final and initial images are so different, should we expect that the same value is optimal for the entire time?\nTo explore this question we can borrow from Neural Network optimizers. Specifically, our idea of a “guidance schedule” is based on the popular schedules for learning rates.\nThis notebook explores two new schedules for the CFG parameter \\(G\\):\n\nCosine\n\nCosine with Warmup."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-the-changes",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-the-changes",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Combining the changes",
    "text": "Combining the changes\nThe natural idea is to combine these approaches: we should both normalize and schedule \\(G\\).\nAfter exploring each change in isolation we combine them to see their joint effects."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#python-imports",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#python-imports",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Python imports",
    "text": "Python imports\nFirst we import the python, PyTorch, and HuggingFace modules that we need. We also use the timm library for its built-in Cosine schedules.\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom textwrap import wrap\nfrom tqdm.auto import tqdm \n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import notebook_login\nfrom diffusers import StableDiffusionPipeline\nfrom diffusers import AutoencoderKL, UNet2DConditionModel\nfrom diffusers import LMSDiscreteScheduler\n\n# use cosine scheduler from timm\nfrom timm.scheduler.cosine_lr import CosineLRScheduler\nfrom timm.optim import create_optimizer\nfrom timm import create_model\n\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-20 19:51:47.940762: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#prompt-for-image-generations",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#prompt-for-image-generations",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Prompt for image generations",
    "text": "Prompt for image generations\nWe use the following prompt to test our guidance changes:\n\n“a photograph of an astronaut riding a horse”\n\nThis is the same prompt folks used in the forums. It seems like a good, simple starting point for future runs.\n\n# the input prompt for diffusion\nprompt = \"a photograph of an astronaut riding a horse\""
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#picking-a-diffusion-model",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#picking-a-diffusion-model",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Picking a Diffusion model",
    "text": "Picking a Diffusion model\nWe also have to pick a Diffusion model. Some possible options are:\n\nstable-Diffusion-v1-4 from CompVis.\n\nstable-Diffusion v1-5 from Runway.ml.\n\nHere we use the Stable Diffusion v1-4 model from CompVis.\nBut it is worth mentioning that this code will work with any Diffusion model name on the HuggingFace hub.\n\n# set the diffusion model\nmodel_name = \"CompVis/stable-diffusion-v1-4\" # \"runwayml/stable-diffusion-v1-5\""
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#utility-functions.",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#utility-functions.",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Utility functions.",
    "text": "Utility functions.\nNext we define some helper functions.\nThese helpers create the text embeddings, convert latent features into images, and plot the decoded images. All of these functions are directly from Fahim’s notebook.\n\ndef text_embeddings(prompts, maxlen=None):\n    \"Extracts text embeddings from the given `prompts`.\"\n    maxlen = maxlen or tokenizer.model_max_length\n    inp = tokenizer(prompts, padding=\"max_length\", max_length=maxlen, truncation=True, return_tensors=\"pt\")\n    return text_encoder(inp.input_ids.to(device))[0]\n\n\ndef image_from_latents(latents):\n    \"Scales the diffusion `latents` and turns them into a PIL Image.\"\n    \n    # scale and decode the latents\n    latents = 1 / 0.18215 * latents\n    with torch.no_grad():\n        data = vae.decode(latents).sample[0]\n\n    # Create PIL image\n    data = (data / 2 + 0.5).clamp(0, 1)\n    data = data.cpu().permute(1, 2, 0).float().numpy()\n    data = (data * 255).round().astype(\"uint8\")\n    image = Image.fromarray(data)\n    return image\n    \n    \ndef show_image(image, scale=0.5):\n    \"Displays the given `image` resized based on `scale`.\"\n    img = image.resize(((int)(image.width * scale), (int)(image.height * scale)))\n    display(img)\n    return img\n\n\ndef image_grid(images, rows = 1, width=256, height=256, title=None):\n    \"Display an array of images in a nice grid, or single row\"\n    count = len(images)\n    cols = int(count / rows)\n    if cols * rows &lt; count:\n        rows += 1\n    # Calculate fig size based on individual image sizes    \n    px = 1/plt.rcParams['figure.dpi']\n    w = cols * width * px\n    # Add some extra space for the caption/title since that can wrap\n    h = (rows * height * px) + (rows * 30 * px)\n    fig, axes = plt.subplots(rows, cols, figsize=(w, h))\n    for y in range(rows):\n        for x in range(cols):\n            index = y*cols + x\n            ref = axes[x] if rows == 1 else axes[y] if cols == 1 else axes[y, x]\n            ref.axis('off')\n            if index &gt; count - 1:\n                continue\n            img = images[index]\n            txt = f'Frame: {index}'\n            if title is not None:\n                if isinstance(title, str):\n                    txt = f'{title}: {index}'\n                elif isinstance(title, List):\n                    txt = title[index]\n            # small change for bigger, more visible titles\n            txt = '\\n'.join(wrap(txt, width=70))\n            ref.set_title(txt, fontsize='x-large')\n            ref.imshow(img)\n            ref.axis('off')"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#plotting-the-cosine-schedules",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#plotting-the-cosine-schedules",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Plotting the Cosine schedules",
    "text": "Plotting the Cosine schedules\nLet’s plot these new schedules to compare them against the previous, constant guidance."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#making-schedules-for-guidancetfm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#making-schedules-for-guidancetfm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Making schedules for GuidanceTfm",
    "text": "Making schedules for GuidanceTfm\nWe start with the following family of Guidance schedules:\n- Constant guidance with \\(\\left(G = 7.5\\right)\\)\n- Constant guidance with \\(\\left(G = 0.15\\right)\\)\n- A cosine schedule from \\(\\left(G = 7.5\\right)\\) down to \\(\\left(G = 0.15\\right)\\)\n- A cosine schedule that warms up to \\(\\left(G = 7.5\\right)\\) over the first 10% of steps\nFor the T-Norm experiments, we also define a smaller-valued cosine schedule:\n- T-Norm cosine schedule from \\(\\left(G = 0.25\\right)\\) down to \\(\\left(G = 0.05\\right)\\)\nThe schedule maps below will be the arguments to our GuidanceTfm instances. \n\n# baseline constant schedules with min and max values\nmax_sched        = {'g': [max_g] * num_steps}\nmin_sched        = {'g': [min_g] * num_steps}\n\n# cosine schedules\ncos_sched        = {'g': cos_g}\ncos_warmup_sched = {'g': warmup_cos_g}\n\n# normalized cosing schedules for T and Full-scale guidance\nsmall_cos_sched = {'g':  t_scale_cos_g}"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#recreating-the-forum-ideas",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#recreating-the-forum-ideas",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Recreating the forum ideas",
    "text": "Recreating the forum ideas\nFirst, let’s recreate the experiment baselines from the forums and Fahim’s notebook.\n\n# stores the guidance experiements to run\nexpts = {}\n\n\n### RECREATE SCALING RUNS FROM fast.ai FORUM POSTS\n#################################################\n#################################################\nbaseline        = GuidanceTfm(max_sched)       # 1) No scaling, guidance fixed to 7.5\nscale_base_hi_g = BaseNormGuidance(max_sched)  # 2) Scale the \"whole\" update\nscale_T_lo_g    = TNormGuidance(min_sched)     # 3) Scale the update of \"t\"\nscale_all_hi_g  = FullNormGuidance(min_sched)  # 4) Scale everything (steps 2 + 3)\n\n# add baselines to the experiment list\nexpts[f'NoNorm_FixedG_{max_g:.2f}']   = baseline\nexpts[f'BaseNorm_FixedG_{max_g:.2f}'] = scale_base_hi_g\nexpts[f'TNorm_FixedG_0{min_g:.2f}']   = scale_T_lo_g\nexpts[f'FullNorm_FixedG_{min_g:.2f}'] = scale_all_hi_g\n#################################################"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-scales-and-schedules",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#combining-scales-and-schedules",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Combining scales and schedules",
    "text": "Combining scales and schedules\nNext, we leverage our GuidanceTfm class to easily make new experiments.\nWe create the following:\n\nDefault and BaseNorm Guidance with Cosine and Cosine Warmup schedules.\nT-Norm and FullNorm Guidance with the smaller T-Cosine schedule.\n\n\n# group the cosine to run, and their names for plotting\nname2sched = {\n    'Cos':        cos_sched,\n    'CosWarmup':  cos_warmup_sched,\n    'TCos':       small_cos_sched,\n}\n\n\n# T-Norm and FullNorm guidance with small T-Cosine\nnorm_scalers = [TNormGuidance, FullNormGuidance]\nfor scaler in norm_scalers:\n    \n    # step through all cosine schedules\n    for name in ['TCos']:\n\n        # experiment for this (scaling, schedule) pair\n        expt = scaler(name2sched[name])\n        # unique name for this experiment\n        expt_name = f'{scaler.name}_Sched_{name}'\n\n        # add scaler to lists of experiments\n        expts[expt_name] = expt\n\n        \n# Default and BaseNorm guidance with cosine schedules \ng_scalers = [GuidanceTfm, BaseNormGuidance]\nfor scaler in g_scalers:\n    \n    # step through all cosine schedules\n    for name in ['Cos', 'CosWarmup']:\n\n        # experiment for this (scaling, schedule) pair\n        expt = scaler(name2sched[name])\n        # unique name for this experiment\n        expt_name = f'{scaler.name}_Sched_{name}'\n\n        # add scaler to lists of experiments\n        expts[expt_name] = expt\n\nHere we print all of the queued experiments:\n\nprint(\"Guidance experiments to run:\\n\")\nprint('\\n'.join(f'{k}' for k,_ in expts.items()))\n\nGuidance experiments to run:\n\nNoNorm_FixedG_7.50\nBaseNorm_FixedG_7.50\nTNorm_FixedG_00.15\nFullNorm_FixedG_0.15\nTNormGuidance_Sched_TCos\nFullNormGuidance_Sched_TCos\nCFGuidance_Sched_Cos\nCFGuidance_Sched_CosWarmup\nBaseNormGuidance_Sched_Cos\nBaseNormGuidance_Sched_CosWarmup"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#showing-all-images-side-by-side",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#showing-all-images-side-by-side",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Showing all images side by side",
    "text": "Showing all images side by side\nOur starting image, the baseline, is in the top-left. All other images are from different Guidance normalizations and schedules.\n\n\n\n\n\nThat’s a lot of images. Thankfully, there is one result that stands out above the rest:"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#biggest-improvement-cosine-with-t-norm-and-fullnorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#biggest-improvement-cosine-with-t-norm-and-fullnorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Biggest Improvement: Cosine with T-Norm and FullNorm",
    "text": "Biggest Improvement: Cosine with T-Norm and FullNorm\nThere seems to be a consistent gain from using either T-Norm or FullNorm with a Cosine schedule.\nThe image below compares our baseline to T-Norm and Cosine schedule. We can see:\n\nA more semantically correct horse (it has all of its legs!).\n\nBetter details and colors in the background.\n\nThe horse’s body is still not quite right, but it’s a marked improvement from the baseline."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#cosine-t-norm-vs.-cosine-fullnorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#cosine-t-norm-vs.-cosine-fullnorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Cosine T-Norm vs. Cosine FullNorm",
    "text": "Cosine T-Norm vs. Cosine FullNorm\nThese images are close, and both are better than the baseline. It seems we traded some background quality for subject quality with FullNorm vs. T-Norm."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs. BaseNorm",
    "text": "Original vs. BaseNorm\nHere we plot our default image and the result from BaseNorm.\nThe differences are subtle, but track the general observations from the forums:\n- More detail in the backgrounds.\n- Better shadowing on subjects.\n- Some moderate clarity gains."
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-t-norm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-t-norm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs. T-Norm",
    "text": "Original vs. T-Norm"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-fullnorm",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-fullnorm",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs. FullNorm",
    "text": "Original vs. FullNorm"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-cosine",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-cosine",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs. Cosine",
    "text": "Original vs. Cosine"
  },
  {
    "objectID": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm-with-cosine",
    "href": "blog/posts/2022-11-15-guidance-expts-1/index.html#original-vs.-basenorm-with-cosine",
    "title": "Intro to normalizing and scheduling Classifier-free Guidance",
    "section": "Original vs. BaseNorm with Cosine",
    "text": "Original vs. BaseNorm with Cosine"
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "",
    "text": "Creating complex-valued Rayleigh initializations for neural networks."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-numbers-a-brief-recap",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-numbers-a-brief-recap",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Complex Numbers: A brief recap",
    "text": "Complex Numbers: A brief recap\n Complex numbers have two components:\n- A real part.\n- An imaginary part.\nThe real component is a regular number like we would find on a plain number line. The imaginary component exists along the i axis.\nTo keep things simple, we can think of these numbers on a two-dimensional plot. The real number is on the x-axis while the imaginary number is on the y-axis.\n\nStarting with a real number\nPlotting examples is a great way to make things concrete. We first plot a regular, real number that we are all familiar with: \\(x = 2\\)\n\n\n\n\n\n\n\nMagnitude of a real number\nThe distance from the origin to our number tells us its magnitude. With positive values this feels redundant, since the magnitude is always the number itself.\nBut what about negative numbers? That is where the absolute value, represented as \\(|x|\\), comes into play. If we had picked \\(x = -2\\) instead, the magnitude would still be the same: \\(|-2| = |2| = 2\\).\nSo for any real number, positive or negative, we can find its magnitude by drawing an arrow starting from the origin \\(0\\). The absolute length of the arrow will be the number’s magnitude.\nWhy are we spelling out this aspect of numbers so much? That will become clear when we introduce the imaginary component next.\n\n\nAdding an imaginary component\nWe will keep our real component the same: \\(x = 2\\).\nBut now, let’s an imaginary component: \\(y = 3\\), to turn it into a complex number.\nWhat does this new complex number look like? We can visualize it on a 2D plot:\n\n\n\n\n\nWe combined these two components to get a complex number! Let’s call this number \\(z\\).\n\\(z\\) will be defined as: \\(z = x + iy\\)\nThe “\\(i\\)” next to a number means that it is the imaginary component.\n\n\nMagnitude of a complex number\nWhile we could use the real and imaginary components, there is another representation of complex numbers that will be more useful to us. This other representation is the magnitude and phase of a complex number.\nRemember how for a real number, its magnitude was the length of an arrow starting from the origin? The same idea applies to complex numbers. With one new detail: we have two components now, so our arrow’s length will be different.\nLet’s first draw our new complex number as an arrow.\n\n\n\n\n\nThe formula to compute the magnitude of a complex number \\(z\\) is:\n\\[|z| = \\sqrt{x^{2} + y^{2}}\\]\nPlugging in our \\(x\\) and \\(y\\) values gives our complex \\(z\\) a magnitude of:\n\\[|z| = \\sqrt{2^{2} + 3^{2}} = \\sqrt{4 + 9} = \\sqrt{13}\\]\nWhile knowing the magnitude is important, it is not enough to fully describe \\(z\\). For example what if instead of \\((x = 2, y = 3)\\) we had swapped them around as \\((x = 3, y = 2)\\). If we plug these values into the magnitude equation we get back the exact same number \\(\\sqrt{13}\\).\nBut looking at our 2D plots, these swapped points would obviously be in different locations. So if we were given only the magnitude, how could we tell that it came from our true, original \\(z\\)?\n\n\nPhase: telling complex magnitudes apart from each other\nThe way to tell two complex numbers with the same magnitude apart lies in the fact that the arrows are no longer flat along the x-axis.\nInstead they are now elevated (“pulled up”) by the imaginary component \\(y = 3\\). The complex number now has an angle respective to the x-axis.\nThis angle, together with a magnitude, is enough to perfectly describe our complex \\(z\\). In other words: we know both how long to make the vector and where to point it.\nLet’s complete the picture by including the angle of \\(z\\):\n\n\n\n\n\nWe use \\(\\theta\\) to represent the angle. The formula to compute \\(\\theta\\) is:\n\\[\\theta = \\arctan{\\frac{y}{x}}\\]\nPlugging in \\(x\\) and \\(y\\) for our complex number \\(z\\) gives us an angle of:\n\\[\\theta = \\arctan{\\frac{3}{2}} = 56.31 ^\\circ\\]\nWith the phase and magnitude, we now have a unique way of representing our complex number \\(z\\)."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#recap-complex-numbers",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#recap-complex-numbers",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Recap: Complex Numbers",
    "text": "Recap: Complex Numbers\nIn this section, we gave a brief overview of complex numbers and their representation. To make things concrete, we picked a complex number \\(z\\) with a real component \\(x = 2\\) and an imaginary component \\(y = 3\\).\nThen, we showed that we can perfectly represent this complex number \\(z\\) with two pieces of information: its magnitude and its phase.\n\nMagnitude: the length of a vector.\n\nPhase: the angle, or direction, where a vector is pointing."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#background-on-neural-network-initializations",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#background-on-neural-network-initializations",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Background on neural network initializations",
    "text": "Background on neural network initializations\nWhile initializations are now taken for granted, they were part of the first key pieces that made it possible to train deep neural networks. Before we knew how to properly initialize networks, training was very unstable as the gradients would either diverge or collapse to 0. This is known as gradient explosion or vanishing, respectively.\nThe main insights to prevent gradients from vanishing or exploding came from analyzing their variance during training.\n&gt; Aside: this is still an important error analysis tool! Looking at the behavior and distribution of gradients is a surefire way to catch problems with the training. Especially during the earliest optimizer steps.\n\nAchieving smooth gradient flows\nIt was the seminal work by He and Glorot, Bengio that showed how to control the variance of gradients to make sure that training was successful. They found that the variance of the sampling distributions, either Normal or Uniform, must meet certain criteria for the gradients to flow “smoothly”.\nHere, “smoothly” means that the gradients neither disappear nor explode during training.\nThe initializations derived in these papers are now the defaults in popular deep learning libraries like TensorFlow and pytorch.\nUnfortunately, the theory of complex-valued neural networks is not as well established. How can we know what are good variances and distributions for complex weights?\nIt turns out we can borrow these hard-earned lessons about good real-valued initializations to make sure that our complex gradients flow smoothly."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-complex-magnitudes",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-complex-magnitudes",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Initializing complex magnitudes",
    "text": "Initializing complex magnitudes\nInstead of drawing from a Normal or Uniform distribution, like we do for real-valued networks, the magnitudes will instead be drawn from a Rayleigh distribution. The reasons for this are described below. We can think of a Rayleigh distribution as the complex version of the familiar Normal distribution we use for real-valued weights."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-phases",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#initializing-phases",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Initializing phases",
    "text": "Initializing phases\nThe phases will be drawn from a Uniform distribution. To see why, think about a compass with 360 degrees to choose from.\nWe could randomly pick a degree and start walking in that direction for a given amount of time. Assuming we are on a flat surface, each degree choice will place us in a different, unique location.\nBecause we don’t know which direction our learned complex weights should point in, the best we can do is to start by randomly pointing everywhere and letting the gradients steer the vectors instead."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#why-rayleigh",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#why-rayleigh",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Why Rayleigh?",
    "text": "Why Rayleigh?\nWhy do we choose the Rayleigh distribution? The reason is that, without having more information about what our complex magnitudes should be, it is the best, unbiased starting point for the network.\nIn other words, we pick the maximum entropy distribution to avoid a-priori biasing our network toward any particular outcome. One of the successes of Deep Learning has been that it’s best to let the learning procedure figure out the values on its own in its higher dimensional activation feature space.\nThis is the complex-valued version of the same logic for using Normal or Uniform distribution to initialize real-valued networks."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#details-of-the-rayleigh-distribution",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#details-of-the-rayleigh-distribution",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Details of the Rayleigh distribution",
    "text": "Details of the Rayleigh distribution\nLet’s dive into the details. The equation below is the Probability Density Function (PDF) of the Rayleigh distribution.\n\\[f(x,\\sigma) = \\frac{x}{\\sigma^2}e^{-x^2/(2\\sigma^2)}, \\ \\ x \\geq 0\\]\nThis equation is a bit intimidating in written form. Let’s instead code it up as a python function with NumPy to make it cleaner.\n\n# start by importing the libraries we need\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom numpy.random import default_rng\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# define the Rayleigh PDF\ndef rayleigh_pdf(x, sigma):\n    \"Evaluates the Rayleigh PDF at a given point `x`.\"\n    p = (x / sigma**2) * np.exp(-x**2 / (2*sigma**2)) # see if you can match this code to the equation above\n    return p\n\nThe parameter sigma \\((\\sigma)\\) is known as the distribution’s scale. It is commonly found in many probability distributions and often controls how spread out or narrow a distribution is.\nLet us start by setting \\(\\sigma = 1\\) to draw the “basic” Rayleigh shape. We will then change sigma to see how this affects the distribution’s shape.\n\n# start with sigma of one as the base case\nsigma = 1\n\n# calculate the Rayleigh PDF on 100 equally spaced points between 0 and 5\npoints = np.linspace(0, 5, 100)\nray_pdf = rayleigh_pdf(points, sigma)  \n\n# setup the plot\nfig, ax = plt.subplots(figsize=(8,7), dpi=80)\nax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels()))))\nax.set_xlabel('Sample Value', fontsize='x-large')\nax.set_ylabel('Probability Density', fontsize='x-large')\nax.set_title('Rayleigh PDF', fontsize='xx-large');\n\n# plot the Rayleigh pdf\nax.plot(ray_pdf);\n\n\n\n\nAs we mentioned the scale \\(\\sigma\\) controls the width or narrowness of the distribution.\nLet’s both halve and double sigma to (\\(\\frac{1}{2}, {2})\\) respectively to see what happens.\n\n# setup plot\nfig,ax = plt.subplots(figsize=(8,7), dpi=80)\nax.set_xlabel('Sample Value', fontsize='x-large')\nax.set_ylabel('Probability Density', fontsize='x-large')\nax.set_title('Rayleigh PDFs', fontsize='xx-large'); \n\n\n# different colors for each sigma\nsigmas = [0.5, 1, 2]\ncolors = ['m', 'b', 'r']\n\n# plot the distributions with different scales\nfor color,sig in zip(colors,sigmas):\n    rpdf = rayleigh_pdf(points, sig)\n    ax.plot(points, rpdf, c=color, label=f'σ: {sig}')\nax.set_xticklabels([-1] + list(range(len(ax.get_xticklabels()))))\nax.legend();\n\n\n\n\nThe blue line in the plot above is the same PDF from our first plot where \\(\\sigma = 1\\).\nWe can see how \\((\\sigma = 0.5)\\) pulls the distribution up and to the left, while \\((\\sigma = 2)\\) squishes it down and to the right.\nIn other words, a smaller sigma makes our distribution narrower while a larger sigma makes it wider.\nPlotting the theoretical Rayleigh PDF only shows what the distribution should looks like. Next, we need to actually generate some Rayleigh values."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#magnitude-phase-vs.-real-imaginary",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#magnitude-phase-vs.-real-imaginary",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "(Magnitude, Phase) vs. (Real, Imaginary)",
    "text": "(Magnitude, Phase) vs. (Real, Imaginary)\nWe mentioned earlier that a complex number has real and imaginary components. But so far we have deal with magnitudes and phases instead. How are these quantities related?\nIt turns out that we can use the phase and magnitude to split our vector into its real and imaginary parts. The cosine of the phase and magnitude gives us the real part, and the sine of the phase gives us the imaginary part.\nThese are two different representations of the same complex number. We do not lose anything going from one to the other or vice-versa.\n\n# splitting our phases and magnitues into real and imaginary components\nreal = ray_vals * np.cos(phase)\nimag = ray_vals * np.sin(phase)\n\nIt turns out this will be a key detail when we are creating complex-valued network layers. As a preview: we will give one set of weights the real values, and another set of weights the imag values. This is because complex operations like addition and multiplication work better on GPUs with real and imaginary representations."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#visualizing-our-random-phases",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#visualizing-our-random-phases",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Visualizing our random phases",
    "text": "Visualizing our random phases\nNow we can check if these phases are truly orienting our magnitudes in random directions. To do so we plot the first 500 complex weights in the polar plane.\n\n\n# indexes for the first 500 random weights\nchosen_samples = range(500) \n\n# plot these first complex weights\nplt.figure(figsize=(8,7), dpi=80)\nfor idx in chosen_samples:\n\n    # index into phase and magnitude variables\n    angle,mag = phase[idx],ray_vals[idx]\n\n    # plot them starting from the origin\n    plt.polar([0,angle], [0,mag], marker='o')\n    \nplt.title('Magnitudes and Phases of our Complex Weights');\n\n\n\n\nThat definitely looks like a random, uniform orientation!"
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#he-and-glorot-criteria-for-rayleigh-distributions",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#he-and-glorot-criteria-for-rayleigh-distributions",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "He and Glorot criteria for Rayleigh distributions",
    "text": "He and Glorot criteria for Rayleigh distributions\nHow can we make sure our Rayleigh magnitudes meet the He and Glorot variance criteria?\nThe Complex Neural Nets paper from earlier includes a nice derivation for the variance of a complex Rayleigh distribution: \\[\\text{Var}(W) = 2\\sigma^{2}\\]\nWe can set the Rayleigh variance equal to the He and Glorot criteria and solve for sigma \\(\\sigma\\).\nTo meet the He criteria, sigma should be: \\[\\sigma_{\\text{He}} = \\frac{1}{\\sqrt{\\text{fanIn}}}\\] \nTo meet the Glorot criteria, sigma should be: \\[\\sigma_{\\text{Glorot}} = \\frac{1}{\\sqrt{\\text{fanIn + fanOut}}}\\] \n\nStarting with a simple one-layer network\nIn the previous sections we used a flat vector of complex weights for the examples and plots. Tying it back at our two concrete examples of wind speed and radio noise, it’s as if we took a single series of measurements.\nBut since the He and Glorot criteria are defined for network layers, we need a new example. Let’s start with to a simple one-layer network. Our layer will have 100 inputs and 50 outputs (fanIn = 100, fanOut = 50).\nPlugging these fanIn and fanOut values into the Rayleigh sigma criteria gives: \\[\\sigma_{\\text{He}} = \\frac{1}{10}\\]\n\\[\\sigma_{\\text{Glorot}} = \\frac{1}{5\\sqrt{6}}\\]\nNow we can pass either of these sigmas to our default_rng and it will draw Rayleigh samples with variances that match the chosen criteria.\n\nA quick word about fanIn and fanOut. We saw the simple feed-forward case with in our example for a single network layer. In that case the number of incoming connections was simply fanIn and the outgoing connections were fanOut.\n\n\nHowever, the convolutional case is a bit more complicated. A convolutional layer has input and output feature maps which are roughly analogous to input and output units in feed-forward layers. But they also have a kernel size to consider. PyTorch has a nice convenience function that handles this for us."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.linear",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.linear",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Complex initializations for nn.Linear",
    "text": "Complex initializations for nn.Linear\n\n# re-create out earlier example with a single layer\nfan_in, fan_out = 100, 50\nsigma_he = 1. / np.sqrt(fan_in) # to match the He criteria\n\n# get the complex-valued weights\nm = torch.nn.Linear(fan_in, fan_out)\nreal, imag = get_complex_inits(m)\n\nWe should check that the magnitude of the weights actually follow a Rayleigh distribution.\n\n# get linear magnitudes as a flat numpy vector\nmagnitude = torch.sqrt(real**2 + imag**2).numpy().reshape(-1)\n\n\n# setup the plot\nplt.figure(figsize=(8,7), dpi=80)\nplt.title('Complex nn.Linear Weights vs. Theoretical Rayleigh PDF', fontsize='x-large')\nplt.xlabel('Sample Value', fontsize='x-large')\nplt.ylabel('Probability Density', fontsize='x-large')\n\n# pick points that cover the sample range to compare with theoretical rayleigh pdf\npoints = np.linspace(0, magnitude.max(), 1000)\nray_pdf = rayleigh_pdf(points, sigma=sigma_he)\n\n# plot histogram of Linear magnitudes vs. the theoretical pdf\nplt.hist(magnitude, bins=35, density=True)\nplt.plot(points, ray_pdf, c='r', linewidth=3);\n\n\n\n\nSuccess! Our Linear module is properly initialized."
  },
  {
    "objectID": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.conv2d",
    "href": "blog/posts/2022-09-01-rayleigh-init/index.html#complex-initializations-for-nn.conv2d",
    "title": "Complex Rayleigh Weight Initializations",
    "section": "Complex initializations for nn.Conv2d",
    "text": "Complex initializations for nn.Conv2d\nCan we do the same for a convolutional layer? Our main concern is correctly handling both the tensor shape and fan_in, fan_out.\n\n# make conv layer with 100 input features, 50 output features, and (3x3) kernel\nk = 3 # kernel size\n# now, these are the number of feature maps (chan_in and chan_out)\nfan_in, fan_out = 100, 50\n\nconv_layer = torch.nn.Conv2d(fan_in, fan_out, k)\nreal_conv, imag_conv = get_complex_inits(conv_layer) # get the initial complex weights\n\n# make sure the shape of weights is ok\nprint(f'Shapes of real and imaginary convolutional tensors: {real_conv.shape}, {imag_conv.shape}')\n\nShapes of real and imaginary convolutional tensors: torch.Size([50, 100, 3, 3]), torch.Size([50, 100, 3, 3])\n\n\nLet’s check if these convolutional weights are still Rayleigh distributed.\n\n# get convolutional magnitudes as a flat numpy vector\nconv_magnitude = torch.sqrt(real_conv**2 + imag_conv**2).numpy().reshape(-1)\n\n\n# setup the plots\nplt.figure(figsize=(8,7), dpi=80)\nplt.title('Complex nn.Conv2d Weights vs. Theoretical Rayleigh PDF', fontsize='x-large')\nplt.xlabel('Sample Value', fontsize='x-large')\nplt.ylabel('Probability Density', fontsize='x-large')\n\n\n# pick points that cover sample range to compare with theoretical rayleigh pdf\npoints = np.linspace(0, conv_magnitude.max(), 1000)\n\n# note: we need to re-compute fanIn for the convolutional layer\nfan_in, fan_out = torch.nn.init._calculate_fan_in_and_fan_out(conv_layer.weight)\nsigma_he_conv = sigma=1. / np.sqrt(fan_in)\n\nray_pdf = rayleigh_pdf(points, sigma_he_conv)\n\n# plot histogram of magnitudes vs. theoretical pdf\nplt.hist(conv_magnitude, bins=35, density=True)\nplt.plot(points, ray_pdf, c='r', linewidth=3);\n\n\n\n\nAnother match! Our convolutional layer is also properly initialized."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chaski",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nLesson 5: Processing text documents for LLMs\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nLesson 4: Quantized LLMs with llama.cpp\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nLesson 3: HuggingFace NLP Models\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nOverview of MLC and llama.cpp\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nLessong 2: Blogging with nbdev\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nnbdev\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nLesson 1: A Python Environment for LLMs\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nSyllabus: Fractal-U Course on LLMs\n\n\n\n\n\n\n\nfractal\n\n\npython\n\n\nLLM\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nChris Kroenke\n\n\n\n\n\n\n  \n\n\n\n\nTips for LLM prompts\n\n\n\n\n\n\n\nLLM\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion v2 with dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 7\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 6\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 5\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 4\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 3\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 2\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nLibraries for dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 1\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nA PyTorch SLERP implementation\n\n\n\n\n\n\n\ndiffusion\n\n\nlatent interpolation\n\n\nSLERP\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nIntro to normalizing and scheduling Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nMerging an arbitrary number of Binary Trees\n\n\n\n\n\n\n\nBinary Tree\n\n\nalgorithms\n\n\nfunctional\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nComplex Rayleigh Weight Initializations\n\n\n\n\n\n\n\ndeep learning\n\n\ncomplex networks\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nNormalizing spectrograms for Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nspectrogram normalizations\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nenzokro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "chaski",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nTips for LLM prompts\n\n\n\n\n\n\n\nLLM\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nStable Diffusion v2 with dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 7\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 27, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 6\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 5\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 4\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nClassifier-free Guidance with Cosine Schedules Pt. 3\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 2\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 22, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nLibraries for dynamic Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nDynamic Classifier-free Guidance Pt. 1\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nA PyTorch SLERP implementation\n\n\n\n\n\n\n\ndiffusion\n\n\nlatent interpolation\n\n\nSLERP\n\n\n\n\n\n\n\n\n\n\n\nNov 16, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nIntro to normalizing and scheduling Classifier-free Guidance\n\n\n\n\n\n\n\ndiffusion\n\n\nclassifier-free guidance\n\n\ndeep learning\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nMerging an arbitrary number of Binary Trees\n\n\n\n\n\n\n\nBinary Tree\n\n\nalgorithms\n\n\nfunctional\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nComplex Rayleigh Weight Initializations\n\n\n\n\n\n\n\ndeep learning\n\n\ncomplex networks\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2022\n\n\nenzokro\n\n\n\n\n\n\n  \n\n\n\n\nNormalizing spectrograms for Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nspectrogram normalizations\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2022\n\n\nenzokro\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Fractal LLMs",
    "section": "",
    "text": "Syllabus: Fractal-U Course on LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\n\n\n\n\n\n\nLesson 1: A Python Environment for LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\n\n\n\n\n\n\nLessong 2: Blogging with nbdev\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2023\n\n\n\n\n\n\n\n\nLesson 3: HuggingFace NLP Models\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\n\n\n\n\n\n\nOverview of MLC and llama.cpp\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\n\n\n\n\n\n\nLesson 4: Quantized LLMs with llama.cpp\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2023\n\n\n\n\n\n\n\n\nLesson 5: Processing text documents for LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html",
    "href": "blog/posts/2022-08-20-spec-norms/index.html",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "",
    "text": "Scaling spectrograms for classification tasks with neural networks."
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#a-quick-note-on-transfer-learning",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#a-quick-note-on-transfer-learning",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "A quick note on Transfer Learning",
    "text": "A quick note on Transfer Learning\nWe also have to talk about Transfer Learning in the context of normalization. In Transfer Learning, it is best-practice to normalize the new dataset with the statistics from the old dataset. This makes sure that the new network inputs are at the same scale as the original inputs. Since most pretrained vision models were trained on ImageNet, we normalize any new inputs with ImageNet statistics.\nHowever, we avoid Transfer Learning in this post and instead train an 18-layer xResNet from scratch. The reason is that pretrained image models operate at a completely different scale than spectrograms. And the main goal here is to learn our own scalings instead!"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#the-esc-50-dataset",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#the-esc-50-dataset",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "The ESC-50 dataset",
    "text": "The ESC-50 dataset\nThe first step is to download the data. ESC-50 is already included in fastaudio so we can grab it with untar_data.\n\n\nCode\n# from fastai.vision.all import *\n# from fastaudio.core.all import *\n# from fastaudio.augment.all import *\n\n# already in fastaudio, can download with fastai's `untar_data`\n# path = untar_data(URLs.ESC50)\n\n\nThe downloaded audio files are inside the aptly named audio folder. Below we use the ls method, a fastai addition to python’s pathlib.Path, to check the contents of this folder.\n\n\nCode\n# wavs = (path/\"audio\").ls()\n# wavs\n\n\nThe output of ls shows 2,000 audio files. But the filenames are not very descriptive, so how do we know what is actually in each one?\nThankfully, as with many datasets, the download includes a table with more information about the data (aka metadata).\n\n\nCode\n# # read the audio metadata and show the first few rows\n# df = pd.read_csv(path/\"meta\"/\"esc50.csv\")\n# df.head()\n\n\nThe key info from this table are in the filename and category columns.\nfilename gives the name of a file inside of the audio folder.\ncategory tells us which class a file belongs to.\nThe last file in the data directory will be our working example for normalization. We can index into the metadata table above using this file’s name to learn more about it.\n\n\nCode\n# # pick the row where \"filename\" matches the file's \"name\".\n# df.loc[df.filename == wavs[-1].name]\n\n\nThis is a recording of crickets!\nWe can load this file with the AudioTensor class in fastaudio. Its create function reads the audio samples straight into a torch.Tensor.\n\n\nCode\n# # create an AudioTensor from a file path\n# sample = AudioTensor.create(wavs[-1])\n\n\nAn AudioTensor can plot and even play the audio with its show method.\n\n\nCode\n# print(f'Audio shape [channels, samples]: {sample.shape}')\n# sample.show();\n\n\nEach “burst” in the plot above is a cricket chirp. There are three full chirps and the early starts of a fourth chirp."
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#building-the-dataset-loader",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#building-the-dataset-loader",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Building the dataset loader",
    "text": "Building the dataset loader\nThe setup below follows the fastaudio ESC-50 baseline to step through the training dataset. It is worth mentioning that the files in ESC-50 are sampled 44.1 kHz, but fastaudio will resample them to 16 kHz by default. Downsampling like this risks throwing away some information. But, keeping the higher sampling rate almost triples the “width” (aka time) of the spectrogram. This larger image will take up more memory in the GPU and limits our batch size and architecture choices. We keep this downsampling since it gives the spectrograms a very reasonable shape of [201, 401], compared with the much larger shape of [201, 1103] if we don’t downsample.\n\n\nCode\n# def CrossValidationSplitter(col='fold', fold=1):\n#     \"Split `items` (supposed to be a dataframe) by fold in `col`\"\n#     def _inner(o):\n#         assert isinstance(o, pd.DataFrame), \"ColSplitter only works when your items are a pandas DataFrame\"\n#         col_values = o.iloc[:,col] if isinstance(col, int) else o[col]\n#         valid_idx = (col_values == fold).values.astype('bool')\n#         return IndexSplitter(mask2idxs(valid_idx))(o)\n#     return _inner\n\n# auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n#                  get_x=ColReader(\"filename\", pref=path/\"audio\"), \n#                  splitter=CrossValidationSplitter(fold=1),\n#                  item_tfms = [AudioNormalize],\n#                  batch_tfms = [audio2spec],\n#                  get_y=ColReader(\"category\"))\n# dbunch = auds.dataloaders(df, bs=64)\n# dbunch.show_batch(figsize=(7,7))"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#calculating-the-statistics",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#calculating-the-statistics",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Calculating the statistics",
    "text": "Calculating the statistics\nNext we make two recorders: one for global statistics and the other for channel-based statistics. Then we step through the training dataset to find both sets of stats.\n\n\nCode\n# # create recorders\n# global_stats  = StatsRecorder()\n# channel_stats = StatsRecorder(red_dims=(0,1,3))\n\n# # step through the training dataset\n# with torch.no_grad():\n#     for idx,(x,y) in enumerate(iter(dbunch.train)):\n#         # update normalization statistics\n#         global_stats.update(x)\n#         channel_stats.update(x)\n    \n# # parse out both sets of stats\n# global_mean,global_std = global_stats.mean,global_stats.std\n# channel_mean,channel_std = channel_stats.mean,channel_stats.std\n\n\nWe can check the shape of the statistics to make sure they are correct. For the global statistics, we expect a shape of: [1,1,1,1]. With spectrogram channel normalizations, we expect one value per spectrogram bin for a shape of [1,1,201,1].\n\n\nCode\n# print(f'Shape of global mean: {global_mean.shape}')\n# print(f'Shape of global standard dev: {global_std.shape}')\n\n\n\n\nCode\n# print(f'Shape of channel mean: {channel_mean.shape}')\n# print(f'Shape of channel standard dev: {channel_std.shape}')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#transforms-to-normalize-mini-batches",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#transforms-to-normalize-mini-batches",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Transforms to normalize mini-batches",
    "text": "Transforms to normalize mini-batches\nWe need to extend the fastai Normalize class in order to use the spectrogram normalization statistics. The reason is type dispatch. fastai normalization uses ImageNet statistics due to the focus on transfer learning with color images. But this ImageNet normalization is only applied on RGB images of the TensorImage class, while AudioSpectrogram subclasses the different TensorImageBase. The solution is to define encodes and decodes for TensorImageBase instead.\n\n\nCode\n# class SpecNormalize(Normalize):\n#     \"Normalize/denorm batch of `TensorImage`\"\n#     def encodes(self, x:TensorImageBase): return (x-self.mean) / self.std\n#     def decodes(self, x:TensorImageBase):\n#         f = to_cpu if x.device.type=='cpu' else noop\n#         return (x*f(self.std) + f(self.mean))\n\n\n\n\nCode\n# # make global and channel normalizers\n# GlobalSpecNorm  = SpecNormalize(global_mean,  global_std,  axes=(0,2,3))\n# ChannelSpecNorm = SpecNormalize(channel_mean, channel_std, axes=(0,1,3))"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#training-helpers",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#training-helpers",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Training helpers",
    "text": "Training helpers\nTo avoid repeating ourselves, the helper functions below build the dataloaders and run the training loops.\nThe get_dls function makes it clear which normalization is being applied. The train_loops function repeats training runs a given number of times.\n\n\nCode\n# def get_dls(bs=64, item_tfms=[], batch_tfms=[]):\n#     \"Get dataloaders with given `bs` and batch/item tfms.\"\n#     auds = DataBlock(blocks=(AudioBlock, CategoryBlock),  \n#                      get_x=ColReader(\"filename\", pref=path/\"audio\"), \n#                      splitter=CrossValidationSplitter(fold=1),\n#                      item_tfms=item_tfms,   # for waveform normalization\n#                      batch_tfms=batch_tfms, # for spectrogram normalization\n#                      get_y=ColReader(\"category\"))\n#     dls = auds.dataloaders(df, bs=bs)\n#     return dls\n\n# def make_xresnet_grayscale(model, n_in=1):\n#     \"Modifies xresnet `model` for single-channel images.\" \n#     model[0][0].in_channels = n_in\n#     # sum weights to reduce dimension\n#     model[0][0].weight = torch.nn.parameter.Parameter(model[0][0].weight.mean(1, keepdim=True))\n\n# def train_loops(dls, name, num_runs=num_runs, epochs=epochs, num_cls=50):\n#     \"Runs `num_runs` training loops with `dls` for given `epochs`.\"\n#     accuracies = []\n#     for i in range(num_runs):\n#         # make new grayscale xresnet\n#         model = xresnet18(pretrained=False, n_out=num_cls)\n#         make_xresnet_grayscale(model, n_in=1)\n#         # get learner for this run\n#         learn = Learner(dls, model, metrics=[accuracy])\n#         # train network and track accuracy\n#         learn.fit_one_cycle(epochs)\n#         accuracies.append(learn.recorder.values[-1][-1])\n#     print(f'Average accuracy for \"{name}\": {sum(accuracies) / num_runs}')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#baseline-performance",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#baseline-performance",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Baseline performance",
    "text": "Baseline performance\nBefore getting carried away with normalization, we have to first set a baseline without normalizations. This allows us to evaluate the impact of normalization later on, else there is no way to know if normalization helps at all.\n\n\nCode\n# # data without normalization\n# dls = get_dls(batch_tfms=[audio2spec])\n# # run training loops\n# train_loops(dls, name='No Norm')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-global-normalization",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-global-normalization",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Performance with global normalization",
    "text": "Performance with global normalization\nNext we normalize each audio waveform and the spectrograms with global, scalar statistics.\n\n\nCode\n# # data with waveform and global normalization\n# dls = get_dls(item_tfms=[AudioNormalize],\n#               batch_tfms=[audio2spec, GlobalSpecNorm])\n# # run training loops\n# train_loops(dls, name='Global Norm')"
  },
  {
    "objectID": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-channel-normalization",
    "href": "blog/posts/2022-08-20-spec-norms/index.html#performance-with-channel-normalization",
    "title": "Normalizing spectrograms for Deep Learning",
    "section": "Performance with channel normalization",
    "text": "Performance with channel normalization\nFinally, we normalize each audio waveform and the spectrograms with channel-based statistics.\n\n\nCode\n# # get data with waveform and channel normalization\n# dls = get_dls(item_tfms=[AudioNormalize],\n#               batch_tfms=[audio2spec, ChannelSpecNorm])\n# # run training loops\n# train_loops(dls, name='Channel Norm')"
  },
  {
    "objectID": "blog/posts/2022-11-08-merge-n-BSTs/index.html",
    "href": "blog/posts/2022-11-08-merge-n-BSTs/index.html",
    "title": "Merging an arbitrary number of Binary Trees",
    "section": "",
    "text": "Using functional python tools to merge several Binary Trees together."
  },
  {
    "objectID": "blog/posts/2022-11-08-merge-n-BSTs/index.html#the-intuition-to-merge-two-binary-trees.",
    "href": "blog/posts/2022-11-08-merge-n-BSTs/index.html#the-intuition-to-merge-two-binary-trees.",
    "title": "Merging an arbitrary number of Binary Trees",
    "section": "The intuition to merge two Binary Trees.",
    "text": "The intuition to merge two Binary Trees.\nThe general intuition to solve this problem is:\n\nOverlay the two trees together, starting from their root nodes.\n\nThen, merge the values of the root nodes.\n\nFinally, merge both the left and and right subtrees in the same way.\n\nWhat will these steps look like in code?"
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html",
    "title": "A PyTorch SLERP implementation",
    "section": "",
    "text": "SLERP implemented in PyTorch with proper thresholding."
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html#why-do-we-need-slerp",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html#why-do-we-need-slerp",
    "title": "A PyTorch SLERP implementation",
    "section": "Why do we need SLERP?",
    "text": "Why do we need SLERP?\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t need the theory, you can skip straight to the code.\n\n\nSLERP interpolates two vectors while keeping their magnitudes intact. Why would this be important for Diffusion models?\nThe reason has to do with how Gaussian distributions behave in higher dimensions. This blog post by Ferenc Huszár has an excellent description of how exactly our intuitions fall apart in high dimensions. The post also has many good visualizations to drive the point home.\n\nGaussians in high dimensions\nTo summarize Ferenc’s blog post: a Gaussian in high dimensions is fundamentally different than its 1-D “Bell curve” version.\nAs we climb to higher dimensions the Gaussian distribution becomes a thin, hollow shell. Its probability density spreads out around this thin shell. Think about how different that is to a 1-D Gaussian. In the 1-D case, most of the density falls within a few standard deviations of the mean.\nBefore long, the inside of this high-dimensional Gaussian is empty. Only its thin shell has any probability at all. Borrowing Ferenc’s excellent analogy: the distribution turns into a “soap bubble”.\nRecall that most Diffusion models are based on high-dimensional Gaussians. That means that, in Diffusion, we are actually dealing with many high-dimensional soap bubbles. If we treat them like regular 2-D or 3-D vectors, our intuitions will fail us.\n\n\nOk, so where does SLERP come in?\nIf we linearly interpolate two high-dimensional Gaussians, the result can easily fly away from the soap bubble’s surface. The section below has an example of what this looks like in 2-D space.\nSLERP makes it possible to properly interpolate Diffusion vectors by keeping us firmly grounded on the surface of the soap bubble."
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html#what-about-linear-interpolation",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html#what-about-linear-interpolation",
    "title": "A PyTorch SLERP implementation",
    "section": "What about linear interpolation?",
    "text": "What about linear interpolation?\nRegular linear interpolation (sometimes called LERP) is a powerful tool. It is a cornerstone in modern computer graphics to move an object between two points.\nLERP has a loose analogy with gravity: the shortest distance between two points is a straight line.\nFor example, imagine you are drinking a cup of coffee. The mug is currently on the table. As you go to take a sip, you pick up the mug and bring it directly to your lips. You wouldn’t swing your arm around in a weird way. That would only be more work and delay the sip of coffee.\nIn other words, when moving objects in our 3-D world we want to do the least amount of work possible. That is what LERP does in 2-D and 3-D space. In a manner of speaking, you used LERP to bring the coffee mug to your lips and take a sip.\nThis coffee example brings us back to why we need SLERP in the first place. Our notions of 3-D paths break down in higher dimensions, and LERP does not work as intended. Here we are much better served by SLERP.\n\nA concrete LERP example\nLet’s show how linear interpolation works on vectors.\nFor this example we will use the familiar \\(x\\) and \\(y\\) basis vectors. We also draw the Unit Circle for reference.\n\n\n\n\n\n\nNote\n\n\n\nThe plotting function plot_vectors is available in the post’s notebook. It is omitted here for space.\n\n\n\nimport torch\n\n# use the X and Y unit vectors as an example\nxhat = torch.tensor([1, 0]).float()\nyhat = torch.tensor([0, 1]).float()\n\n\n# plot the basis vectors, with a unit circle outline\nfig = plot_vectors(xhat, yhat, labels=['$\\hat{x}$', '$\\hat{y}$'], draw_unit_circle=True)\n\nplt.xlabel('X Axis', fontsize='x-large')\nplt.ylabel('Y Axis', fontsize='x-large')\nplt.title('Basis Vectors on the Unit Circle', fontsize='xx-large', pad=10);\n\n\n\n\nWhat happens if we linearly interpolate (LERP) these vectors to their midpoint?\n\n# use linear interpolation to find the midpoint\np_lerp = (xhat + yhat) / 2\n\n\n# plotting the LERP of basis vectors x and y\nfig = plot_vectors(xhat, yhat, p_lerp, labels=['$\\hat{x}$', '$\\hat{y}$', 'P_lerp'])\n\nplt.xlabel('X Axis', fontsize='x-large')\nplt.ylabel('Y Axis', fontsize='x-large')\nplt.title('Linear Interpolation of Unit Vectors', fontsize='xx-large', pad=10);\n\n\n\n\nIf we only cared about getting from \\(\\hat{y}\\) to \\(\\hat{x}\\) then we are on the right track. LERP is following the shortest possible path.\nBut imagine if the Unit Circle was like a slice of a high-dimensional Gaussian. In that case, linear interpolation has moved us away from the surface of the soap bubble!\nIf we were dealing with a 1-D Gaussian, it’s as if we have moved very far from the mean. Imagine going out \\(+10\\) \\(\\sigma\\) away. That would obviously be an incredibly unlikely sample. And that is exactly where the \\(P_\\text{LERP}\\) vector ends up.\nWith SLERP, we can still interpolate the vectors while also staying firmly anchored to the soap bubble."
  },
  {
    "objectID": "blog/posts/2022-11-16-pytorch-slerp/index.html#slerp-interpolation-of-the-unit-vectors",
    "href": "blog/posts/2022-11-16-pytorch-slerp/index.html#slerp-interpolation-of-the-unit-vectors",
    "title": "A PyTorch SLERP implementation",
    "section": "SLERP interpolation of the unit vectors",
    "text": "SLERP interpolation of the unit vectors\nWhat happens if we instead use SLERP to interpolate the unit vectors?\n\n# SLERP the unit vectors to their midpoint\np = slerp(xhat, yhat, 0.5)\n\n\n# plot the SLERP iterpolated vector\nfig = plot_vectors(xhat, yhat, p, labels=['$\\hat{x}$', '$\\hat{y}$', \"P_slerp\"])\nplt.xlabel('X Axis', fontsize='x-large')\nplt.ylabel('Y Axis', fontsize='x-large')\nplt.title('SLERP on Unit Vectors to their midpoint P', fontsize='xx-large', pad=10);\n\n\n\n\nThat looks much better!\nIf the Unit Circle was like a Gaussian soap bubble, then we’ve properly moved along its film."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "",
    "text": "Introducing two helper libraries to run dynamic Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html#motivation",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html#motivation",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "Motivation",
    "text": "Motivation\nThe initial experiments had a lot of boilerplate and repeated code.\nFor example, the same code was used in multiple notebooks to load Stable Diffusion models. The code for guidance schedules and normalizations was also repeated across notebooks.\nThat meant that each notebook needed a lot of overhead before it got to the actual experiments.\nTo make life a bit easier, and because we hope that these ideas are broadly usable, this repeated code was moved to two libraries:\n\nmin_diffusion\ncf_guidance\n\nNow we can import these libraries and jump straight to the important part: running the guidance experiments."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-min_diffusion-library",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-min_diffusion-library",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "The min_diffusion library",
    "text": "The min_diffusion library\nIn this section we generate an image using min_diffsion.\n\nfrom min_diffusion.core import MinimalDiffusion\n\n2022-11-22 15:42:08.507717: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n\n\n\nLoading the openjourney model from Prompt Hero\nThe following code load the openjourney Stable Diffusion model on the GPU, in torch.float16 precision.\n\nmodel_name = 'prompthero/openjourney'\ndevice     = 'cuda'\ndtype      = torch.float16\n\n\npipeline = MinimalDiffusion(model_name, device, dtype)\n\n\npipeline.load();\n\nEnabling default unet attention slicing.\n\n\n\n\nGenerating an image\nNext we use the familiar prompt to generate an image:\n\n“a photograph of an astronaut riding a horse”\n\n\n\n\n\n\n\nNote\n\n\n\nThe openjourney model was fine-tuned to create images in the style of Midjourney v4.\nTo enable this fine-tuned style, we have to add the keyword \"mdjrny-v4\" at the start of the prompt.\n\n\n\n# text prompt for image generations\nprompt = \"mdjrny-v4 style a photograph of an astronaut riding a horse\"\n\n\n# set the seed for reproducibility\ntorch.manual_seed(2147483647);\n\n\n# generate the image\nimg = pipeline.generate(prompt);\n\nUsing the default Classifier-free Guidance.\n\n\n\n\n\n\n# display the generated image\nimg\n\n\n\n\nThat’s the entire process!\nThe main difference between MinimalDiffusion and the HuggingFace API is that now we can easily customize the image generation loop. This allows us to explore a wide range of dynamic Classifier-free Guidances."
  },
  {
    "objectID": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-cf_guidance-library",
    "href": "blog/posts/2022-11-20-minimal-diffusion/index.html#the-cf_guidance-library",
    "title": "Libraries for dynamic Classifier-free Guidance",
    "section": "The cf_guidance library",
    "text": "The cf_guidance library\nThe sections below are based on the cf_guidance documentation.\nWe create a few Cosine schedules and plug them into different Classifier-free Guidances.\nThe schedule parameter come from the initial post on dynamic Classifier-free Guidance.\n\nfrom cf_guidance.schedules import get_cos_sched\n\n\n# Parameters from the blog post\n# https://enzokro.dev/blog/posts/2022-11-15-guidance-expts-1/\nmax_val = 7.5\nmin_val = 0.15\nnum_steps = 50\nnum_warmup_steps = 5\n\n# 1) Baseline cosine schedule\ncos_params = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_warmup_steps':  0,\n}\n\n# 2) Cosine schedule with warmup \nwarmup_cos_params = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'warmup_init_val':   min_val + 1., # to show we can offset the warmup relative to min\n    'num_warmup_steps':  num_warmup_steps,\n}\n\n# create the schedules\ncos_g = get_cos_sched(**cos_params)\nwarmup_g = get_cos_sched(**warmup_cos_params)\n\nLet’s plot these cosine schedules to see what they look like.\n\n# plot the schedules\nplt.plot(cos_g)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Cosine Schedule');\n\n\n\n\n\nplt.plot(warmup_g)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Warmup Cosine Schedule');\n\n\n\n\n\nCreating Guidance Normalizers\nNow we can use these schedules during Classifier-free Guidance. The Guidance Transform class, GuidanceTfm, makes this possible.\nGuidance transforms take one initialization parameter: schedules. This is a map from parameter names to an array-like, indexable sequence of values.\nFor a given parameter name at diffusion timestep idx, the value of schedules[name][idx] should be the parameter’s scheduled value at the given timestep.\nIn this case we call the guidance parameter \\(G\\) as a lowercase \\(g\\).\n\nfrom cf_guidance.transforms import GuidanceTfm, BaseNormGuidance\n\n\n# create the `schedules` parameter\nexample_schedules = {'g': cos_g}\n\n# Create a Guidance with cosine schedule.\nguidance = GuidanceTfm(example_schedules)\n\n# Normalized Guidance with a cosine schedule.\nnorm_guidance = BaseNormGuidance(example_schedules)\n\n\n\nUsing the transforms in a Diffusion pipeline\nThe following snippet shows where and how the Guidance Transforms are used in a diffusion loop.\nWe use the norm_guidance example class created above. Specifically, we call norm_guidance with the following arguments:\n\nThe unconditioned noise predictions.\n\nThe conditional noise predictions.\n\nThe index of the current timestep.\n\nThe code is borrowed from HuggingFace’s official StableDiffusionPipeline to show where norm_guidance should go.\nThis seems like a good starting point, since many scripts and functions are based on this HuggingFace setup.\n    # inside of `StableDiffusionPipeline`\n    \n    for i, t in enumerate(self.progress_bar(timesteps_tensor)):\n        # expand the latents if we are doing classifier free guidance\n        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n\n        # predict the noise residual\n        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n\n        # NOTE: our transforms go here:\n        ###############################\n        if do_classifier_free_guidance:\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n\n            ## OLD UPADTE\n            #noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n\n            # NEW cf_guidance UPDATE\n            noise_pred = norm_guidance(noise_pred_uncond, noise_pred_text, i)\n\n\nCreating more complex schedules\nOur cosine scheduler is based on a combination of the schedulers in timm and HuggingFace.\nIt has a variety of parameters to support many schedule combinations as shown below.\n\n# cosine schedule with a full cycle\nfull_cycle = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        1,\n    'num_warmup_steps':  0,\n}\n\n# cosine schedule with k-decay\nk_decay_cos = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        1.5,\n    'k_decay':           0.7,\n    'warmup_init_val':   min_val + 1., # to show we can offset the warmup value\n    'num_warmup_steps':  num_warmup_steps,\n}\n\n# create the schedules\nfull_cycle_sched = get_cos_sched(**full_cycle)\nk_decay_sched = get_cos_sched(**k_decay_cos)\n\n\nplt.plot(full_cycle_sched)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Cosine With a Full Cycle');\n\n\n\n\n\nplt.plot(k_decay_sched)\nplt.xlabel('Diffusion Timesteps')\nplt.ylabel('$G$ Guidance Parameter')\nplt.title('Cosine with Offset-Warmup, 1.5 Cycles, and K-decay');"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "",
    "text": "Experiments with cosine schedules and normalizations for Classifier-free Guidance."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#quick-recap-of-parts-1-and-2",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#quick-recap-of-parts-1-and-2",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Quick recap of Parts 1 and 2",
    "text": "Quick recap of Parts 1 and 2\nIn Part 1, we generated a baseline image using a constant Classifier-free Guidance. Attempting to improve on the baseline, we swept the guidance parameter \\(G\\) over a set of Cosine Schedules.\nIn Part 2, we introduced normalizations for Classifier-free Guidance. There was one kind of normalization, Prediction Normalization, that seems to improve the overall quality of generated images."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#part-3-combining-schedules-and-normalizations",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#part-3-combining-schedules-and-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Part 3: Combining schedules and normalizations",
    "text": "Part 3: Combining schedules and normalizations\nIn Part 3, we build on the previous results by now combining guidance normalizations and schedules.\nThe goal is to find a combo of normalized schedules that universally improve the outputs of Diffusion image models."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#leveraging-a-few-helper-libraries",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#leveraging-a-few-helper-libraries",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Leveraging a few helper libraries",
    "text": "Leveraging a few helper libraries\nWe reuse our helper libraries to more efficiently run guidance experiments. The two libraries are:\n\nmin_diffusion\ncf_guidance\n\nThey were introduced in this separate post."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#python-imports",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#python-imports",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Python Imports",
    "text": "Python Imports\nFirst we import the needed python modules.\n\nimport os\nimport math\nimport random\nimport warnings\nfrom PIL import Image\nfrom typing import List\nfrom pathlib import Path\nfrom types import SimpleNamespace\nfrom fastcore.all import L\nfrom functools import partial\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# imports for diffusion models\nimport torch\nfrom transformers import logging\n# for clean outputs\nwarnings.filterwarnings(\"ignore\")\nlogging.set_verbosity_error()\n\n# set the hardware device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps else \"cpu\"\n\n2022-11-24 18:34:14.079096: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n\n\n\nSeed for reproducibility\nWe use the seed_everything function to make sure that the results are repeatable across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#importing-the-helper-libraries",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#importing-the-helper-libraries",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Importing the helper libraries",
    "text": "Importing the helper libraries\nThe cf_guidance library has the guidance schedules and normalizations.\n\n# helpers to create cosine schedules\nfrom cf_guidance.schedules  import get_cos_sched\n\n# normalizations for classifier-free guidance\nfrom cf_guidance.transforms import GuidanceTfm, BaseNormGuidance, TNormGuidance, FullNormGuidance\n\nThe min_diffusion library loads a Stable Diffusion model from the HuggingFace hub.\n\n# to load Stable Diffusion pipelines\nfrom min_diffusion.core import MinimalDiffusion\n\n# to plot generated images\nfrom min_diffusion.utils import show_image, image_grid, plot_grid"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#image-parameters",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Image parameters",
    "text": "Image parameters\nThe images will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#setting-the-schedule-parameters",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#setting-the-schedule-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Setting the schedule parameters",
    "text": "Setting the schedule parameters\nRecall that there are three kinds of schedules:\n\nA static schedule with a constant \\(G\\).\n\nA decreasing Cosine schedule.\n\nA Cosine schedule with some initial warm up steps.\n\nWe already created the static schedule 1. in the baseline above. This section creates variations of schedules 2. and 3..\n:::: {.callout-note}.\nWe need smaller guidance values for T-Normalization and Full Normalization.\nThese normalizations get their own, smaller value of \\(G_\\text{small} = 0.15\\). This smaller value keeps the guidance update vector \\(\\left( t - u \\right)\\) from exploding in scale.\n::::\n\n# Default schedule parameters from the blog post\n######################################\nmax_val           = 7.5   # guidance scaling value\nmin_val           = 1     # minimum guidance scaling\nnum_steps         = 50    # number of diffusion steps\nnum_warmup_steps  = 0     # number of warmup steps\nwarmup_init_val   = 0     # the intial warmup value\nnum_cycles        = 0.5   # number of cosine cycles\nk_decay           = 1     # k-decay for cosine curve scaling \n\n# smaller values for T-Norm and FullNorm\nmax_T = 0.15\nmin_T = 0.05\n######################################\n\nTo make sure our changes always reference this shared starting point, we can wrap these parameters in a dictionary.\nWe also create a matching dictionary for the T-Norm params.\n\nDEFAULT_COS_PARAMS = {\n    'max_val':           max_val,\n    'num_steps':         num_steps,\n    'min_val':           min_val,\n    'num_cycles':        num_cycles,\n    'k_decay':           k_decay,\n    'num_warmup_steps':  num_warmup_steps,\n    'warmup_init_val':   warmup_init_val,\n}\n\nDEFAULT_T_PARAMS = {\n    'max_val':           max_T, # max G_small value\n    'num_steps':         num_steps,\n    'min_val':           min_T, # min G_small value\n    'num_cycles':        num_cycles,\n    'k_decay':           k_decay,\n    'num_warmup_steps':  num_warmup_steps,\n    'warmup_init_val':   warmup_init_val,\n}\n\nEvery new, incremental schedule will start from these shared dictionaries. Then, a single parameter is changed at a time.\nThe cos_harness below gives us an easy way of making these minimum-pair changes.\n\ndef cos_harness(new_params={}, default_params={}):\n    '''Creates cosine schedules with updated parameters in `new_params`\n    '''\n    # start from the given baseline `cos_params`\n    cos_params = dict(default_params)\n    # update the schedule with any new parameters\n    cos_params.update(new_params)\n    \n    # return the new cosine schedule\n    sched = get_cos_sched(**cos_params)\n    return sched"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#plotting-the-cosine-schedules",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#plotting-the-cosine-schedules",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Plotting the Cosine Schedules",
    "text": "Plotting the Cosine Schedules\nNow we create the different Cosine schedules that will be swept.\n\ncos_params = {\n    'num_warmup_steps': [5, 10],\n    'num_cycles':       [1, 1.5, 2],\n    'k_decay':          [0.7, 2],\n    'max_val':          [8, 10, 12],\n    'min_val':          [2, 3],\n}\n\n# create the cosine experiments\ncos_func  = partial(cos_harness, default_params=DEFAULT_COS_PARAMS)\ncos_expts = create_expts(cos_params, cos_func)\n\n\nplot_grid([o['schedule'] for o in cos_expts], rows=4, titles=[o['title'] for o in cos_expts])\n\n\n\n\nWe repeat the steps above to create the T-Norm experiments\n\nT_params = {\n    'num_warmup_steps': [5, 10],\n    'num_cycles':       [1, 1.5, 2],\n    'k_decay':          [0.7, 2],\n    'max_val':          [0.1, 0.2, 0.3],\n    'min_val':          [0.01, 0.1],\n}\n\n# create the T-norm cosine experiments\nT_func  = partial(cos_harness, default_params=DEFAULT_T_PARAMS)\nT_expts = create_expts(T_params, T_func)\n\nWe also plot the T-Norm schedules below. Note that we are trying a few max and min values.\n\nplot_grid([o['schedule'] for o in T_expts], rows=4, titles=[o['title'] for o in T_expts])"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#running-the-normalized-cosine-experiments",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#running-the-normalized-cosine-experiments",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Running the normalized cosine experiments",
    "text": "Running the normalized cosine experiments\nNext we sweep the schedules for each type of normalization."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-runs",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "BaseNorm runs",
    "text": "BaseNorm runs\n\nprint('Running the BaseNorm experiments...')\nbase_norm_cos_res = run(prompt, cos_expts, guide_tfm=BaseNormGuidance)\n\nRunning the BaseNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.BaseNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"k_decay\", val=0.7...\nRunning experiment [2 of 12]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 12]: Param: \"max_val\", val=8...\nRunning experiment [4 of 12]: Param: \"max_val\", val=10...\nRunning experiment [5 of 12]: Param: \"max_val\", val=12...\nRunning experiment [6 of 12]: Param: \"min_val\", val=2...\nRunning experiment [7 of 12]: Param: \"min_val\", val=3...\nRunning experiment [8 of 12]: Param: \"num_cycles\", val=1...\nRunning experiment [9 of 12]: Param: \"num_cycles\", val=1.5...\nRunning experiment [10 of 12]: Param: \"num_cycles\", val=2...\nRunning experiment [11 of 12]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [12 of 12]: Param: \"num_warmup_steps\", val=10...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-runs",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "T-Norm runs",
    "text": "T-Norm runs\n\nprint('Running the T-Norm experiments...')\nt_norm_cos_res = run(prompt, T_expts, guide_tfm=TNormGuidance)\n\nRunning the T-Norm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"k_decay\", val=0.7...\nRunning experiment [2 of 12]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.1...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.2...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.3...\nRunning experiment [6 of 12]: Param: \"min_val\", val=0.01...\nRunning experiment [7 of 12]: Param: \"min_val\", val=0.1...\nRunning experiment [8 of 12]: Param: \"num_cycles\", val=1...\nRunning experiment [9 of 12]: Param: \"num_cycles\", val=1.5...\nRunning experiment [10 of 12]: Param: \"num_cycles\", val=2...\nRunning experiment [11 of 12]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [12 of 12]: Param: \"num_warmup_steps\", val=10...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-runs",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-runs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "FullNorm runs",
    "text": "FullNorm runs\n\nprint('Running the FullNorm experiments...')\nfull_norm_cos_res = run(prompt, T_expts, guide_tfm=FullNormGuidance)\n\nRunning the FullNorm experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"k_decay\", val=0.7...\nRunning experiment [2 of 12]: Param: \"k_decay\", val=2...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.1...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.2...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.3...\nRunning experiment [6 of 12]: Param: \"min_val\", val=0.01...\nRunning experiment [7 of 12]: Param: \"min_val\", val=0.1...\nRunning experiment [8 of 12]: Param: \"num_cycles\", val=1...\nRunning experiment [9 of 12]: Param: \"num_cycles\", val=1.5...\nRunning experiment [10 of 12]: Param: \"num_cycles\", val=2...\nRunning experiment [11 of 12]: Param: \"num_warmup_steps\", val=5...\nRunning experiment [12 of 12]: Param: \"num_warmup_steps\", val=10...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-results",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#basenorm-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "BaseNorm results",
    "text": "BaseNorm results"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-results",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#t-norm-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "T-Norm results",
    "text": "T-Norm results"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-results",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#fullnorm-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "FullNorm results",
    "text": "FullNorm results"
  },
  {
    "objectID": "blog/posts/2022-11-23-guidance-expts-4/index.html#analysis",
    "href": "blog/posts/2022-11-23-guidance-expts-4/index.html#analysis",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 3",
    "section": "Analysis",
    "text": "Analysis\nThere are many images and parameter changes going on.\nBroadly speaking, across normalizations, the following schedules show the most promise:\n\nChanging k-decay.\n\nAllowing for some warmup steps.\n\nIncreasing the maximum value of \\(G\\).\n\nAllow the cosine to go through more cycles.\n\nThe other changes either have negligible gains or actively corrupted the image."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "",
    "text": "Exploring a range of guidance values for T-Normalization."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#recap-of-parts-1-4",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#recap-of-parts-1-4",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Recap of Parts 1-4",
    "text": "Recap of Parts 1-4\nThe first three parts explored how to turn Classifier-free Guidance into a dynamic process. We found an initial set of schedules and normalizers that seem to improve the quality of Diffusion images. We then dug in and refined a few of the most promising schedules."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#part-5-exploring-values-for-t-normalization",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#part-5-exploring-values-for-t-normalization",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Part 5: Exploring values for T-Normalization",
    "text": "Part 5: Exploring values for T-Normalization\nPart 5 answers the question: what should the value of \\(G_\\text{small}\\) be for T-Normalization and Full Normalization?\nRecall that these two normalizations scale the update vector \\(\\left(t - u \\right)\\). That places the update vector on a different scale than the unconditioned vector \\(u\\). If we then scaled the update vector by a large scalar, say \\(G = 7.5\\), the output collapses to noise. In fact it seems to collapse to the true mode of the latent image distribution: uniform, brown values.\nThese two normalizations are very promising: they improve the syntax and details of the image. However, we only explored a single value of \\(G_\\text{small} = 0.15\\). This is very different from the default \\(G = 7.5\\) that has been truly explored in regular Classifier-free Guidance.\nThis notebook tries to find a good starting point for \\(G_\\text{small}\\), so we can try the normalizations with our best schedules so far."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed and pseudo random number generator\nSEED = 1024\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#image-parameters",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. They will have a height and width of 512 x 512 pixels.\n\n# the number of diffusion steps\nnum_steps = 50\n\n# generated image dimensions\nwidth, height = 512, 512"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-sweep",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-sweep",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "T-Normalization with \\(G_\\text{small}\\) sweep",
    "text": "T-Normalization with \\(G_\\text{small}\\) sweep\n\nprint('Running the k-Sweep experiments...')\nt_norm_res = run(prompt, const_expts, guide_tfm=TNormGuidance)\n\nRunning the k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 10]: Param: \"max_val\", val=0.01...\nRunning experiment [2 of 10]: Param: \"max_val\", val=0.03...\nRunning experiment [3 of 10]: Param: \"max_val\", val=0.05...\nRunning experiment [4 of 10]: Param: \"max_val\", val=0.08...\nRunning experiment [5 of 10]: Param: \"max_val\", val=0.1...\nRunning experiment [6 of 10]: Param: \"max_val\", val=0.15...\nRunning experiment [7 of 10]: Param: \"max_val\", val=0.2...\nRunning experiment [8 of 10]: Param: \"max_val\", val=0.22...\nRunning experiment [9 of 10]: Param: \"max_val\", val=0.25...\nRunning experiment [10 of 10]: Param: \"max_val\", val=0.3...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-sweep",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-sweep",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Full Normalization with \\(G_\\text{small}\\) sweep",
    "text": "Full Normalization with \\(G_\\text{small}\\) sweep\n\nprint('Running the k-Sweep experiments...')\nfull_norm_res = run(prompt, const_expts, guide_tfm=FullNormGuidance)\n\nRunning the k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 10]: Param: \"max_val\", val=0.01...\nRunning experiment [2 of 10]: Param: \"max_val\", val=0.03...\nRunning experiment [3 of 10]: Param: \"max_val\", val=0.05...\nRunning experiment [4 of 10]: Param: \"max_val\", val=0.08...\nRunning experiment [5 of 10]: Param: \"max_val\", val=0.1...\nRunning experiment [6 of 10]: Param: \"max_val\", val=0.15...\nRunning experiment [7 of 10]: Param: \"max_val\", val=0.2...\nRunning experiment [8 of 10]: Param: \"max_val\", val=0.22...\nRunning experiment [9 of 10]: Param: \"max_val\", val=0.25...\nRunning experiment [10 of 10]: Param: \"max_val\", val=0.3...\nDone."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-g_textsmall-results",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-g_textsmall-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "T-Normalization \\(G_\\text{small}\\) results",
    "text": "T-Normalization \\(G_\\text{small}\\) results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-g_textsmall-results",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-g_textsmall-results",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Full Normalization \\(G_\\text{small}\\) results",
    "text": "Full Normalization \\(G_\\text{small}\\) results"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#phase-change-in-the-image",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#phase-change-in-the-image",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Phase change in the image",
    "text": "Phase change in the image\nMost interesting, there is a “phase change” between the values of 0.08 and 0.1. The image completely changes style and pose from the previous results we’ve seen so far in the series. This phase change on its own deserves more exploration! What happens around these values of \\(G_\\text{small}\\)?\nLet’s re-run experiments focused on this range. We will pick 10 points uniformly spread between 0.08 and 0.1 to see if we can catch where the phase changes."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-phase-change",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#t-normalization-with-g_textsmall-phase-change",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "T-Normalization with \\(G_\\text{small}\\) phase change",
    "text": "T-Normalization with \\(G_\\text{small}\\) phase change\n\n\n\n\n\nIt seems the phase change happens between 0.088 and 0.09. Let’s check if this is also true for Full Normalization."
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-phase-change",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#full-normalization-with-g_textsmall-phase-change",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Full Normalization with \\(G_\\text{small}\\) phase change",
    "text": "Full Normalization with \\(G_\\text{small}\\) phase change\n\nprint('Running the phase change k-Sweep experiments...')\nfull_phase_res = run(prompt, phase_expts, guide_tfm=FullNormGuidance)\n\nRunning the phase change k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.FullNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"max_val\", val=0.08...\nRunning experiment [2 of 12]: Param: \"max_val\", val=0.082...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.084...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.086...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.088...\nRunning experiment [6 of 12]: Param: \"max_val\", val=0.09...\nRunning experiment [7 of 12]: Param: \"max_val\", val=0.092...\nRunning experiment [8 of 12]: Param: \"max_val\", val=0.094...\nRunning experiment [9 of 12]: Param: \"max_val\", val=0.096...\nRunning experiment [10 of 12]: Param: \"max_val\", val=0.09799999999999999...\nRunning experiment [11 of 12]: Param: \"max_val\", val=0.09999999999999999...\nRunning experiment [12 of 12]: Param: \"max_val\", val=0.102...\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe phase change happens in the same place! In fact the change is more pronounced, there is definitely something strange with the horse’s head as we hit the phase transition around \\(G_\\text{small} = 0.09\\).\nHowever, it seems that the image grows darker and less clear as we move away from the phase change. The horse’s body is less illuminated and is even hard to see.\nOne last check, what if the images before the phase change are better? We already saw that 0.05 was a bit too low, but what about values between 0.06 and 0.08?"
  },
  {
    "objectID": "blog/posts/2022-11-24-guidance-expts-6/index.html#checking-for-phase-change-multiples",
    "href": "blog/posts/2022-11-24-guidance-expts-6/index.html#checking-for-phase-change-multiples",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 5",
    "section": "Checking for phase change multiples",
    "text": "Checking for phase change multiples\nWill we find another phase change around three times from the first one? Let’s find out.\n\nlow_bound = 0.25\nhi_bound = 0.29\nnpoints = 11\n\npoints = np.linspace(low_bound, hi_bound, npoints+1); points\n\narray([0.25      , 0.25363636, 0.25727273, 0.26090909, 0.26454545,\n       0.26818182, 0.27181818, 0.27545455, 0.27909091, 0.28272727,\n       0.28636364, 0.29      ])\n\n\n\n# create the constant G_small cosine experiments\nlater_phase_params = {'max_val': list(points)}\nlater_phase_func = lambda val: [val for _ in range(num_steps)]\nlater_phase_expts = create_expts(later_phase_params, later_phase_func)\n\n\nprint('Running the phase change k-Sweep experiments...')\nlater_phase_res = run(prompt, later_phase_expts, guide_tfm=TNormGuidance)\n\nRunning the phase change k-Sweep experiments...\nUsing Guidance Transform: &lt;class 'cf_guidance.transforms.TNormGuidance'&gt;\nRunning experiment [1 of 12]: Param: \"max_val\", val=0.25...\nRunning experiment [2 of 12]: Param: \"max_val\", val=0.25363636363636366...\nRunning experiment [3 of 12]: Param: \"max_val\", val=0.25727272727272726...\nRunning experiment [4 of 12]: Param: \"max_val\", val=0.2609090909090909...\nRunning experiment [5 of 12]: Param: \"max_val\", val=0.26454545454545453...\nRunning experiment [6 of 12]: Param: \"max_val\", val=0.2681818181818182...\nRunning experiment [7 of 12]: Param: \"max_val\", val=0.2718181818181818...\nRunning experiment [8 of 12]: Param: \"max_val\", val=0.27545454545454545...\nRunning experiment [9 of 12]: Param: \"max_val\", val=0.27909090909090906...\nRunning experiment [10 of 12]: Param: \"max_val\", val=0.2827272727272727...\nRunning experiment [11 of 12]: Param: \"max_val\", val=0.2863636363636364...\nRunning experiment [12 of 12]: Param: \"max_val\", val=0.29...\nDone.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is no clear phase change, but the image is starting to fall apart. It is safe to say we are in territory where \\(G_\\text{small}\\) is too large.\n\\(G_\\text{small} = 0.25\\) is the last image where we have a fully correct, non-smeared astronaut."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "",
    "text": "Improving generated images with dynamic Classifier-free Guidance across Diffusion models."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#recap-of-parts-1-6",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#recap-of-parts-1-6",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Recap of Parts 1-6",
    "text": "Recap of Parts 1-6\nIn the first six parts, we found a good, initial set of schedules and normalizations. The most promising schedules are used in this notebook."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#part-7-improvement-across-models",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#part-7-improvement-across-models",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Part 7: Improvement across models",
    "text": "Part 7: Improvement across models\nPart 7 runs our best schedules on the following Diffusion models:\n\nStable Diffusion v1-4\n\nStable Diffusion v1-5\n\nPrompt Hero’s openjourney\n\nStable Diffusion 2-base"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#seed-for-reproducibility",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#seed-for-reproducibility",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Seed for reproducibility",
    "text": "Seed for reproducibility\nseed_everything makes sure that the results are reproducible across notebooks.\n\n# set the seed for rng\nSEED = 977145576 \ndef seed_everything(seed: int) -&gt; torch.Generator:\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    generator = torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    return generator\n\n# for sampling the initial, noisy latents\ngenerator = seed_everything(SEED)"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#image-parameters",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#image-parameters",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Image parameters",
    "text": "Image parameters\nImages will be generated over \\(50\\) diffusion steps. The height and width will depend on the Stable Diffusion model\n\n# the number of diffusion steps\nnum_steps = 50\n\n# dimensions for v1 and v2 Stable Diffusions\nv1_sd_dims = {'height': 640, 'width': 512}\nv2_sd_dims = {'height': 768, 'width': 768}"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#static-baselines",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#static-baselines",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Static baselines",
    "text": "Static baselines\nFirst we create the constant, baseline Guidances.\nFor Prediction Normalization we use the same default of \\(G = 7.5\\). For T-Normalization and Full Normalization, we use a static \\(G_\\text{small} = 0.15\\).\n\n# create the baseline schedule with the new function\nbaseline_g = 7.5\nbaseline_params = {'max_val': [baseline_g]}\nbaseline_func = lambda *args, **kwargs: [baseline_g for _ in range(num_steps)]\nbaseline_expts = create_expts(baseline_params, baseline_func)\n\nT_baseline_g = 0.15\nT_baseline_params = {'max_val': [T_baseline_g]}\nT_baseline_func = lambda *args, **kwargs: [T_baseline_g for _ in range(num_steps)]\nT_baseline_expts = create_expts(T_baseline_params, T_baseline_func)"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#improving-the-baseline-with-schedules-and-normalizations",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Improving the baseline with schedules and normalizations",
    "text": "Improving the baseline with schedules and normalizations\nNow we build the most promising schedule so far: Inverse kDecay with a fast warmup.\n\n# start by creating regualr kDecay cosine schedules\ninv_k_params = {'k_decay': [0.15]}\ninv_k_func = partial(cos_harness, default_params=DEFAULT_COS_PARAMS)\ninv_k_expts = create_expts(inv_k_params, inv_k_func)\n\n# invert the schedules to turn them into a type of warmup \nfor s in inv_k_expts:\n    s['schedule'] = [max_val - g + min_val for g in s['schedule']]\n\n# put all schedules together\nall_k_expts = inv_k_expts\n\n\n\n\n\n\nWe also build a matching schedule with smaller \\(G\\) values for the T and Full Normalizations.\n\n# create the kDecay cosine experiments\nT_inv_k_func = partial(cos_harness, default_params=DEFAULT_T_PARAMS)\nT_inv_k_expts = create_expts(inv_k_params, T_inv_k_func)\n\n# inverse the schedules\nfor s in T_inv_k_expts:\n    s['schedule'] = [max_T - g + min_T for g in s['schedule']]\n\nall_T_k_expts = T_inv_k_expts"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#function-to-run-the-experiments",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#function-to-run-the-experiments",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Function to run the experiments",
    "text": "Function to run the experiments\nThe previous notebooks ran one Diffusion model at a time. Now, we need to load the model as part of the pipeline.\n\n# to load Stable Diffusion pipelines\nfrom min_diffusion.core import MinimalDiffusion\n# to plot generated images\nfrom min_diffusion.utils import show_image, image_grid, plot_grid\n\nTo do this, we move the model loading code load_sd_model into the run function. We also add some memory cleanup at the end to free up the GPU for the next model.\n\ndef load_sd_model(model_name, device, dtype, model_kwargs={}, generator=None):\n    '''Loads the given `model_name` Stable Diffusion in `dtype` precision.  \n    \n    The model is placed on the `device` hardware. \n    The optional `generator` is used to create noisy latents.  \n    Optional `model_kwargs` are passed to the model's load function.\n    '''\n    pipeline = MinimalDiffusion(model_name, device, dtype, generator=generator)\n    pipeline.load(**model_kwargs);\n    return pipeline\n\n\ndef run(pipeline, prompt, schedules, gen_kwargs={},\n        guide_tfm=None, generator=None, show_each=False, test_run=False):\n    \"\"\"Runs a dynamic Classifier-free Guidance experiment. \n    \n    Generates an image for the text `prompt` given all the values in `schedules`.\n    Uses a Guidance Transformation class from the `cf_guidance` library.  \n    Stores the output images with a matching title for plotting. \n    Optionally shows each image as its generated.\n    If `test_run` is true, it runs a single schedule for testing. \n    \"\"\"\n    # store generated images and their title (the experiment name)\n    images, titles = [], []\n    \n    # make sure we have a valid guidance transform\n    assert guide_tfm\n    print(f'Using Guidance Transform: {guide_tfm}')\n    \n    # optionally run a single test schedule\n    if test_run:\n        print(f'Running a single schedule for testing.')\n        schedules = schedules[:1]\n        \n    # run all schedule experiments\n    for i,s in enumerate(schedules):\n        \n        # parse out the title for the current run\n        cur_title  = s['title']\n        titles.append(cur_title)\n        \n        # create the guidance transformation \n        cur_sched = s['schedule']\n        gtfm = guide_tfm({'g': cur_sched})\n        \n        print(f'Running experiment [{i+1} of {len(schedules)}]: {cur_title}...')\n        img = pipeline.generate(prompt, gtfm, **gen_kwargs)\n        images.append(img)\n        \n        # optionally plot each generated image\n        if show_each:\n            show_image(img, scale=1)\n            \n    print('Done.')\n    return {'images': images,\n            'titles': titles}"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#reading-the-plots",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#reading-the-plots",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Reading the plots",
    "text": "Reading the plots\nFor each model, we plot a grid with its generated images. The grid has two rows and four columns.\nThe first row shows results from the fixed, constant Guidance. The second row shows results for the Inverse kDecay cosine schedules.\nThe first column shows the baseline: unnormalized Classifier-free Guidance with a constant \\(G = 7.5\\).\nThe second column has the Prediction Normalization results.\nThe third column has the T-Normalization results.\nThe fourth column has the Full Normalization results.\nIn general we expect that normalization should improve the images. In other words, the second, third, and fourth column should be better than the first column (the baseline).\nLikewise, we expect that the Inverse kDecay schedules are better than the static schedules. That means that, for a given column, the result in its second row should be better than its first row.\n\n\n\n\n\n\nNote\n\n\n\nThe plotting functions are available in the notebook. They are omitted here for space."
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-4",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-4",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Stable Diffusion v1-4",
    "text": "Stable Diffusion v1-4\n\nplot_all_results('CompVis/stable-diffusion-v1-4')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-5",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-v1-5",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Stable Diffusion v1-5",
    "text": "Stable Diffusion v1-5\n\nplot_all_results('runwayml/stable-diffusion-v1-5')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#openjourney",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#openjourney",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "openjourney",
    "text": "openjourney\n\nplot_all_results('prompthero/openjourney')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-2-base",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#stable-diffusion-2-base",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Stable Diffusion 2-base",
    "text": "Stable Diffusion 2-base\n\nplot_all_results('stabilityai/stable-diffusion-2-base')"
  },
  {
    "objectID": "blog/posts/2022-11-26-guidance-expts-8/index.html#evaluating-the-outputs",
    "href": "blog/posts/2022-11-26-guidance-expts-8/index.html#evaluating-the-outputs",
    "title": "Classifier-free Guidance with Cosine Schedules Pt. 7",
    "section": "Evaluating the outputs",
    "text": "Evaluating the outputs\nIn general, it seems that Prediction Normalization adds more details to the image and background. T-Normalization makes the image “smoother” and can help with its syntax. Full Normalization, which is a combination of the two, seems to get a bit from both worlds."
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html",
    "title": "Tips for LLM prompts",
    "section": "",
    "text": "General tips to get better outputs from ChatGPT.\n# Tips and tricks for ChatGPT Prompts\nThis post is a recap of openai’s suggestions (as of writing) for improving ChatGPT’s outputs.\nThere are many teams actively working on improving and deploying new LLMs with useful (powerful) abilities. Their work has produced several papers that show different ways of improving an LLM’s output.\nThe official openAI documentation has the full details and discussion of a few key papers.\nHowever, we can also extract a broad set of suggestions based on what the different proposed improvements share in common."
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html#improving-outputs",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html#improving-outputs",
    "title": "Tips for LLM prompts",
    "section": "Improving outputs",
    "text": "Improving outputs\n\nSplit large, complex tasks into subtasks\n\nStructure and isolate the instructions of each subtask\n\nPrompt the model to explain its reasoning(s) before answering\nIf the output was bad, try making the instructions clearer\n\nStart with simple and direct language\nCan get more complex as the conversation and context grow\n\nHave the model generate many answers, then ask it to distill them into a single, best answer\nIf possible, Fine-tune custom models to maximize performance"
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html#generic-tips",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html#generic-tips",
    "title": "Tips for LLM prompts",
    "section": "Generic tips",
    "text": "Generic tips\n\nExplicitly guide the model through the thought process\n\nHelps it stay focused on sub-tasks and subprocesses\n\n“Let’s think step by step…”\n\nWorks best on logical, mathematical, and reasoning tasks\nPossible leverage for other tasks by breaking them down into “logical” steps\n\nGive the model a few examples of the task you want (Few-Shot)\nSplit a question into two types of prompts and alternate between the two\n\nSelection prompt -&gt; find the relevant pieces of into\nInference prompt -&gt; use the relevant pieces to generate the answer\nHalter prompt -&gt; figure out when the alternating should halt, if possible add a value function to evaluate different prompts\n\nReduce hallucinations by constraining what the model can say"
  },
  {
    "objectID": "blog/posts/2023-04-17-llm-prompt-tips/index.html#api-tips",
    "href": "blog/posts/2023-04-17-llm-prompt-tips/index.html#api-tips",
    "title": "Tips for LLM prompts",
    "section": "API Tips",
    "text": "API Tips\n\nGive the model an identity that behaves in a certain way with an explicit intent\nAsk to model to answer from the perspective of an expert\nTry restating the original “system” message to keep the model on-task.\nIf the model is getting off-track, try reminding it of the instruction and context at the end of the prompt"
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html",
    "href": "blog/posts/fractal-llms/01-env/index.html",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "",
    "text": "Creating an LLM python environment with mamba and pip"
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#installing-mamba",
    "href": "blog/posts/fractal-llms/01-env/index.html#installing-mamba",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "Installing mamba",
    "text": "Installing mamba\nmamba offers an installation script that handles all of the setup for us. But in case you run into any issues, here is a link to the official installation instructions.\nNext we install mamba on a Mac computer.\n\n\n\n\n\n\nTip\n\n\n\nThe installation steps are identical for Linux, but they change a bit for Windows.\n\n\n\nmamba on Mac\nHow do we know which mamba installation script to use? The uname shell command comes to the rescue. It returns information about the computer and system it is called from. We can use uname to automatically grab the right installation script for our specific Mac.\nThe bash commands below will do the following:\n- Find the appropriate mamba Mac installation script.\n- Download the script from the official mamba repo.\n# find the name of the appropriate installation script\nscript_name=\"Mambaforge-$(uname)-$(uname -m).sh\"\n\n# mamba repo url with all the installation scripts\nscript_repo=\"https://github.com/conda-forge/miniforge/releases/latest/download/\"\n\n# download the appropriate script\ncurl -L -O ${script_repo}/${script_name}\nNote that this command downloads the script into the directory that you’re running it from.\nOnce the shell script is downloaded, run it to install mamba:\n# run the Mambaforge installer\nbash Mambaforge-$(uname)-$(uname -m).sh\n\n\n\n\n\n\nNote\n\n\n\nIf you prefer to download the script directly, grab it from here: https://github.com/conda-forge/miniforge/releases/\n\n\nThe script now steps through the installation process. It will prompt you for some info along the way, but you can accept all of the defaults for now (i.e. don’t type anything in, just hit enter).\nOnce mamba is installed, we are ready to create a base python environment."
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#bringing-in-pip",
    "href": "blog/posts/fractal-llms/01-env/index.html#bringing-in-pip",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "Bringing in pip",
    "text": "Bringing in pip\nWe could install all of the needed python libraries with mamba. However, we will use python’s built in pip package manager instead.\nThis is because we’ll rely on some new and state-of-the-art code repos. Repos that are not always available via mamba. And, more than that, sometimes the repos need extra installation steps which are better handled through pip. To recap: pip offers us more flexibility and power than mamba when installing bleeding edge LLM libraries.\nFirst, make sure that the new llm-env environment is activated. Then we’ll install a few basic libraries that just about all LLM applications need.\n\nInstalling pytorch\nThe basic library we will need is pytorch. This is the main library that handles most of the heavy lifting for python Neural Networks.\n# install the pytorch libraries\npip install torch torchvision torchaudio\nNote that this will also grab and install pytorch’s many dependencies.\n\n\nInstalling helper libraries\nNext we install a few helper libraries for the rest of the course. These are libraries for editing Jupyter notebooks, making plots, and writing a blog. We also install the popular scientific package scipy, which many ML libraries rely on.\n# install the jupyter notebook library\npip install jupyterlab\n\n# install matplotlib for drawing plots\npip install matplotlib\n\n# library for writing blogs\npip install nbdev \n\n# helpful python utilities\npip install fastcore\n\n# a powerful scientific library\npip install scipy \n\n\nAside: Installing Rust\nMany LLMs rely on the Rust programming language for fast and optimized tokenizers. Run the short command below to install Rust on our systems and leverage the optimized tokenizers:\n# install Rust\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n\n\nInstalling HuggingFace libraries\nNext up, we install a suite of HuggingFace libraries for dealing with LLMs. With these libraries you’ll be able to fully leverage the powerful tools offered by the HuggingFace team.\nWe won’t use all of them initially, but they will be available should you ever need them in other personal projects.\n# install the main LLM library\npip install transformers\n\n# library for optimized LLM training \npip install accelerate\n\n# library for optimized LLM inference\npip install optimum\n\n# quick access to great data utilities\npip install datasets\n\n# install an optimized tokenizer library\npip install setuptools-rust\npip install tiktoken\nCongrats! We have now created a powerful python environment for LLMs. Going forward, we will use this llm-env in the rest of the course."
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#an-environment-for-the-future",
    "href": "blog/posts/fractal-llms/01-env/index.html#an-environment-for-the-future",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "An environment for the future",
    "text": "An environment for the future\nThere is a nice benefit to spending this much time up front on our environment.\nWe now not only have a specialized environment to run and fine-tune an LLM. But it is also a springboard to keep up with the state of the art in the field. A setup in which to bring in other groundbreaking improvements. And, to weave in the latest and greatest models as they are released. The LLM world is now our oyster, and llm-env the small grain of sand-would-be-pearl."
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#helper-libraries",
    "href": "blog/posts/fractal-llms/01-env/index.html#helper-libraries",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "Helper libraries",
    "text": "Helper libraries\nFirst, some best practice. Make sure to update the Ubuntu package list:\n# update the ubuntu package list\nsudo apt update\nThere are two Ubuntu packages that are worth installing:\n- software-properties-common - build-essential\nsoftware-properties-common is a set of tools for adding and managing software repositories. It makes our life a bit easier.\nbuild-essential contains a list of packages that are essential for building Ubuntu packages. It has software key for development like the GNU Compiler Collection (GCC) and GNU Make. It also has the tools to build and install projects from source (aka straight from the repo’s folder).\n# install useful linux packages\nsudo apt install software-properties-common\nsudo apt install build-essential"
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#installing-nvidia-drivers",
    "href": "blog/posts/fractal-llms/01-env/index.html#installing-nvidia-drivers",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "Installing NVIDIA Drivers",
    "text": "Installing NVIDIA Drivers\nWe’ll use one of the most reliable and straightforward methods to install the NVIDIA drivers: the graphics drivers PPA.\n# add the graphics drivers ppas\nsudo add-apt-repository ppa:graphics-drivers/ppa\n\n# update the package list again\nsudo apt update\nNow we can install the nvidia drivers themselves. As of writing, the 535 version of the driver is stable and supports a good number of GPU cards.\n# install the nvidia drivers\nsudo apt install nvidia-driver-535\n\n\n\n\n\n\nImportant\n\n\n\nAfter installing the drivers, make sure to reboot your system before going forward!\n# restart the system after installing the drivers\nsudo reboot\n\n\nOnce the machine is back up, run the following command to check if the drivers were installed correctly.\n# should show us any available gpus\nnvidia-smi"
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#installing-cuda",
    "href": "blog/posts/fractal-llms/01-env/index.html#installing-cuda",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "Installing CUDA",
    "text": "Installing CUDA\nWith the drivers working, we can now install the CUDA library. The CUDA library has a set of ML tools optimized for NVIDIA GPUs.\nThe example below uses a local .dev installer for CUDA version 12.1. The steps comee straight from the official CUDA website.\nThere is a lot going on in the steps below. But it has been, in my experience, one of the most straightforward and reliable ways to install specific CUDA versions. Other methods may be easier, but it can be harder to pin down specific versions which leads to many headaches down the road.\n# full steps to install CUDA 12.1 libraries \n\n# setting up the repo\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\nsudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\nwget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\nsudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\n\n# installing the libraries\nsudo apt-get update\nsudo apt-get -y install cuda\nHere is a full breakdown of what the bash commands above did. Feel free to skim this list on a first pass. The key takeaway: these commands install CUDA version 12.1 on our system. 12.1 is the latest version of CUDA as of writing used in many of the bleeding edge LLM libraries:\n\nDownload the Pinning File: Utilize wget to download the cuda-ubuntu2204.pin file from NVIDIA’s developer website. This file aids in managing APT preferences regarding the CUDA repository.\nRelocate and Rename the Pinning File: Move the downloaded cuda-ubuntu2204.pin file to the /etc/apt/preferences.d/ directory, and rename it to cuda-repository-pin-600. This step ensures that APT recognizes the preferences for the CUDA repository.\nFetch the CUDA Repository Package: Download the Debian package for setting up the CUDA repository on your system. Ensure to get the package corresponding to CUDA version 12.1 for Ubuntu 22.04.\nDeploy the CUDA Repository Package: Utilize dpkg to install the downloaded Debian package, which in turn sets up the CUDA repository on your system.\nTransfer the GPG Keyring File: Copy the GPG keyring file from the CUDA repository directory to your system’s keyrings directory. This file is crucial for verifying the authenticity of packages from the CUDA repository.\nRefresh the APT Package List: Instruct APT to update its list of available packages. This step incorporates the information from the newly added CUDA repository.\nInitiate CUDA Installation: Command APT to install the cuda package along with all its necessary dependencies from the CUDA repository. The -y flag is used to automate the process by affirming “yes” to any prompts encountered.\n\n\n\n\n\n\n\nImportant\n\n\n\nAfter installing CUDA, we need to run the following lines to modify our ~/.bashrc file. These changes make sure that we can actually see and find the newly installed CUDA libraries:\n# modify paths so we can find CUDA binaries\necho 'export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}' &gt;&gt; ~/.bashrc\necho 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\nThen reboot the system on more time, with feeling:\nsudo reboot\n\n\nOnce the machine is back online, run the following command to check if CUDA was installed correctly:\n# this command shows us the CUDA version\nnvcc --version\n\n# # it should output something like this:\n#   nvcc: NVIDIA (R) Cuda compiler driver\n#   Copyright (c) 2005-2023 NVIDIA Corporation\n#   Built on Tue_Feb__7_19:32:13_PST_2023\n#   Cuda compilation tools, release 12.1, V12.1.66\n#   Build cuda_12.1.r12.1/compiler.32415258_0"
  },
  {
    "objectID": "blog/posts/fractal-llms/01-env/index.html#installing-accelerated-cuda-libraries-on-linux",
    "href": "blog/posts/fractal-llms/01-env/index.html#installing-accelerated-cuda-libraries-on-linux",
    "title": "Lesson 1: A Python Environment for LLMs",
    "section": "Installing accelerated CUDA libraries on Linux",
    "text": "Installing accelerated CUDA libraries on Linux\nIf you are running on a Linux machine, you’ll have access to many powerful libraries to speed up LLM training and inference even more. Not all of these are available on Mac or Windows, but hopefully that changes with time.\n\n\n\n\n\n\nNote\n\n\n\nHere we also see the first instance of needing a pip install with extra steps - something we could not have done with mamba alone.\n\n\n# install the optimized CUDA LLM libraries:\n\n# library to massively speed up Transformer LLMs\npip install flash-attn --no-build-isolation\n\n# library crucial for quantized LLMs\npip install bitsandbytes \n\n# xformers library from Meta\npip install install -U xformers --index-url https://download.pytorch.org/whl/cu121\nAnd that does it! Phew, we made it. After following the above, your Ubuntu 22.04 machine stands ready at the bleeding edge of LLMs."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "",
    "text": "Running powerful NLP models with the HuggingFace transformers library."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#notebook-best-practices",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#notebook-best-practices",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Notebook best practices",
    "text": "Notebook best practices\nFirst, let’s set up our notebook to be fully interactive and easy to use. We can do this with a couple of “magic functions” built-in to Jupyter.\nSpecifically, we use the magic autoreload and matplotlib functions. The cell below shows them in action:\n\n# best practice notebook magic\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nLet’s take a look at what these magic functions do.\nautoreload dynamically reloads code libraries, even as they’re changing under the hood. That means we do not have to restart the notebook after every change. We can instead code and experiment on the fly.\nmatplotlib inline automatically displays any plots below the code cell that created them. The plots are also saved in the notebook itself, which is perfect for our blog posts.\nAll of our notebooks going forward will start with these magic functions.\nLet’s start with the \"hello, world!\" of NLP: sentiment analysis."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#first-a-pipeline",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#first-a-pipeline",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "First, a Pipeline",
    "text": "First, a Pipeline\nLet’s take a look at the HuggingFace NLP model that we’ll run. At a high level, the model is built around three key pieces:\n\nA Config file.\n\nA Preprocessor file.\n\nModel file(s).\n\nThe HuggingFace API has a handy, high-level pipeline that wraps up all three objects for us.\n\n\n\n\n\n\nImportant\n\n\n\nBefore going forward, make sure that the llm-env environment from the first lesson is active. This environment has the HuggingFace libraries used below.\n\n\nThe code below uses the transformers library to build a Sentiment Analysis pipeline.\n\n# load in the pipeline object from HuggingFace\nfrom transformers import pipeline\n\n# create a sentiment analysis pipeline\nclassifier = pipeline(\"sentiment-analysis\")\n\nNo model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n\n\nSince we didn’t specify a model, you can see in the output above that HuggingFace picked a distilbert model for us by default.\nWe will learn more about what exactly distilbert is and how it works later on. For now, think of it as a useful NLP genie who can look at a sentence and tell us whether its has a positive or negative tone.\nNext, let’s find out what the model thinks about the sentence: \"HuggingFace pipelines are awesome!\"\n\n# sentiment analysis on a simple, example sentence\nexample_sentence = \"HuggingFace pipelines are awesome!\"\nclassifier(example_sentence)\n\n[{'label': 'POSITIVE', 'score': 0.9998503923416138}]\n\n\nNot bad. We see a strong confident score for a POSITIVE label, as could be expected.\nWe can also pass many sentences at once, which starts to show the bulk processing power of these models. Let’s process four sentences at once: three positive ones, and a clearly negative one.\n\n# many sentences at once, in a python list\nmany_sentences = [\n    \"HuggingFace pipelines are awesome!\",\n    \"I hope you're enjoying this course so far\",\n    \"Hopefully the material is clear and useful\",\n    \"I don't like this course so far\",\n]\n\n# process many sentences at once\nresults = classifier(many_sentences)\n\n# check the tone of each sentence\nfor result in results:\n    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n\nlabel: POSITIVE, with score: 0.9999\nlabel: POSITIVE, with score: 0.9998\nlabel: POSITIVE, with score: 0.9998\nlabel: NEGATIVE, with score: 0.8758\n\n\nCongrats! You’ve now ran a HuggingFace pipeline and used it to analyze the tone of a few sentences. Next, let’s take a closer look at the pipeline object."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#config-class",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#config-class",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Config class",
    "text": "Config class\nThe config class is a simple map with the options and configurations of a model. It has the key-value pairs that define a model’s architecture and hyperparameters.\n\n# config for the model\nfrom transformers import DistilBertConfig"
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#preprocessor-class",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#preprocessor-class",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Preprocessor class",
    "text": "Preprocessor class\nThe preprocessor object in this case is a Tokenizer. Tokenizers convert strings and characters into special tensor inputs for the LLM.\n\n\n\n\n\n\nNote\n\n\n\nCorrectly pre-processing inputs is one of the most important and error-prone steps in using ML models. In other words, it’s good to offload to a class that’s already been tested and debugged.\n\n\n\n# input preprocessor to tokenize strings\nfrom transformers import DistilBertTokenizer"
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#model-class",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#model-class",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Model class",
    "text": "Model class\nThe model class holds the weights and parameters for the actual LLM. It’s the “meat and bones” of the setup, so to speak.\n\n# the text classifier model\nfrom transformers import DistilBertForSequenceClassification"
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#naming-the-model",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#naming-the-model",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Naming the model",
    "text": "Naming the model\nWe need to know a model’s full, proper name in to load it from HuggingFace. Its name is how we find the model on the HuggingFace Model Hub.\nOnce we know its full name, there is a handy from_pretrained() function that will automatically find and download the pieces for us.\nIn this case, the distilbert model’s full name is:\n&gt; distilbert-base-uncased-finetuned-sst-2-english.\n\n# sentiment analysis model name\nmodel_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n\nIn the code below we can now load each of the three NLP pieces for this model.\n\n# create the config\nconfig = DistilBertConfig.from_pretrained(model_name)\n\n# create the input tokenizer \ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\n\n# create the model\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name)\n\nNext we will compose these three pieces together to mimic the original pipeline example."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#putting-together-a-simple_pipeline",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#putting-together-a-simple_pipeline",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Putting together a simple_pipeline",
    "text": "Putting together a simple_pipeline\n\nPreprocessing the inputs\nFirst, we create a preprocess function to turn a given text string into the proper, tokenized inputs than an LLM expects.\n\ndef preprocess(text: str):\n    \"\"\"\n    Sends `text` through the model's tokenizer.  \n    The tokenizer turns words and characters into proper inputs for an NLP model.\n    \"\"\"\n    tokenized_inputs = tokenizer(text, return_tensors='pt')\n    return tokenized_inputs\n\nLet’s test this preprocessing function on the example sentence from earlier.\n\n# manually preprocessing the example sentence: \"HuggingFace pipelines are awesome!\"\npreprocess(example_sentence)\n\n{'input_ids': tensor([[  101, 17662, 12172, 13117,  2015,  2024, 12476,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n\n\nIt turned an input string into numerical embeddings for the LLM. We’ll breakdown what exactly this output means later on in the course. For now, think of it as sanitizing and formatting the text into a format that the LLM has been trained to work with.\n\n\nRunnning the model\nNext up, let’s make our own forward function that run the LLM on preprocessed inputs.\n\ndef forward(text: str):\n    \"\"\"\n    First we preprocess the `text` into tokens.\n    Then we send the `tokenized_inputs` to the model.\n    \"\"\"\n    tokenized_inputs = preprocess(text)\n    outputs = model(**tokenized_inputs)\n    return outputs\n\nLet’s check what this outputs for our running example sentence.\n\noutputs = forward(example_sentence); outputs\n\nSequenceClassifierOutput(loss=None, logits=tensor([[-4.2326,  4.5748]], grad_fn=&lt;AddmmBackward0&gt;), hidden_states=None, attentions=None)\n\n\nYou’ll see a lot going on in the SequenceClassifierOutput above. To be honest, this is where the original pipeline does most of the heavy-lifting for us. It takes the raw, detailed output from an LLM and converts it into a more human-readable format.\nWe’ll mimic this heavy-lifting by using the Config class and model outputs to find out whether the sentence is positive or negative.\n\ndef process_outputs(outs):\n    \"\"\"\n    Converting the raw model outputs into a human-readable result.\n\n    Steps:\n        1. Grab the raw \"scores\" from the model for Positive and Negative labels.  \n        2. Find out which score is the highest (aka the model's decision).  \n        3. Use the `config` object to find the class label for the highest score.  \n        4. Turn the raw score into a human-readable probability value.  \n        5. Print out the predicted labels with its probability.  \n    \"\"\"\n    # 1. Grab the raw \"scores\" that from the model for Positive and Negative labels\n    logits = outs.logits\n\n    # 2. Find the strongest label score, aka the model's decision\n    pred_idx = logits.argmax(1).item()\n\n    # 3. Use the `config` object to find the class label\n    pred_label = config.id2label[pred_idx]  \n\n    # 4. Calculate the human-readable number for the score\n    pred_score = logits.softmax(-1)[:, pred_idx].item()\n\n    # 5. return the label and score in a dictionary\n    return {\n        'label': pred_label,\n        'score': pred_score, \n    }\n\nWe can now put together a simple_pipeline, and check how it compares to the original pipeline.\n\ndef simple_pipeline(text):\n    \"\"\"\n    Putting the NLP pieces and functions together into a pipeline.\n    \"\"\"\n    # get the model's raw output\n    model_outs = forward(text)\n    # convert the raw outputs into a human readable result\n    predictions = process_outputs(model_outs)\n    return predictions\n\nCalling the simple_pipeline on the example sentence, drumroll please…\n\n# running out simple pipeline on the example text\nsimple_pipeline(example_sentence)\n\n{'label': 'POSITIVE', 'score': 0.9998503923416138}\n\n\nAnd just like that, we too a small peek under the pipeline hood and built our own, simple working version.\nOne pain point: we had to know the full, proper name of the different Distilbert* pieces to import the Config, Preprocessor, and Model. This gets overwhelming fast given the flood of LLM models released almost daily. Thankfully, HuggingFace has come up with a great solution to this problem: the Auto class."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#using-the-custom-sentimentpipeline",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#using-the-custom-sentimentpipeline",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Using the custom SentimentPipeline",
    "text": "Using the custom SentimentPipeline\nLet’s leverage both the new class and a different model, to show the power of Auto classes.\nFor fun, let’s use BERT model that was trained specifically on tweets. The full model’s name is finiteautomata/bertweet-base-sentiment-analysis.\n\n# using a different model\nnew_model_name = 'finiteautomata/bertweet-base-sentiment-analysis'\n\n\n# creating a new sentiment pipeline\nsimple_pipeline = SentimentPipeline(new_model_name)\n\nemoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n\n\nNow let’s run it on our handy example sentence.\n\n# calling our new, flexible pipeline\nsimple_pipeline(example_sentence)\n\n{'label': 'POS', 'score': 0.9908382296562195}\n\n\nCongrats! You’ve now built a flexible pipeline for Sentiment Analysis that can leverage most NLP models on the HuggingFace hub."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#asking-questions-and",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#asking-questions-and",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Asking questions: ? and ??",
    "text": "Asking questions: ? and ??\nLastly, we can literally interrogate an object in Jupyter for more information.\nIf we tag a single ? after an object, we’ll get its basic documentation (docstring). Note that we omit it here to keep the notebook from getting too busy.\n\n## the power of asking questions\nclassifier?\n\nIf we tag on two question marks: ??, then we get the full source code of the object:\n\n## really curious about classifier\nclassifier??\n\nBoth ? and ?? are excellent and quick ways to look under the hood of any object in Jupyter."
  },
  {
    "objectID": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#inspecting-a-specific-classifier-function",
    "href": "blog/posts/fractal-llms/03-hf-nlp-models/index.html#inspecting-a-specific-classifier-function",
    "title": "Lesson 3: HuggingFace NLP Models",
    "section": "Inspecting a specific classifier function",
    "text": "Inspecting a specific classifier function\nLet’s take a look at the function that does the heavy lifting for our sentiment analysis task: forward().\n\n # looking at what actually runs the inputs\nclassifier.forward\n\n&lt;bound method Pipeline.forward of &lt;transformers.pipelines.text_classification.TextClassificationPipeline object at 0x15ac5f6d0&gt;&gt;\n\n\nWhat does this function actually do? Let’s find out.\n\n# source code of the forward function\nclassifier.forward??\n\nSignature: classifier.forward(model_inputs, **forward_params)\nDocstring: &lt;no docstring&gt;\nSource:   \n    def forward(self, model_inputs, **forward_params):\n        with self.device_placement():\n            if self.framework == \"tf\":\n                model_inputs[\"training\"] = False\n                model_outputs = self._forward(model_inputs, **forward_params)\n            elif self.framework == \"pt\":\n                inference_context = self.get_inference_context()\n                with inference_context():\n                    model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n                    model_outputs = self._forward(model_inputs, **forward_params)\n                    model_outputs = self._ensure_tensor_on_device(model_outputs, device=torch.device(\"cpu\"))\n            else:\n                raise ValueError(f\"Framework {self.framework} is not supported\")\n        return model_outputs\nFile:      ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/base.py\nType:      method\n\n\nWe can see that it automatically handles whether we’re running a TensorFlow (tf) or PyTorch (pt) model. Then, it makes sure the tensors are on the correct device. Lastly is calls another function, _forward() on the prepared inputs.\nWe can follow the rabbit hole as far down as needed. Let’s take a look at the source of _forward.\n\n# going deeper\nclassifier._forward??\n\nSignature: classifier._forward(model_inputs)\nDocstring:\n_forward will receive the prepared dictionary from `preprocess` and run it on the model. This method might\ninvolve the GPU or the CPU and should be agnostic to it. Isolating this function is the reason for `preprocess`\nand `postprocess` to exist, so that the hot path, this method generally can run as fast as possible.\n\nIt is not meant to be called directly, `forward` is preferred. It is basically the same but contains additional\ncode surrounding `_forward` making sure tensors and models are on the same device, disabling the training part\nof the code (leading to faster inference).\nSource:   \n    def _forward(self, model_inputs):\n        # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\n        model_forward = self.model.forward if self.framework == \"pt\" else self.model.call\n        if \"use_cache\" in inspect.signature(model_forward).parameters.keys():\n            model_inputs[\"use_cache\"] = False\n        return self.model(**model_inputs)\nFile:      ~/mambaforge/envs/llm_base/lib/python3.11/site-packages/transformers/pipelines/text_classification.py\nType:      method\n\n\nAh, we can see it calls the model of the classifier. This is the distilbert model we saw earlier! Now we can peek under the hood at the actual Transformer LLM.\n\n# the distilbert sentiment analysis model\nclassifier.model\n\nDistilBertForSequenceClassification(\n  (distilbert): DistilBertModel(\n    (embeddings): Embeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (transformer): Transformer(\n      (layer): ModuleList(\n        (0-5): 6 x TransformerBlock(\n          (attention): MultiHeadSelfAttention(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (ffn): FFN(\n            (dropout): Dropout(p=0.1, inplace=False)\n            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n            (activation): GELUActivation()\n          )\n          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.2, inplace=False)\n)\n\n\nWe will breakdown the different pieces in this model later on in the course.\nThe important takeaway for now is that this shows the main structure of most Transformer LLM models. The changes are mostly incremental from this foundation."
  },
  {
    "objectID": "blog/posts/fractal-llms/04-llama/index.html",
    "href": "blog/posts/fractal-llms/04-llama/index.html",
    "title": "Lesson 4: Quantized LLMs with llama.cpp",
    "section": "",
    "text": "Using llama.cpp to run a quantized Mistral-v0.1 model."
  },
  {
    "objectID": "blog/posts/fractal-llms/04-llama/index.html#quantized-models",
    "href": "blog/posts/fractal-llms/04-llama/index.html#quantized-models",
    "title": "Lesson 4: Quantized LLMs with llama.cpp",
    "section": "Quantized models",
    "text": "Quantized models\nllama.cpp is a library that lets us easily run quantized LLMs. What does it mean for a model to be quantized?\nQuantizing a model reduces the amount of memory it needs to run. This means we can fit a previously too-large model on less powerful machines, like a laptop or even a smaller GPU.\nTo be more specific, quantization reduces the number of bits that represent each of the model’s weights. For example, instead of using floats with 32 bits of precision, we can use 8-bit or 4-bit floats to cut down on the overall memory.\nLLMs lose some of their accuracy and power when we quantize the weights. But, the performance drop is more than made up by the ability to run larger models on smaller machines."
  },
  {
    "objectID": "blog/posts/fractal-llms/04-llama/index.html#downloading-mistral-v0.1",
    "href": "blog/posts/fractal-llms/04-llama/index.html#downloading-mistral-v0.1",
    "title": "Lesson 4: Quantized LLMs with llama.cpp",
    "section": "Downloading Mistral-v0.1",
    "text": "Downloading Mistral-v0.1\nWe will download the Mistral-7B-Instruct-v0.1 model. What exactly does it do? We can find out by breaking down the name a bit:\n- Mistral is the name given by the developers, in this case the Mistral.ai team\n- 7B means that the model has 7 billion parameters\n- Instruct means that it was trained to follow and complete user instructions\n- v0.1 is the release version for this model\nFollow the link below to see the model on the HuggingFace Model Hub.\n\nDownload link for Mistral-7B-Instruct-v0.1\n\nOnce on the page, click on the Files and version tab near the top. Here you’ll see a big list of different quantized models.\n\nThe files shown here are variants of the same, base Mistral model that were quantized in different ways.\nThe names can be overwhelming. Let’s break them down a bit.\nYou’ll notice that each file ends with a format like this: Q*_*.gguf.\nFor example one model from the list is: mistral-7B-Instruct-v0.1.Q4_K_S.gguf. We already covered the first part of the name above.\nThe Q4 part means that the model was quantized with 4-bits. The K_S part refers to the specific flavor of quantization that was used.\nThere is an unfortunate tradeoff between quantization and performance. The fewer bits we use, the smaller and faster the model will be at the code of performance. And the more bits we use, the better its performance but the slower the model.\nIn general, the Q4 and Q5 models offer a good balance between speed, performance, and size.\nLet’s get back to running the model.\nWe choose the Q5_K_M model. It is not much larger than the Q4 models and has a performance better enough to make it worthwhile.\nBefore grabbing this model, make sure the huggingface-hub CLI is installed. This will let us download models easily in the terminal.\n# install a tool to download HuggingFace models via the terminal\npip install huggingface-hub\nThen move into the models/ folder inside of the llama.cpp repo, and download the our chosen model as follows:\n# download the Mistral Q5_K_M model\nhuggingface-cli download TheBloke/Mistral-7B-Instruct-v0.1-GGUF \\\n    mistral-7b-instruct-v0.1.Q5_K_M.gguf \\\n    --local-dir . \\\n    --local-dir-use-symlinks False\nOnce the model is downloaded, we’ll use the binary created by the build process to run it."
  },
  {
    "objectID": "blog/posts/fractal-llms/04-llama/index.html#running-the-mistral-model",
    "href": "blog/posts/fractal-llms/04-llama/index.html#running-the-mistral-model",
    "title": "Lesson 4: Quantized LLMs with llama.cpp",
    "section": "Running the Mistral model",
    "text": "Running the Mistral model\nWe’ll use the main binary inside of the build/ folder from before to run the Q5_K_M model.\nRun the following command to see the Mistral LLM in action! Here we prompt it to tell us how to build a website in 10 steps.\nThe -m flag points to the model weight we downloaded into the models/ folder.\nThe -p flag is the prompt for the model to follow.\n# from build/, run the official example to see Mistral-v0.1 in action\n./bin/main -m ../models/mistral-7b-instruct-v0.1.Q5_K_M.gguf \\\n    -p \"Building a website can be done in 10 simple steps:\\nStep 1:\"\nHere’s a short snippet from my output after running the command:\nBuilding a website can be done in 10 simple steps:  \n\nStep 1: Choose your website builder\nStep 2: Select a template and customize it to suit your needs.\nStep 3: Add content to your website such as text, images, and videos.\nStep 4: Optimize your website for search engines using keywords and meta tags.\nStep 5: Publish your website on the web server or hosting provider.\nStep 6: Promote your website through social media, email marketing, paid advertising, and other channels.\nStep 7: Monitor your website's analytics to see how users interact with it.\nStep 8: Keep your website up-to-date by regularly updating content and fixing any bugs or errors.\nStep 9: Consider adding e-commerce functionality to sell products or services online.\nStep 10: Continuously improve your website's design, usability, and performance to enhance the user experience\nCongratulation! We have now:\n- Downloaded and built llama.cpp.\n- Downloaded a quantized Mistral-v0.1 model.\n- Ran the Mistral model on a sample input.\nEverything so far was done in C++ via the terminal.\nNext, let’s run the Mistral model inside a Jupyter Notebook with the llama.cpp python bindings. This will give us a preview into a fun way of augmenting your work with LLMs: coding alongside an Agent that you can talk to anytime by popping into a code cell."
  },
  {
    "objectID": "blog/posts/fractal-llms/04-llama/index.html#conclusion",
    "href": "blog/posts/fractal-llms/04-llama/index.html#conclusion",
    "title": "Lesson 4: Quantized LLMs with llama.cpp",
    "section": "Conclusion",
    "text": "Conclusion\nThis notebook covered the llama.cpp library and how to use it to run LLMs. We then ran a Mistral-7B-Instruct-v0.1 model with llama.cpp in both C++ and python.\nThe main goal here was to get you familiar with quantized models, which are the ones we’ll eventually be deploy on our local devices."
  }
]