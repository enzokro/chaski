{"title":"Yet Another Stable Diffusion Post","markdown":{"headingText":"Yet Another Stable Diffusion Post","containsRefs":false,"markdown":"\n\n> Diving into the details of Diffusion Models.\n\n- toc:true- branch: master\n- badges: true\n- comments: true\n- categories: [diffusion, deep learning]\n- image: images/dallE_tries_diffusion.png\n\n## Background Info\n\nThis post goes over Diffusion Models and how they work. It is mainly a future repo of personal notes about this topic, based on my own learnings. \n\nAt the same time, the post covers many details and gaps that were either missing or assumed in other diffusion tutorials. To be clear: the content in those other write ups is fantastic, and I would encourage everyone to read them as well. But, while trying to explain this subject to non-AI friends, I realized there are many tricky and unclear parts that are likely hard for an experienced person (or someone who works on this all day) to see. \n\nHopefully this post gives a good introduction and covers those gaps, while being practically relevant and fun. It is to be the blog I wish I had when starting out.  \n\nThe content comes mainly from the following excellent resources:\n- annotated diffusion model\n- Phil Wang’s code implementation\n- Lilian Wang’s post\n- Song’s compendium\n- Karras et. Al, Elucidating the Design Space \n- Understanding Diffusion Models: A Unified Perspective\n\n## Intro  \n\n### The task: what are we trying to do?  \n\nBefore jumping into equations and definitions, we should take a step back and remember what these diffusion models are trying to do. What is the end goal behind all of this compute? \n\nThe goal is to generate new images.  \n\nIn the spirit of artistry, let's say we are \"creating\" new images instead. Consider this example: we want to create a painting of a vase with sunflowers.  \n\nAs a person, it is easy to imagine the process of creating this painting. We would pick up a paintbrush and start painting a vase. Assuming we aren't trained artists, then this first painting will likely be... rough, to put it gently. But, with practice, our paintings would steadily get better. After enough practice and effort, we would be able to create a more than respectable painting of a sunflower vase. \n\nThe sunflower vase creation described above would take a lot of effort and time. But there is a clear path for getting there. The same is true if we'd instead asked for a photo or drawing of the sunflower phase. In that case, instead of practicing painting, we would have taken up photography or illustration, respectively. \n\nIn the real world, we would have likely bought or commissioned this sunflower art from someone else. An artist who has already put in the time and effort to improve their skills. This artist will have already gone through intense practice and effort, saving us the trouble.  \n\nIn any case, whether we chose to create or buy this sunflower art, there is a clear path for bringing it into the world.  \n\n\n### Beyond sunflowers\n\nThe example above described a relatively simple scene: vase + flowers. What if we needed a painting of something more complex? For example a fresco of a field of sunflowers as the dawn sun first cracks the horizon, perhaps with birds in the skies or animals roaming about. \n\nIf we were painting it ourselves, then we'd have to practice for a lot longer to nail this more complex scene and subjects. Or, we'd need to find increasingly specialized artists who can handle the task.  \n\nThis was an elaborate and drawn-out of way of saying something quite obvious: there is a skills and time bottleneck in creating something new. Good old human effort and practice can in practice improve this bottleneck. However, there *must* be an easier way?  \n\nThat is where computers come into play. Computers are incredible at rote, automatic tasks that would otherwise take a lot of time and brainpower from a human. But computers can famously only do exactly what they are told: nothing else and nothing more. How could we possibly get a computer to follow the process above, of human effort, practice, and refinement? In other words, how could a computer learn to paint a sunflower vase?  \n\n\n### Painting vs. Sculpting\n\nDiffusion models are part of a broad family of approaches for creating new images, via a computer. In a very loose analogy, they are trained and refined just like our aspiring painter in the previous section.  \n\nLet's revisit how a human would paint the sunflower vase. Then, we will see how a Diffusion Model would paint the vase.  \n\nA human painter would start with a blank canvas. Then, through practice, the artist learns how to fill this canvas with brush strokes to create our sunflower painting.  \nA Diffusion Model starts with a different kind of canvas. Instead of starting as blank, the Model's canvas is filled with noise. Then, through training, the model learns how to remove and carve out the noise to reveal our sunflower painting underneath.  \n\nTo link the two canvases: imagine if someone threw several buckets of paint at a blank canvas. The buckets themselves are of different sizes, and filled with every kind of color. Imagine someone has been throwing these buckets for a long time at random. Long enough for there to be hundreds or thousands of layers of paint, stacked in every combination of colors. \n\nNow, if an artist stopped by, and we asked them to turn this paint-drenched canvas into a sunflower vase, how would they do it? The easiest way would be to throw on another coat of all-white paint, wait for it to dry, and paint the sunflowers as requested. But, suppose this artist was a masochist (wild, right?), and wanted to do something much harder.  \n\nIf this artist knew the exact order in which the buckets were thrown, and where they were thrown, he could do something else: he could take a scalpel and carefully remove the layers of paint he didn't need.  \n\nHe could initially carve down until the first white coat of painting in every spot. This would give him a blank canvas, if maybe one that's a little uneven. Then, he could continue slicing out small strips of paint until he reached: yellow for the sunflowers, green for the stem, etc. Remember, there many layers of paint of every color. The artist simply has to know how far down to cut to reach his desired color for an area. After a long time of carving, the artist will have uncovered a sunflower painting in his specific vision. \n\nIf the artist follows the example above, he has instead \"sculpted\" the sunflower as opposed to \"painting\" it. This sculpting is exactly what a diffusion model learns to do. It starts with \"noise\" (aka the paint-soaked canvas with multiple layers of every-colored paint), and learns to remove layers bit by bit, in order to reveal the masterpiece underneath. \n\nThe main question now becomes: how does a computer learn to sculpt new images out of noise? Diffusion Models are a formal answer to this question, where the learning can be automated by a computer. \n\n### Aside: Deep Learning  \n\nDeep Learning has enabled computers to do things that were previously impossible (or at least incredibly difficult) for them. Some of these impossible tasks include: finding specific objects in a photo, figuring out the exact words someone spoke, or writing a short story. As people these tasks are relatively easy for us. But to a computer, it is as hard as asking a human to calculate giant matrix multiplications in their heads. One type of knowledge isn't objectively better than the other. The hardware (circuits vs. brains) has simply been specialized to different tasks. \n\n### Formal digital sculpting\n\nThe original approach for this task actually came about in [Sohl, 2015]. That seems like an eternity ago measured by the speed of Deep Learning progress. But, it took more recent work [2019, 2020] and followups since to truly unlock the capabilities of these models.  \n\nTaking a different approach. Start with the forward diffusion process, and change the subscript notation. More “left-to-right to diffuse noise, then undo to generate” More intuitive based on talks with artist friends excited about these models and wanting to know more\n“Drawing denoised samples from a parametrized latent variable” makes sense to stats nerds and math people. “Making an image blurry, then learning how to de-blur it” is understood by normal people. \n\nI’ve never liked how latent folks draw their arrows. Directions feel backwards. Be normal, left-to-right, pointing downwards (generating, from the AI heavens above). The latent is unobservable. We can always look down at the ground, but looking up at the sky burns our eyes. \n\nTheir posts are far more rigorous and better subject mastery\nThis is for my own future reference\nMany typical diffusion details are unclear, or the lead is buried\n\nSmall details and perspective change that feel more natural to me\n\nDefinitions and notations start with backwards first, which seems like a random starting point. Magically sampling out of thin air. Better to first define the forward diffusion process, very intuitive and people easily grok it\nThen, show how we could reverse or un-do this to recover the original image. \nShow how we could use this exact process, but starting from different noise, to create a new image\n\nNot really adding, we are mixing instead. Pure adding would saturate. It is more like smoothing - link to Cold Diffusion which seems to break from notion of “adding Gaussian noise” \nThe nice property\nNotation and constants\nDerivations, what they are doing\n\n\nFocused broadly on score-matching and denoising models, which have shown some equivalences or similarities, and there are useful and important points in both.\n\nScore-matching is often introduced as a floating term. Matching what and why? The true underlying distribution has a value. We want noisy updates to march away from this, and sampling to get progressively closer to the true distribution.\n\nBad log likelihoods explained by increasing focus on imperceptible details. Nature has a sparse basis, can lose many components without affecting fidelity. But, weighting the imperceptible differences tells us the “static” has a high-weight value. What could cause this sharp texture loss?  \n\nSample code with CIFAR-10\nRefactored version of Annotated diffusion for myself \n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"01_diffusionIntroPost.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}