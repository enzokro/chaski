{"title":"Complex Weight Initializations","markdown":{"headingText":"Complex Weight Initializations","containsRefs":false,"markdown":"\n> Building complex-valued layers with Rayleigh initializations.  \n\n- toc: true \n- badges: true\n- comments: true\n- categories: [deep learning, complex networks]\n- image: images/polar_plot.png\n\n# Overview\n\nThis post describes how to create weight initialization for complex-valued neural networks.  \n<br>\nAs a refresher, a complex number has both a real and imaginary components. These two parts define a vector that can be perfectly described by two values: its *magnitude* and its *phase*. \n- **Magnitude**: the length of a vector.\n- **Phase**: the angle, or direction, that the vector points in. \n<br>  \n\nMost neural networks use real-valued numbers instead of complex ones. These real-valued networks include NLP networks like Transformers for text, CNNs in Computer Vision for images, and feed-forward networks for audio signals like speech. When initializing all of these neural networks, each weight only needs a single, real-valued number. The networks for these tasks and domains have been incredibly successful, so why would need complex-valued weights in the first place? \n\nIt turns our that signals are more naturally represented in the complex domain: robotics, Radio Frequency communications, bio-informatics, radar, and even speech. That is not to say we *must* use complex values for these signals. In fact real-valued networks have been used in these domains. The crucial detail is that they are technically throwing away half of the potential input data. Moreover, the phase of signals often contains important information: the phase of an image describes the actual position of the photo's subjects, while the magnitude mainly contains color information. In speech, the phase of a signal is important for how understandable the audio is. In communication, sonar, radar, and robotics signals the phase carries information about both the content and location of signals. \n\nIf we want to leverage the full potential of these complex-valued input signals, we need to match them with complex-valued neural networks. The main difference for initialization is that we need now need two values: one for phase and the other for the magnitude. But we can't just take two regular, real-valued initializations and call it a day. The rest of this post covers the details of how to accurately create the correct type of complex-valued weight initializations.  \n\n\n\n## Specifics of complex-valued initializations. \n\nTo make things more concrete, the magnitudes will be drawn from a specific kinds of distribution called a **[Rayleigh distribution](https://en.wikipedia.org/wiki/Rayleigh_distribution])**. The reasons for this are described later, but for now just think of it as the complex-version of the familiar Random or Normal distributions we use to initialize real-valued weights.  \n\nThe phases will be drawn from a Uniform distribution. If you think about a compass, there are 360 degrees to choose from. We could randomly pick a degree and start walking in that direction. Assuming we are on a mostly flat surface, for each degree we will end up in a different, unique location. Because we don't know which direction our learned complex-valued weights should point in, the best we can do is start randomly pointing everywhere and let the gradients steer the vectors instead.  \n\n# Background on initializations \n\nNow we know how to pick (aka sample) both the magnitude and phase of our complex-valued weights.  \n\nBut! There is another important step. While initializations are now taken for granted, they were one of the first key pieces that made it possible to train deep neural networks in the first place. In the early attempts to train neural networks, before we knew what made for good initializations, the gradients and weight values would either diverge or collapse to 0. This was known as gradient explosion and vanishing, respectively.  \n\nThe main insights about how to prevent gradients from vanishing or exploding came from inspecting their variance during training. (As an aside: this remains an important error analysis tool even today! Looking at the behavior and distribution of gradients is a surefire way to find problems, if there are any, with the training. Especially during the earliest learning steps).    \n\nIt was the following great work by [He](https://arxiv.org/pdf/1502.01852.pdf) and [Glorot, Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) that showed how the variance of the weights must meet certain criteria to ensure that gradients flow smoothly. Here, \"smoothly\" means that during training the gradients neither disappeared (went to 0) nor exploded (grew without bound).  The best-known initializations described in these papers are now the defaults in popular deep learning libraries like TensorFlow and pytorch.  \n\nHowever, the theory of complex-valued neural networks is newer and not as well established. Thankfully, we can borrow the lessons above about good initializations to make sure the gradients of our complex-valued weights are well-behaved and allow the networks to converge and learn.    \n\n# Complex networks\n\nDespite the fact that they are not as popular as real-valued approaches, complex-valued networks actually have a rich, long history. See [Chapter 3 of this thesis](https://www.cs.dartmouth.edu/~trdata/reports/TR2018-859.pdf) for a great recap.  \n\nThe first modern and complete take on deep complex neural nets was [Deep Complex Networks](https://arxiv.org/pdf/1705.09792.pdf) by Trabelsi et. al. This paper explored many fundamental building blocks for deep complex networks. It developed the following pieces: initializations, convolutions, activations, batch normalizations, and stacked them together to build complex Residual Networks (ResNets).  \n\nDespite this fantastic work the field stayed quiet at first. But, there has been a recent spike in activity with follow ups in: medical imaging, radio frequency signal processing, physical optical networks, and even some quantum networks! Some of the applications and advances in this domain are detailed in this more recent, comprehensive [Survey of Complex-Valued Neural Networks](https://arxiv.org/pdf/2101.12249.pdf) by Bassey at. al.  \n<br>  \n\nNow that we know a bit more about complex-valued networks, we are ready to initialize their weights.  \n\n# Rayleigh Distribution  \n\n\n\nTo best describe a Rayleigh distribution, imagine setting up a wind-speed sensor out in an open field. If we analyze the wind passing through this sensor in two directions, say North and East, then the magnitude of the wind's velocity will be Rayleigh distributed.  \n\nMore generally, this distribution happens when two random variables are added together. To be Rayleigh distributed, the random variables must be uncorrelated, normally distributed, have zero mean, and share the same standard deviation.  \n\nAnother example of a Rayleigh distribution is tuning in to a radio signal. Imagine we tune a radio to an empty RF spectrum region where all we hear is noise. If we record the real and imaginary components of this RF noise, then its magnitude will follow a Rayleigh distribution.\n<br>  \n\nWe chose the Rayleigh distribution because, without knowing more information about our complex-valued magnitudes, it is a good unbiased starting point from which the network can then learn better values. \n\n\nLet's dive into the details. The equation below is the Probability Density Function (PDF) of a Rayleigh distribution.\n\n$$f(x,\\sigma) = \\frac{x}{\\sigma^2}e^{-x^2/(2\\sigma^2)}, \\ \\ x \\geq 0$$\n\nIf this equation looks intimidating, we can instead code it up as a python function using the NumPy library to make it much cleaner:\n\nIn the equation and code above sigma ($\\sigma$) is known as the scale parameter. It is common in many probability distributions and usually controls how spread out or narrow a distribution is.\n\nLet's start by setting $\\sigma = 1$ to see the \"basic\" Rayleigh shape. We will then change sigma to see how it affects the distribution.\n\nAs we mentioned the scale, $\\sigma$, changes the width or narrowness of the distribution. Let's both halve and double sigma to ($\\frac{1}{2}, {2})$ respectively to see what happens.\n\nThe blue line in the plot above is the same PDF from our first plot where $\\sigma = 1$.  \n\nWe can see how $\\sigma = 0.5$ pulls the distribution up and to the left, while $\\sigma = 2$ squishes it down and to the right.  \n\nIn other words, a smaller sigma makes our distribution narrower while a larger sigma makes it wider.\n\nPlotting the theoretical Rayleigh PDF only showed us what the distribution *should* looks like. Now, we need to actually generate the Rayleigh values.\n\n# Generating Rayleigh samples\n\nWe will use the [RandomState](https://numpy.org/doc/stable/reference/random/legacy.html?highlight=numpy%20random%20randomstate#numpy.random.RandomState) class in the numpy library to generate Rayleigh samples. RandomState is a helpful class that can sample from just about every known distribution.\n\nFirst we create the RandomState class with the chosen seed of $0$.\n\nThis helpful `RandomState` instance can now directly sample from a Rayleigh distribution. We use the sampling function `RandomState.rayleigh` which accepts two parameters:\n- `scale`: $\\sigma$ with a default value of 1.\n- `size`: the shape of the output array  \n<br>\n\nLet's start by drawing 1,000 Rayleigh samples with $\\sigma = 1$. \n\nHow to check if these samples are actually Rayleigh distributed?\nWe can refer back to our PDF plots at the beginning, which tell us how Rayleigh samples should be \"spread out\". The easiest way to check if these samples are spread out as expected is with a histogram.\n\nThis is an ok start. If we squint, we can somewhat see the PDF outline shape we plotted earlier. But, 1000 samples isn't that much.  \n\nAs we draw more samples the distribution should get even closer to the earlier PDF plots. Let's make sure this happens by now drawing 10,000 samples.\n\nMuch better! Let's compare this histogram against the theoretical Rayleigh PDF.  \n\nNote that we pass `density=True` to the histogram function below to make it approximate the PDF.\n\nA perfect match! \n\nNow that we can generate our initial complex magnitudes, let's move on to the phase.  \n\n# Adding phase information\n\nThe Rayleigh samples from above give us the magnitude, or length, of the complex weights. But that is only one part of a complex number. We are still missing information about the phase, or angle. The phase tells us in which direction a vector is pointing.  \n\nFor our purposes it is enough to use random angles. Why? Many processes such as speech, images, and RF modulations encode information in phase. But we don't know what this pointing should look like beforehand, and we do not want to bias the networks to any particular phase setup. Instead, uniformly picking a starting phase is like starting with many vectors fanned out in all directions. Then, during training, the network will learn how to best both orient and scale the weights for its task. \n\nAdding this random uniform phase is straightforward. We pick uniform samples from $-\\pi$ to $\\pi$ radians which maps to a full loop of the unit circle. We can even reuse the same `RandomState` from before!\n\nWe mentioned earlier that a complex number has a real and imaginary component. But so far we've talked about magnitude and phases instead. How are they related?  \n\nIt turns out we can use the phase together with the magnitude to split our vector into real and imaginary part. We use the cosine of the phase for the real part, and the sine of the phase for the imaginary part. These are two different representations of the same complex number, we don't lose anything going from one to the other or vice-versa.\n\nNow we can check if these complex valued weight vectors are truly pointing in random directions. To do this we can plot the first 500 complex weights in the complex plane.\n\nWe are almost there! To recap, we now have:  \n- Random magnitude initializations drawn from a Rayleigh distribution. \n- Random phase initializations drawn from a Uniform distribution.  \n\nCombined, these give us random complex-valued vectors that are pointing in roughly random directions.  \n\nThere is only one missing piece: making sure that these complex weights are well-behaved during training. \n\n# Matching He and Glorot variance criteria\n\nEven though we now have real and imaginary components, they are not quite good initializations yet. The polar plot above gives some clues as to why (hint: look at the range of magnitudes in the vectors).  \n\nRecall from the earlier section on initializations: the key insight was that the variance of the weights needs to meet certain criteria. This criteria helps the gradients flow well during backpropagation.  \n\nTo be more specific, both the He and Glorot criteria are based on the incoming and outgoing connections of a network layer. The number of connections are typically called `fanIn` and `fanOut`, respectively.  \n\nThe He criteria says that the variance of weights $W$ should be: $$\\text{Var}(W) = \\frac{2}{\\text{fanIn}}$$\n\nThe Glorot criteria says that the variance should be: $$\\text{Var}(W) = \\frac{2}{\\text{fanIn + fanOut}}$$\n\nDeep networks typically have hundreds or thousands of connections. In practice this means that the variance of the weights will be very small. Now we can see why the values in the earlier polar plot are not good: their variance is clearly too large!\n\nSo how can we make sure our Rayleigh magnitudes meet the He and Glorot variance criteria?  \nThe [Complex Neural Nets paper](https://arxiv.org/pdf/1705.09792.pdf) from earlier includes a nice derivation for the variance of a complex Rayleigh distribution: $$\\text{Var}(W) = 2\\sigma^{2}$$   \n\nWe can then set this Rayleigh variance equal to the He and Glorot criteria and solve for sigma.\n\nTo meet the He criteria, sigma should be: $$\\sigma_{\\text{He}} = \\frac{1}{\\sqrt{\\text{fanIn}}}$$ <br>\n\nTo meet the Glorot criteria, sigma should be: $$\\sigma_{\\text{Glorot}} = \\frac{1}{\\sqrt{\\text{fanIn + fanOut}}}$$ <br>\n\nLet's take a step back. In the earlier sections we used a flat vector of complex weights as an example. Tying it to our two concrete examples of wind speed and RF noise, it's as if we took a single series of velocity or noise measurements.  \n\nSince the He and Glorot criteria are defined specifically for network layers, we now switch to a simple one-layer network as an example. Let's arbitrarily choose a layer with 100 inputs and 50 outputs (`fanIn` = 100, `fanOut` = 50).\n\nPlugging these `fanIn` and `fanOut` values into the Rayleigh sigma criteria gives:\n$$\\sigma_{\\text{He}} = \\frac{1}{10}$$\n\n$$\\sigma_{\\text{Glorot}} = \\frac{1}{5\\sqrt{6}}$$\n\nThat's all! Now we can pass either of these sigmas into our `RandomState` and it will draw Rayleigh samples that match the chosen variance criteria. Note that this only applies to the magnitude, we can leave the phase as-is.\n\n# Putting it all together: A complex-valued PyTorch initializer\n\nHere is a recap of the previous sections:\n1. We drew a flat series of Rayleigh magnitudes to learn more about the distribution.\n2. We picked a random phase component, then split the magnitudes into real and imaginary parts.\n3. We saw how to match the He and Glorot variance criteria with Rayleigh samples for a single network layer.  \n\nTo make the above practical and usable, we need to automatically generate complex weigths that:\n- Match the He/Glorot variance criteria\n- Are PyTorch tensors\n- Have the correct shape for the given network layer\n<br>  \n\nWe can put all of the pieces above into a Python function that does this for us.  \n\n> A quick word about `fanIn` and `fanOut`. We saw the simple feed-forward case with in our example for a single network layer. In that case the number of incoming connections was simply `fanIn` and the outgoing connections were `fanOut`.  \n\n> However, the convolutional case is a bit more complicated. A convolutional layer has input and output feature maps which are roughly analogous to input and output units in feed-forward layers. But they also have a kernel size to consider. PyTorch has a nice [convenience function](https://pytorch.org/docs/stable/_modules/torch/nn/init.html#_calculate_fan_in_and_fan_out) that handles this for us. \n\nWe can now refactor the earlier code into a function that automatically creates complex-valued Rayleigh weights given an input PyTorch module.\n\n## Complex initialization for a `nn.Linear` module\n\nLet's check if the weights are correctly distributed. Going back to our Rayleigh introduction, it is the magnitude that should be Rayleigh distributed.\n\nSuccess!\n\n## Testing on a `nn.Conv2d` module\n\nWhat about a convolutional layer? Our main concern here is that both the tensor shape and `fan_in`/`fan_out` are handled correctly.\n\nLet's check if these convolutional weights are still Rayleigh distributed.\n\nAnother success!\n\n# Conclusion\n\nIn this post we created Rayleigh initializations for complex-valued neural networks. We started with an overview of the Rayleigh distribution. Next we used this distribution to create the magnitudes of complex-valued weights. We then added some phase information to randomly orient the vectors. After that, we made sure the weights matched a certain variance criteria to be good initializations.  \n\nFinally, we put all everything together into a python function that returns PyTorch tensors. This initialization function is the first building block of complex-valued neural networks.\n\nPart two will look at another type of complex initialization based on (semi) unitary matrixes. After that we will proceed to build complex convolutions and actvations.\n"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"02_complexRayleighInitPost.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","theme":"cosmo"},"extensions":{"book":{"multiFile":true}}}}}