{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Tips for LLM prompts\"\n",
    "author: \"enzokro\"\n",
    "date: \"04/17/2023\"\n",
    "toc: true \n",
    "badges: true\n",
    "categories: [LLM, deep learning]\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> General tips to get better outputs from ChatGPT."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Tips and tricks for ChatGPT Prompts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is a recap of openai's suggestions (as of writing) for improving ChatGPT's outputs.  \n",
    "\n",
    "There are many teams actively working on improving and deploying new LLMs with useful (powerful) abilities. Their work has produced several papers that show different ways of improving an LLM's output.   \n",
    "\n",
    "The official openAI documentation has the full details and discussion of a few key papers.  \n",
    "\n",
    "However, we can also extract a broad set of suggestions based on what the different proposed improvements share in common. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving outputs\n",
    "\n",
    "- Split large, complex tasks into subtasks\n",
    "    - Structure and isolate the instructions of each subtask\n",
    "- Prompt the model to explain its reasoning(s) before answering\n",
    "- If the output was bad, try making the instructions clearer\n",
    "    - Start with simple and direct language\n",
    "    - Can get more complex as the conversation and context grow\n",
    "- Have the model generate many answers, then ask it to distill them into a single, best answer\n",
    "- If possible, Fine-tune custom models to maximize performance\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generic tips\n",
    "\n",
    "- Explicitly guide the model through the thought process\n",
    "    - Helps it stay focused on sub-tasks and subprocesses\n",
    "- “Let’s think step by step…”\n",
    "    - Works best on logical, mathematical, and reasoning tasks\n",
    "    - Possible leverage for other tasks by breaking them down into “logical” steps\n",
    "- Give the model a few examples of the task you want (Few-Shot)\n",
    "- Split a question into two types of prompts and alternate between the two\n",
    "    - Selection prompt -> find the relevant pieces of into\n",
    "    - Inference prompt -> use the relevant pieces to generate the answer\n",
    "    - Halter prompt -> figure out when the alternating should halt, if possible add a value function to evaluate different prompts\n",
    "- Reduce hallucinations by constraining what the model can say\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## API Tips\n",
    "\n",
    "- Give the model an identity that behaves in a certain way with an explicit intent\n",
    "- Ask to model to answer from the perspective of an expert\n",
    "- Try restating the original “system” message to keep the model on-task.\n",
    "- If the model is getting off-track, try reminding it of the instruction and context at the end of the prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
