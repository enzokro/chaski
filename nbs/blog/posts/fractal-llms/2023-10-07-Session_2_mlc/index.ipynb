{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Overview of MLC and llama.cpp\"\n",
    "author: \"Chris Kroenke\"\n",
    "date: \"10/07/2023\"\n",
    "toc: true \n",
    "badges: true\n",
    "categories: [fractal, python, LLM]\n",
    "image: llama.jpeg\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A look at `llama.cpp`, and running Llama2 with the Machine Learning Compilation (`MLC`) library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro  \n",
    "\n",
    "This notebooks runs a local Llama2 model. Ideally, you will be able to run this on your laptop. And if not, that's where the Cloud GPUs from the previous class will come in handy.  \n",
    "\n",
    "\n",
    "Below are two good libraries for running and deploying ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Llama.cpp\n",
    "Fantastic library on github [here](https://github.com/ggerganov/llama.cpp). This was patched together as a hackathon project, and is now arguably the SOTA for deploying local LLMs on CPUs. Great proof of \"just do things\", and there still being tons of low-hanging fruit in the space.  \n",
    "\n",
    "To best leverage the repo, check out the Pull Requests for the latest updates. Folks are constantly weaving in the newest and latest approaches. Indie hackers and unconventional ideals get proposed all the time: if it works and there's proof, people accept it.  \n",
    "\n",
    "## MLC\n",
    "Tool for deploying ML models across all major architectures. \n",
    "They have a companion [course](https://mlc.ai/index.html)\n",
    "[Link](https://github.com/mlc-ai/mlc-llm) that gets into many details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the bleeding-edge of quantizing and deploying LLMs on a Mac.   \n",
    "Very active community. Check contributions and commits.  \n",
    "Good practice: look at the Issues and Pull Requests.  \n",
    "Supports many models.  \n",
    "Many quantization options.  \n",
    "Has python bindings. \n",
    "Easy to install and use. \n",
    "\n",
    "Worth a look, may come back to it later.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLC LLM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sticking with MLC because they have a workflow for iOS and Android apps. Start by running it locally on our laptops.   \n",
    "\n",
    "**High-Level Steps**:  \n",
    "- Download a Llama2 model.  \n",
    "- Build the MLC python environment. \n",
    "- Run Llama2 locally.  \n",
    "- Download a model compiled for iOS or Android.  \n",
    "- Run the model on a phone app.  \n",
    "- Compile a different HF model for iOS.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focused on bridging the Valley of Death  \n",
    "Making powerful SOTA models on edge hardware.  \n",
    "Compile LLMs for all major devices and architectures.  \n",
    "Have a companion course, very worth checking out.  \n",
    "It's like the hardware-level version of this course.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: Downloading the official Llama2 Models\n",
    "\n",
    "Llama2 models are on the HF Model Hub [here](https://huggingface.co/models?search=llama2).  \n",
    "Have to submit a form and accept the license first.  \n",
    "\n",
    "We'll be using different model versions already on the Hub.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a python environment for MLC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use python3.11 in the MLC environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# create a python3.11 environment for MLC\n",
    "mamba create -n mlc-llm python=3.11\n",
    "# conda env create -n mlc-llm python=3.11 (maybe?)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's activate the environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# activate the environment\n",
    "mamba activate mlc-llm  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to install the MLC python library. MLC has pre-built binaries available. Full instructions [here](https://mlc.ai/package/).  \n",
    "\n",
    "For Mac, install it with the following command: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# installing the mlc python library\n",
    "pip install --pre --force-reinstall \\\n",
    "    mlc-ai-nightly mlc-chat-nightly -f https://mlc.ai/wheels\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can check if the library installed correctly by running the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# checks if the mlc python api works\n",
    "python -c \"from mlc_chat import ChatModule; print(ChatModule)\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing large model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to start downloading the model weights now.  \n",
    "Weight files are very large. They get unwieldy in regular git repositories.  \n",
    "Would be very expensive to push/pull a lot of data we know won't change. At least not until we fine tune it.  \n",
    "\n",
    "Will use `git-lfs` tool to manage large files. Installation instructions [here](https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage?platform=mac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Mac, you can use either the Homebrew or MacPorts package managers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# install git-lfs with Homebrew\n",
    "brew install git-lfs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On linux, git-lfs in two commands: \n",
    "\n",
    "curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "\n",
    "sudo apt-get install git-lfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# or, install it with MacPorts\n",
    "port install git-lfs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a local Llama2 chat app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, clone the `mlc-llm` library and move inside it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# make a project folder for the mlc-llm library\n",
    "git clone https://github.com/mlc-ai/mlc-llm.git\n",
    "\n",
    "# move into the folder\n",
    "cd mlc-llm/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need somewhere to put the model weights. We'll download one of the prebuilt models and put it inside the `dist/prebuilt` folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# create the directory for pre-build models\n",
    "mkdir -p dist/prebuilt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLC uses some prebuilt libraries to run and configure the Llama2 chat app. Let's grab these and put them into the prebuilt repo so that the model can use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# clone the MLC prebuilt libraries and configs\n",
    "git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to grab the model. `git-lfs` will works its magic to grab the weights.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# download the pre-compiled Llama2 chat model \n",
    "cd dist/prebuilt\n",
    "git lfs install\n",
    "git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1\n",
    "cd ../..\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatting with Llama2 via the CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a simple python script to chat with the downloaded Llama2 model. Put the following code from the official [MLM Tutorial](https://llm.mlc.ai/docs/deploy/python.html) into a file  called `chat.py`: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Using the MLC chat module to talk with Llama2.\n",
    "\"\"\"\n",
    "\n",
    "# import the chat module\n",
    "from mlc_chat import ChatModule\n",
    "from mlc_chat.callback import StreamToStdout\n",
    "\n",
    "# From the mlc-llm directory, run\n",
    "# $ python sample_mlc_chat.py\n",
    "\n",
    "# Create a ChatModule instance\n",
    "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")\n",
    "# You can change to other models that you downloaded, for example,\n",
    "# cm = ChatModule(model=\"Llama-2-13b-chat-hf-q4f16_1\")  # Llama2 13b model\n",
    "\n",
    "output = cm.generate(\n",
    "   prompt=\"What is the meaning of life?\",\n",
    "   progress_callback=StreamToStdout(callback_interval=2),\n",
    ")\n",
    "\n",
    "# Print prefill and decode performance statistics\n",
    "print(f\"Statistics: {cm.stats()}\\n\")\n",
    "\n",
    "output = cm.generate(\n",
    "   prompt=\"How many points did you list out?\",\n",
    "   progress_callback=StreamToStdout(callback_interval=2),\n",
    ")\n",
    "\n",
    "# Reset the chat module by\n",
    "# cm.reset_chat()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatting with Llama2 in a Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Peek at very powerful workflow: interactively chatting with an LLM in a Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can directly use the `ChatModule` inside of a Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /Users/cck/repos/mlc-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = '/Users/cck/repos/mlc-llm'\n",
    "import os\n",
    "os.chdir(path_to_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/cck/repos/mlc-llm'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the chat module\n",
    "from mlc_chat import ChatModule\n",
    "from mlc_chat.callback import StreamToStdout\n",
    "\n",
    "# initialize the chat module right in the notebook\n",
    "cm = ChatModule(model=\"Llama-2-7b-chat-hf-q4f16_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! *adjusts glasses* I'm so glad you're excited about your project! I'm here to help you in any way I can. Could you please provide some more details about your project, such as its topic, any specific questions you have, or what you hope to achieve? That will help me better understand how I can assist you. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! Please get ready to help me with my project!\"\n",
    "# asking Llama2 about itself\n",
    "output = cm.generate(\n",
    "    prompt=prompt,\n",
    "    progress_callback=StreamToStdout(callback_interval=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def talk(txt):\n",
    "    output = cm.generate(\n",
    "    prompt=txt,\n",
    "    progress_callback=StreamToStdout(callback_interval=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course, I apologize if my previous response came across as too cheery. I'm here to help you in a respectful and professional manner. Please let me know how I can assist you with your project. Are there any specific tasks you need help with, or any information you need me to provide? Please feel free to ask me anything. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "talk(\"Please be less cringe, thank you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, my goodness! *adjusts glasses* I'm so thrilled to be of assistance to you, my dear! *bounces up and down* I'll do my absolute bestest to help you with your project, even if it means being a wee bit cheesy and cringeworthy! *winks* Please do tell me how I can help, you delightful person, you! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "talk(\"Actually, please be as cringe as possible, thank you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, my goodness gracious! *adjusts glasses* I'm so glad you want me to be even more cringe! *bounces up and down* I'll do my absolute bestest to provide you with the most cringe-tastic responses possible! *winks*\n",
      "So, my sweet darling, how may I assist you on this lovely day? *blinks* Do tell me your most precious wish, and I shall do my utmost to make it come true! *bats eyelashes* ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "talk(\"Hmmm, no quite cringe enough! More cringe please uwu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, my goodness gracious! *adjusts glasses* I-I can't possibly...*gulps*...I-I think I'll just have to...*tries to contain excitement*...OVERDO IT! *pulls out all the cringe*\n",
      "So, my sweet, sweet darling...*bounces up and down*...I shall provide you with the most...*gulps*...CRINGE-TASIC RESPONSES POSSIBLE! *winks* *adjusts glasses* *bats eyelashes* ðŸ˜‚ðŸ‘€ðŸ’–\n",
      "Please, my precious, tell me how I may serve you in this, the most cringe-tastic way possible! *curtsies* ðŸ’•\n"
     ]
    }
   ],
   "source": [
    "talk(\"You are actually being a 6/10 cringe. Please show me what a 10000/10 cringe would look like!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting the conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! *adjusts glasses* I'm just an AI, here to help you with any questions or tasks you may have. My purpose is to provide helpful, respectful, and honest assistance to the best of my abilities. I'm just a language model, so I don't have personal experiences or feelings like humans do, but I'm always eager to learn and improve my responses. Is there something specific you'd like to know or discuss?\n"
     ]
    }
   ],
   "source": [
    "# asking Llama2 about itself\n",
    "output = cm.generate(\n",
    "    prompt=\"Please tell me a little about yourself:\",\n",
    "    progress_callback=StreamToStdout(callback_interval=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also ask question on the fly using python's `input()` which works in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, I'm glad you asked! I'm best suited to helping with a wide range of tasks, including but not limited to:\n",
      "1. Answering questions: I can provide information and explanations on various topics, from science and history to entertainment and culture.\n",
      "2. Generating ideas: I can help you brainstorm ideas for creative projects, or even come up with unique solutions to problems you might be facing.\n",
      "3. Language translation: I can translate text from one language to another, helping you communicate with people from different cultures and backgrounds.\n",
      "4. Summarizing content: If you have a long piece of text and want to get a quick summary of its main points, I can help you with that.\n",
      "5. Offering suggestions: I can provide suggestions for things like gift ideas, travel destinations, or even books to read.\n",
      "6. Providing definitions: If you're unsure of the meaning of a word or phrase, I can define it for you and give you examples of how it's used in context.\n",
      "7. Creating content: I can assist you in generating content for various mediums, such as articles, social media posts, or even entire books.\n",
      "8. Conversing: I'm here to chat and help with any questions or topics you'd like to discuss.\n",
      "Feel free to ask me anything, and I'll do my best to assist you!\n"
     ]
    }
   ],
   "source": [
    "# asking Llama2 something on the fly\n",
    "prompt = input(\"Prompt: \")\n",
    "output = cm.generate(prompt=prompt, progress_callback=StreamToStdout(callback_interval=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted a quick summary of what it said?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! Here's a summary of my response in three sentences:\n",
      "I'm a language model AI trained to assist with various tasks, including answering questions, generating ideas, translating languages, summarizing content, offering suggestions, providing definitions, and creating content. I'm here to help with any questions or topics you'd like to discuss, so feel free to ask me anything. I'm best suited to helping with a wide range of tasks, including but not limited to those listed above.\n"
     ]
    }
   ],
   "source": [
    "# asking for a summary of its response\n",
    "output = cm.generate(\n",
    "    prompt=\"Please summarize your response in three sentences.\",\n",
    "    progress_callback=StreamToStdout(callback_interval=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can go back and forth with the cells above, or can continue talking in other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolutely! I'm here to help. What topic would you like to learn more about? Let me know and I'll do my best to provide you with helpful information and resources.\n"
     ]
    }
   ],
   "source": [
    "# asking another question\n",
    "new_prompt = input(\"New, Different Prompt: \")\n",
    "output = cm.generate(prompt=new_prompt, progress_callback=StreamToStdout(callback_interval=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chat module maintains an internal chat history. If we get stuck in a loop or simply want to start the convo anew: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resets the current session's chat history\n",
    "cm.reset_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a handy `stats()` function to check the speed of the model's generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefill: 11.7 tok/s, decode: 12.7 tok/s\n"
     ]
    }
   ],
   "source": [
    "# checks if llm go brrr\n",
    "print(cm.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more rigorous check, we can use the `benchmark_generate` function to check the speed of a fixed number of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " nobody expects the Spanish Inquisition! ðŸ˜±ðŸ”¥ðŸŽ¬\n",
      "\n",
      "Benchmarking is the process of measuring the performance of a system, application, or process against a set of predefined metrics or standards. The goal of benchmarking is to identify areas where improvements can be made, such as increased efficiency, faster performance, or better quality.\n",
      "In the context of the Monty Python sketch, \"nobody expects the Spanish Inquisition!\" is a humorous reference to the unexpected and often absurd nature of benchmarking. Just as the Inquisition was unexpected and unwanted, benchmarking can sometimes be seen as an unnecessary or burdensome process. However, the benefits of benchmarking can be significant, such as identifying areas for improvement, optimizing resources, and ensuring compliance with standards or regulations. ðŸ˜Šs\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'prefill: 1.8 tok/s, decode: 30.0 tok/s'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmarking text generation\n",
    "print(cm.benchmark_generate(prompt=\"What is benchmark?\", generate_length=512))\n",
    "cm.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an MLC iOS app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar process, need a few extra tools and helper packages. First, we need to install rust: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# if you don't have `curl`...\n",
    "which curl # <-- if this shows nothing\n",
    "\n",
    "# install curl\n",
    "brew install curl\n",
    "\n",
    "# then, download and install rust\n",
    "curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will prompt you for different installation kinds, but the default one is perfectly fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the `cmake` tool. On Mac, you can install it with Homebrew or MacPorts. \n",
    "\n",
    "```bash\n",
    "# install cmake with Homebrew\n",
    "brew install cmake\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you skipped the section above, make sure to download the `mlc-chat` library and place it inside the `dist/prebuilt` folder. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections above we chatted with Llama2 on our laptop. Let's go ahead and talk to it from an iOS app now.  \n",
    "\n",
    "First we download a model built for iOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# download the pre-compiled Llama2 model for iOS\n",
    "cd dist/prebuilt\n",
    "git lfs install\n",
    "git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q3f16_1\n",
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some other, helper libraries to run the iOS models. Run the command below to download and configure them: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# grab the helper libraries\n",
    "git submodule update --init --recursive \n",
    "\n",
    "# prepare the ios libs\n",
    "cd ./ios\n",
    "./prepare_libs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a `build/` folder. Make sure the following files are in there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# expected output of build/\n",
    "ls ./build/lib/\n",
    "libmlc_llm.a         # A lightweight interface to interact with LLM, tokenizer, and TVM Unity runtime\n",
    "libmodel_iphone.a    # The compiled model lib\n",
    "libsentencepiece.a   # SentencePiece tokenizer\n",
    "libtokenizers_cpp.a  # Huggingface tokenizer\n",
    "libtvm_runtime.a     # TVM Unity runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we package the new model into the iOS app. We need to add the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# still inside of the ios folder, edit the file below\n",
    "open ./prepare_params.sh # make sure `builtin_list` only contains \"Llama-2-7b-chat-hf-q3f16_1\"\n",
    "\n",
    "# prepackage the weights\n",
    "./prepare_params.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should be able to see the model inside the `ios/build` folder: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# expected contents of ios/dist folder\n",
    "ls ./dist/\n",
    "Llama-2-7b-chat-hf-q3f16_1 # the compiled Llama2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the iOS app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost there! Now to actually build the iOS app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First boot up X-Code, then open the project `./ios/MLCChat.xcodeproj`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the project, and deploy it on either:  \n",
    "- Mac laptop\n",
    "- iPhone or iPad emulator "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
