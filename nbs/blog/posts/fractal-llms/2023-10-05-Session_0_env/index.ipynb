{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"A Python Environment for LLMs\"\n",
    "author: \"Chris Kroenke\"\n",
    "date: \"10/05/2023\"\n",
    "toc: true \n",
    "badges: true\n",
    "categories: [fractal, python, LLM]\n",
    "image: mamba_logo.webp\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Building an LLM python environment using `mamba` and `pip`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this course is to build an AI Agent by fine-tuning a Large Language Model (LLM) on documents chosen by each student. What exactly do we mean by Agent, and why would we focus on a simple one?  \n",
    "\n",
    "To give some context, there is a lot of talk and hype around Agents at the moment. Folks are looking towards a future where we all have personalized AI assistants, aka Agents, at our fingertips. These Agents will make our lives better both in the day to day and in the long term. They are the AI assistants of Science Fiction made material. Agents like TARS from Interstellar, Jarvis from Iron Man, HAL 9000 from Space Odyssey, Samantha from Her, etc.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For as fast as the progress in AI and LLMs has been, Agents as powerful as those are still a ways off. It's hard if not impossible to predict the exact timelines. Suffice it to say that these Agents won't be here anytime \"soon\". But, barring some force majeure, they *will* exist at some point.   \n",
    "\n",
    "The gap, then, is that folks are talking about (and promising) these advanced Agent capabilities, while on the ground we are still dealing with LLM hallucinations and prohibitive compute requirements.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Where does that leave us? Well, as [a recent announcement from OpenAI](https://twitter.com/gdb/status/1694107518488981868) shows, fine-tuning GPT-3.5 on small, clean datasets can even surpass GPT-4 on certain tasks. *That's* what we are aiming for. In other words, we already have the ability to develop outrageously powerful tools by fine-tuning LLMs on small, clean datasets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So we won't build a simple chatbot, nor will we be build Iron Man's Jarvis. We will land somewhere in the middle. If it helps, try thinking about this simple Agent as an Intelligent Rubber Duck. In case you're not familiar with the concept: a Rubber Duck is anything (actual yellow rubber duckie optional) that you keep around your desk and talk to about your work. It is a physical tool for thought, since it's so often helpful to say out loud the swarm of thoughts in our head.  \n",
    "\n",
    "Our simple Agent will be a Rubber Duck that speaks back at you. When you ask it a question about your work, it will respond given what it knows about the project as a whole. Or, if you are simply verbalizing a thought to untangle it, the Agent will give you some feedback or suggest other approaches. If we can be so bold: our Agent will be a Jarvis-lite, laser-focused on a narrow scope. Then, as both the tools and tech progresses, we'll have everything needed to unlock even more capabilities from our Intelligent Rubber Duck. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Summary: In this course we will fine-tune an LLM on a small, clean dataset of our choosing to build an Intelligent Rubber Duck that can help us work or create better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we need for the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fully use a current, open source LLM, the first thing we need to do is set up a proper `programming environment`. The environment is a computing ecosystem with all the software libraries and packages needed to run an LLM.  \n",
    "\n",
    "Note that setting up this environment is often one of the most time-consuming and challenging tasks when it comes to Machine Learning. There is no silver bullet or universal solution, as you will see by the dozens of tools that folks have come up with to tackle this problem (insert xkcd comic about competing standards).  \n",
    "\n",
    "The main point here is that setting up the environment is often annoying. It can even be straight up painful. It's ok to feel lost or struggle with it. Please take some comfort in the fact that once we have the environment, many of the downstream tasks will feel easy by comparison!  \n",
    "\n",
    "So what makes building this environment so challenging? And why do we need it in the first place? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Silent Failures in AI Models\n",
    "\n",
    "LLMs, and Machine Learning models more generally, often fail in different ways than other, standard software fails. For instance, classic bugs in regular software include: type mismatches, syntax errors, compilation errors, etc. In other words failures that clearly stem from a *wrong* operation (aka a bug) that snuck into the code. We wanted the computer to do `X`, but we told it by accident to do `Y` instead.\n",
    "\n",
    "In contrast, ML models often have \"silent\" failures. There is no syntax or compilation error - the program still runs and completes fine. But, there is *something* wrong in the code: adding where we should have subtracted, grabbing the wrong element from a list, or using the wrong mathematical function. There is no type checker or compiler that would (or even could, for now) catch these errors.  \n",
    "\n",
    "The fix for the silent failures above is clear:  \n",
    "- Carefully inspecting the code.  \n",
    "- Monitoring and validating its outputs.  \n",
    "- Clarity in both the algorithms and models we are using.   \n",
    "\n",
    "There is another, unfortunate kind of silent failure: `version` mismatches. Version failures happen when we use a different version of a programming library than the version originally used by the model. As the software libraries we rely on are frequently updated, both subtle and major changes in their internals can affect the output of a model. These failures are unfortunately immune to our careful logical checks.  \n",
    "\n",
    "Avoiding these silent failures is the main reason for being consistent and disciplined with our model's programming environment. A good environment setup keeps us focused on the important, conceptual part of our model instead of getting bogged down in managing software versions. \n",
    "\n",
    "\n",
    "#### Looking forward with our environment  \n",
    "\n",
    "There is a nice benefit to spending this much time and effort up front on our environment.  \n",
    "\n",
    "We will not only have a specialized environment to run and fine-tune a single LLM. We'll have a springboard and setup to keep up with the state of the art in the field. A setup to bring in groundbreaking improvements as they are released. And to weave in the latest and greatest models. The LLM world is our oyster, and the base environment the grain of sand soon-to-be pearls. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `mamba` package manager will handle the python version. Why Mamba? To start it is way fast and better than Anaconda, and it makes it easier to install OS and system-level packages we need outside of python. \n",
    "\n",
    "\n",
    "We will use `pip` to install the actual python packages. Note that we could use mamba for this as well, but a few of the libraries need custom pip options to install.  \n",
    "\n",
    "> Note: Run `pip install -e .` to install a dynamic version of this package that tracks live code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Installing `mamba` on our computers\n",
    "\n",
    "Follow the steps in the [official install instructions](https://github.com/conda-forge/miniforge#install). \n",
    "\n",
    "\n",
    "## Mac Installation  \n",
    "\n",
    "First find the name of your architecture. We then use it to pick the right install script for each Mac.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "# check your mac's architecture\n",
    "arch=$(uname) \n",
    "echo $arch\n",
    "\n",
    "# download the appropriate installation script\n",
    "curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\n",
    "\n",
    "# run the Mambaforge installer\n",
    "bash Mambaforge-$(uname)-$(uname -m).sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you prefer to download the file directly, grab it from here:  \n",
    "\n",
    "https://github.com/conda-forge/miniforge/releases/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the environment \n",
    "\n",
    "After installing Mamba, head to the Lesson 0 here: `Fractal_LLM_Course/lesson_0/envs`. The `README.md` in that folder has the full instructions to build the mamba environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder has all of the pieces we need to build our LLM environment.  \n",
    "\n",
    "Details:\n",
    "- Mamba package manager to create the base python environment.  \n",
    "- Requirements file to install the needed packages with pip.  \n",
    "\n",
    "\n",
    "The steps below will create an environment called `llm_base`. It will have all the pieces we need to get started. \n",
    "```bash\n",
    "\n",
    "# create the base environment\n",
    "mamba env create -f environment.yml\n",
    "\n",
    "# activate the environment\n",
    "mamba activate llm_base\n",
    "\n",
    "# install the python packages, after activating the env\n",
    "python -m pip install -r requirements.txt  \n",
    "\n",
    "# install the pytorch library\n",
    "python -m pip install -r reqs_torch_cpu.txt\n",
    "```  \n",
    "\n",
    "- MANAGING THE CUDA DRIVERS RESOURCE  \n",
    "\n",
    "The first line installs the \"helper\" libraries that will make our lives easier.  \n",
    "The second line installs the `pytorch` library, which we'll use to load and use the actual LLMs.  \n",
    "\n",
    "> Note: On the cloud, you would install the `reqs_torch.txt` which uses the GPU.  \n",
    "\n",
    "Eventually, to speed up the LLMs, we will also need the following libraries:  \n",
    "```bash\n",
    "# OPTIONAL: install the additional libraries\n",
    "python -m pip install -r reqs_optim.txt\n",
    "```   \n",
    "\n",
    "But these libraries can be tricky to install. Don't worry if you run into issues, we will revist them later.  \n",
    "\n",
    "\n",
    "Resources:  \n",
    "- (Installing `python` on your computer)[https://realpython.com/installing-python]\n",
    "- (Downloading `VSCode` to edit our code)[https://code.visualstudio.com/download]\n",
    "- (Installing `git` to manage our code)[https://git-scm.com/book/en/v2/Getting-Started-Installing-Git]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing NVIDIA Drivers and CUDA Libraries on a fresh Ubuntu 22.04 machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three things we need to install:  \n",
    "- NVIDIA Drivers.  \n",
    "- CUDA Libraries.  \n",
    "- cuDNN Libraries.  \n",
    "\n",
    "## Helper libraries\n",
    "\n",
    "First, some best practice. Make sure to update the Ubuntu package list:\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "```\n",
    "\n",
    "There are two Ubuntu packages that are worth installing:  \n",
    "- software-properties-common\n",
    "- build-essential \n",
    "\n",
    "`software-properties-common` is a set of tools for adding and managing software repositories. It makes our life a bit easier.  \n",
    "\n",
    "`build-essential` contains a list of packages that are essential for building Ubuntu packages. It has software key for development like the GNU Compiler Collection (GCC) and GNU Make. It also has the tools to build and install projects from source (aka straight from the repo's folder). It'll help us build and install repos like `llama.cpp` from source. \n",
    "\n",
    "```bash\n",
    "sudo apt install software-properties-common\n",
    "sudo apt install build-essential\n",
    "```  \n",
    "\n",
    "Then we'll add the graphics drivers PPA and update the package list again.  \n",
    "\n",
    "```bash\n",
    "sudo add-apt-repository ppa:graphics-drivers/ppa\n",
    "sudo apt update\n",
    "```\n",
    "\n",
    "## Install Driver\n",
    "\n",
    "Now we can install the nvidia drivers. As of writing, the `535` version of the driver is stable and supports a good number of GPU cards.  \n",
    "```bash\n",
    "sudo apt install nvidia-driver-535\n",
    "```\n",
    "\n",
    "Make sure to reboot your system:  \n",
    "```bash\n",
    "sudo reboot\n",
    "```\n",
    "\n",
    "After logging back in, we can check that the driver is installed correctly:  \n",
    "```bash\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "## Install CUDA  \n",
    "\n",
    "With the driver working we can now install the CUDA library. The CUDA library has a set of libraries optimized for NVIDIA GPUs. \n",
    "\n",
    "The example below uses a local `.dev` installer, and comes straight from the official [CUDA website](https://developer.nvidia.com/cuda-12-1-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=22.04&target_type=deb_local)\n",
    "wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin\n",
    "sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600\n",
    "wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\n",
    "sudo dpkg -i cuda-repo-ubuntu2204-12-1-local_12.1.0-530.30.02-1_amd64.deb\n",
    "sudo cp /var/cuda-repo-ubuntu2204-12-1-local/cuda-*-keyring.gpg /usr/share/keyrings/\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install cuda\n",
    "\n",
    "Export the following to the `~/.bashrc` file to make sure the CUDA libraries are available in the terminal:  \n",
    "```bash\n",
    "echo 'export PATH=/usr/local/cuda-12.1/bin${PATH:+:${PATH}}' >> ~/.bashrc\n",
    "echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.1/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}' >> ~/.bashrc\n",
    "source ~/.bashrc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This command will take a while to run. Once it's done, reboot one last time just to be to be safe:  \n",
    "```bash\n",
    "sudo reboot\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paperspace@ps3r37lmx:~$ nvcc --version\n",
    "nvcc: NVIDIA (R) Cuda compiler driver\n",
    "Copyright (c) 2005-2023 NVIDIA Corporation\n",
    "Built on Tue_Feb__7_19:32:13_PST_2023\n",
    "Cuda compilation tools, release 12.1, V12.1.66\n",
    "Build cuda_12.1.r12.1/compiler.32415258_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything goes well, we should now see that the CUDA version is `release 12.1`.\n",
    "\n",
    "```bash\n",
    "nvcc --version # should output something like the following:\n",
    "\n",
    "# nvcc: NVIDIA (R) Cuda compiler driver\n",
    "# Copyright (c) 2005-2023 NVIDIA Corporation\n",
    "# Built on Tue_Feb__7_19:32:13_PST_2023\n",
    "# Cuda compilation tools, release 12.1, V12.1.66\n",
    "# Build cuda_12.1.r12.1/compiler.32415258_0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
