{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: enzokro\n",
    "badges: true\n",
    "categories:\n",
    "- classification\n",
    "- regression\n",
    "- MLE\n",
    "- MAP\n",
    "date: 11/10/2022\n",
    "image: bayes_rule2.png\n",
    "jupyter: python3\n",
    "output-file: index.html\n",
    "title: MAP and MLE for hidden observations and classification\n",
    "toc: true\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0de61eb",
   "metadata": {},
   "source": [
    "> Examples of Maximum Likelihood and Maximum A Posteriori."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa01ee",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25eabb",
   "metadata": {},
   "source": [
    "- Both MLE and MAP try to figure out the best approximation of an unseen quantity.\n",
    "- Consider a series of measurements over a noisy channel. We have two things: our observations and some information (or best, compromise guess) about the channel. The goal becomes to find out the original data. \n",
    "\n",
    "$$y = ax + n$$  \n",
    "\n",
    "- Consider a supervised learning problem where we have some training data and test data. In this case, we know the labels of the training data (our observations), and we have the \"true\" data itself (the evidence), our input features. The goal becomes to learn the a, or \"channel\" information. \n",
    "- That way, when we get some new test data (new evidence), we can pass it through our channel to determine what we the observation should be. \n",
    "- If we had enough data, picked the right model, and trained in the correct way, we will have an accurate estimate of the channel A. Whether its accuracy is enough will depend on the specific use case.  \n",
    "\n",
    "$$ f_{\\theta}(x) \\rightarrow y $$ \n",
    "\n",
    "\n",
    "- In both cases $n$ can be seen as the irreducible noise. In the signal example, it captures everything we don't know and can't control: perfect knowledge of the measurement environment, micro and macro electronic faults, heat and electrical noise from circuits, interference from unseen or unknown sources, occlusions, etc. For the supervised learning example, it captures the inherent uncertainty we have by the mere fact that $x$ does not contain all possible examples of the data, recorded until every possible condition.  \n",
    "- The best we can do is find a good set of weights a that best maps the data to labels. Or, make the best and most accurate estimate of the channel to figure out the most likely true measurement from the observations.  \n",
    "- This \"best we can do\" is what both MLE and MAP attempt to do.  \n",
    "\n",
    "- The following examples are for future notes, since there is an \"inversion\" when considering MAP and MLE for classification and regression. In regression, we have the measurements and what to find the inputs/data that most likely caused these measurements. In classification, we have the inputs/data for one set of observations, and want to make sure that we can correctly derive measurements for a new, unseen set of inputs/data.  \n",
    "- If you notice the X and Y swapped places as the quantities of interest.  \n",
    "- However, with some manipulation of Bayes Rule, we can easily handle both cases.  \n",
    "- The key is remembering that we are always after some unknown quantity. The quantity itself will depend on the problem setup and its constraints.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1e9637",
   "metadata": {},
   "source": [
    "# Setting up the examples  \n",
    "- To see how the unseen quantity of interest varies, let's consider two examples.\n",
    "- One for regression, the other for classification.  \n",
    "\n",
    "## Regression - temperature readings from an IoT sensor.  \n",
    "\n",
    "- Imagine we've setup a sensor in a warehouse. This warehouse stores food and must be kept within a certain temperature range to keep the food from spoiling.  \n",
    "- The food is very sensitive, and one of the worst things bacteria-wise is for food to warm up then cool back down. Especially if this happens many times.  \n",
    "- In order to prevent this, the warehouse owner wants measurements taken every 5 minutes.  \n",
    "- If one measurement is above the threshold, a the sensor is flagged for human inspection.   \n",
    "- If the same sensor is above the threshold for a second reading, this raises a system alarm and demands intervention.  - For our purposes, let's consider only one of these sensors.  \n",
    "\n",
    "\n",
    "## Classification - the Iris flower problem  \n",
    "\n",
    "- To keep things simple, let's use the classic Iris flow problem.  \n",
    "- While rich in history, this dataset has been analyzed and worked for all its worth.  \n",
    "- There are no more key Machine Learning insights to gain from it, it is now a toy problem.  \n",
    "- But, it is perfect for visualizations or instructions, since we can focus on aspects other than data cleaning, augmentation, error analysis, etc. In other words, we are sure that the data and its results are on good footing, and can focus our effort on other things. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb474fb9",
   "metadata": {},
   "source": [
    "# Quick recap of Bayes Rule  \n",
    "\n",
    "$$P(\\theta|\\textbf{D}) = P(\\theta ) \\frac{P(\\textbf{D} |\\theta)}{P(\\textbf{D})}$$\n",
    "\n",
    "## Posterior  \n",
    "- The probability we truly care about. What people think we usually have given a problem setup, but this is usually a causal trap because of all our priors and biases.  \n",
    "## Likelihood  \n",
    "- What we usually have, i.e. we start with a certain set of evidence, and think we have a hypothesis about it. \n",
    "## Prior  \n",
    "- Our background information about the hypothesis, and how prevalent it is in the real world.  \n",
    "## Evidence  \n",
    "- The total probability of our evidence.  \n",
    "- This is usually intractable for several reasons. It involves knowing absolutely everything about our given evidence. Both in scope and degrees. For example with a given set of symptoms, we'd have to know not only about every possible symptom we *could* have, but also about all variations of the symptoms we do have. For example I may have a slight cough and a headache on the right side of my head. Not only do we have to know about weaker coughs, or slightly harder coughs that other people may have, but we also have to know about our exact same symptoms, but with headache on the left side instead. This labored exampled is only to highlight that, to truly know $P(x)$, we usually need infinite and complete knowledge about some aspect of our reality. Were it so easy... \n",
    "- Thankfully, many of the quantities we care about don't directly need this value. It is only proportional or related to the posterior/likelihood/priors, and we can hand-wave the evidence away by saying it will be some constant we don't truly care about. We only care about the direct, numerator proportions.  \n",
    "\n",
    "## Bayes Rule with Gaussian Distributions   \n",
    "\n",
    "- The above terms are general and hold for any probability distributions.  \n",
    "- To keep things simple, let's use Bayes Equations with Gaussian distributions.  \n",
    "- The reasons for this will become clear in the MLE and MAP sections. It is mainly a convenience choice from three angles:  \n",
    "- With enough samples, most things in the world can be treated as ~Gaussian distributed.  \n",
    "- Their theory is very well fleshed out, have loads of equations and knowledge to draw from.  \n",
    "- With a few algebraic manipulations, the Gaussian quantities for each Bayes terms are very easy to compute.\n",
    "\n",
    "$$P(x_{i}\\mid y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^{2}}} \\exp \\left(-\\frac{(x_{i} -\\mu_{y})^2}{2\\sigma_y^{2}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78974dcc",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimate  \n",
    "\n",
    "Let's dive into some numbers for our two examples. First we will deal with sensor measurements of regression, then the Iris flower classifications. \n",
    "\n",
    "## Regression with MLE  \n",
    "\n",
    "- We have a set of sensor temperature observations y. \n",
    "- We have some knowledge about the channel a. \n",
    "- For a given y, we want to find the most likely, true temperature x that cause the reading for this y.  \n",
    "\n",
    "## Classification with MLE  \n",
    "\n",
    "- For the training set:\n",
    "- We have a given set of flower feature inputs x.  \n",
    "- We also have the species that each measurement belongs to as labels y.  \n",
    "- We want to find a good mapping $a$ (technically $\\theta$ or $w$) from our features to the species.  \n",
    "\n",
    "- Then, critically, when given a new, unseen set of flower features x (maybe a different plot of land, or the same plot after some time), we want to make sure our mapping can still accurately tell us the flower species.  \n",
    "\n",
    "## MLE Recap  and limitations\n",
    "- We saw two examples: finding out the true measured temperature of our sensor. And finding which class a flower most likely belongs to.  \n",
    "\n",
    "- But in both cases, we were using a limited amount of information. For the temperature, knowing that we are on planet Earth, it makes sense to think the sensor will never read -100 or +100 celsius. Why check those temperatures at all? Further, if we knew that the temperature would be, for example, between 10 and 18 degrees, we could focus on those values to get more accurate readings.  \n",
    "- And for the flowers, we assumed that the each flower had an equal number of examples. That is the ideal case with \"balanced classes\". But it's hardly ever the reality. If we know that one type of flower is vastly more frequent than the other, either because it grows better or because we planted more of them, that should factor into our decision!  \n",
    "\n",
    "- In other words, it helps to bring in our knowledge about the outside world, or about the known limits or characteristics of our problem. While this risks biasing us in one direction vs. all the other, if this previous knowledge, our *priors*, are well-founded and earned, we can gain much by using them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11272994",
   "metadata": {},
   "source": [
    "# Maximum A Posteriori  \n",
    "\n",
    "- MAP builds upon MLE by brining in knowledge about our priors.  \n",
    "- At its basic, we scale the likelihood estimate from the previous section based on our prior knowledge about the unobserved quantities. \n",
    "> When you think hooves, think horses not zebras.  \n",
    "- For our sensor example, we can bring in the knowledge both about the average temperatures inside our warehouse, and even knowledge about the temperature outside in case we ever experience a catastrophic breach.   \n",
    "- For classification, we can bring in knowledge about the frequency of each flower in our training data. If we expect the test data to be a good representation of the data our model will see in the field, this will also give us better performance.  \n",
    "\n",
    "## Regression with MAP  \n",
    "\n",
    "- Let's say we know that the true temperature will almost certainly be between 6 and 18 degrees celsius, with a mean of 12 degrees and a standard deviation of 2 degrees. We could figured this out with a series of accurate thermometers spread throughout the warehouse, recorded at different times of the day over a long period of time. Now we arrive at the key motivation for our sensors! While we could have someone walk around the entire warehouse, checking each thermometer and dealing with any problems, that is a human and labor intensive operation. Ideally we'd need several people to cover the warehouse efficiently. And there are certainly better things for employees to do than walking around starting at thermometers.  \n",
    "- That means that, for a given recorded temperature on the sensor, we know how likely it is to be the true temperature, independent of any other measurement or sensors. That is the $P(\\text{temperature})$.  \n",
    "- Now, when we find the likelihood of our measurement, we can likewise scale it by this prior knowledge. \n",
    "- This also makes our alarm system potentially better: if we see two extreme temperatures back to back, far outside the expected range, it is even more likely that either the sensor is broken or, worse, that there is a problem in the warehouse.  \n",
    "\n",
    "## Classification with MAP  \n",
    "\n",
    "- For classification, we can scale the likelihood by the normalized frequency of each class in the dataset.  \n",
    "- This means we are now taking into account the class balance into our answers.  \n",
    "- For example, if we had a likelihood score that was very high, but its for a class that is exceedingly rare in our dataset, then we should not be so sure that it is actually the rare class.  \n",
    "- In the MLE case, we'd always take the class with the highest likelihood. If, based on our training data, we ended up with a rare class that always has a high likelihood by virtue of its features (either intrinsic or via some peculiarity in the data collection), we'd almost always predict that class. \n",
    "- By scaling our belief with our prior, we can now more accurately gauge the true probability of the class.  \n",
    "\n",
    "## MAP Recap  \n",
    "\n",
    "- MAP is a way to bring in our prior outside knowledge of the world.  \n",
    "- Adding in the prior risks biasing our estimates, especially if they are inaccurate.  \n",
    "- But, if the priors are good approximations or even flat out very accurate, they will help scale our decisions to the truly most probably outcomes, given the entire context of the observed and hidden qualities. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fea6eb",
   "metadata": {},
   "source": [
    "# MLE vs. MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25015094",
   "metadata": {},
   "source": [
    "- MLE only looks at the likelihood function.  \n",
    "- MAP bring in our prior information about the problem.  \n",
    "- In the case where the priors are equally likely, aka uniform distribution, then MAP and MLE.  \n",
    "- So in a way, by doing MLE we are always doing MAP with a maximum entropy distribution (Uniform) over our labels.  \n",
    "- And whenever we do have non-uniform prior knowledge, aka whenever there is even a slight class imbalance in the labels, then we are doing MAP.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac7f85",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b28f970",
   "metadata": {},
   "source": [
    "- This post looked at two examples for MAP and MLE.  \n",
    "- Sensor temperature readings to find the most likely temperature.  \n",
    "- Flower characteristics to find the most likely flower species.  \n",
    "- MAP and MLE can be used for any problem where we have a set of givens and a set of hiddens we want to know more about.  \n",
    "- Depending on the problem details and constraints, the terms in our Bayes Theorem will move around. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735846f",
   "metadata": {},
   "source": [
    "###### References. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0967d8",
   "metadata": {},
   "source": [
    "[Supervised vs. Unsupervised Learning](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
