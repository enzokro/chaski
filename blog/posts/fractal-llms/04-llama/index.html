<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris Kroenke">
<meta name="dcterms.date" content="2023-10-14">

<title>chaski - Lesson 4: Quantized LLMs with llama.cpp</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../favicon.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Rosarivo">
<meta property="og:title" content="chaski - Lesson 4: Quantized LLMs with llama.cpp">
<meta property="og:description" content="">
<meta property="og:image" content="https://enzokro.dev/blog/posts/fractal-llms/04-llama/llama-cpp-logo.png">
<meta property="og:site-name" content="chaski">
<meta property="og:image:height" content="640">
<meta property="og:image:width" content="1280">
<meta name="twitter:title" content="chaski - Lesson 4: Quantized LLMs with llama.cpp">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://enzokro.dev/blog/posts/fractal-llms/04-llama/llama-cpp-logo.png">
<meta name="twitter:image-height" content="640">
<meta name="twitter:image-width" content="1280">
<meta name="twitter:card" content="summary_large_image">
</head><body class="nav-fixed"><header id="custom-site-header" class="custom-nav page-columns page-rows-contents"> 
    <nav class="custom-nav-content">
        <div class="navbar-brand-container">
            <a class="navbar-brand" href="http://enzokro.dev/">
                <span class="navbar-title custom-title">Chaski</span>
            </a>
        </div>
    </nav>
    <div class="custom-nav-sidebar">
        <div id="quarto-search" title="Search"></div>
    </div>
</header>


<link rel="stylesheet" href="../../../../styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">chaski</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/enzokro_" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/enzokro" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">Intro</a>
  <ul class="collapse">
  <li><a href="#quantized-models" id="toc-quantized-models" class="nav-link" data-scroll-target="#quantized-models">Quantized models</a></li>
  </ul></li>
  <li><a href="#overview-of-llama.cpp" id="toc-overview-of-llama.cpp" class="nav-link" data-scroll-target="#overview-of-llama.cpp">Overview of llama.cpp</a></li>
  <li><a href="#running-mistral-v0.1-with-llama.cpp" id="toc-running-mistral-v0.1-with-llama.cpp" class="nav-link" data-scroll-target="#running-mistral-v0.1-with-llama.cpp">Running Mistral-v0.1 with llama.cpp</a>
  <ul class="collapse">
  <li><a href="#building-llama.cpp" id="toc-building-llama.cpp" class="nav-link" data-scroll-target="#building-llama.cpp">Building llama.cpp</a></li>
  <li><a href="#downloading-mistral-v0.1" id="toc-downloading-mistral-v0.1" class="nav-link" data-scroll-target="#downloading-mistral-v0.1">Downloading Mistral-v0.1</a></li>
  <li><a href="#running-the-mistral-model" id="toc-running-the-mistral-model" class="nav-link" data-scroll-target="#running-the-mistral-model">Running the Mistral model</a></li>
  </ul></li>
  <li><a href="#running-mistral-v0.1-with-python" id="toc-running-mistral-v0.1-with-python" class="nav-link" data-scroll-target="#running-mistral-v0.1-with-python">Running Mistral-v0.1 with python</a>
  <ul class="collapse">
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/enzokro/chaski/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Lesson 4: Quantized LLMs with llama.cpp</h1>
  <div class="quarto-categories">
    <div class="quarto-category">fractal</div>
    <div class="quarto-category">python</div>
    <div class="quarto-category">LLM</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris Kroenke </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 14, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<blockquote class="blockquote">
<p>Using llama.cpp to run a quantized Mistral-v0.1 model.</p>
</blockquote>
<section id="intro" class="level1">
<h1>Intro</h1>
<p>Welcome to the fourth lesson of the course. Let’s recap our progress so far:</p>
<ul>
<li>Lesson 1: We made a python environment for LLMs.<br>
</li>
<li>Lesson 2: Set up a personal blog to track our progress.</li>
<li>Lesson 3: Ran our first LLM with the HuggingFace API.</li>
</ul>
<p>In this notebook, we will now run a quantized LLM using the <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> library. We choose the powerful and recently released <a href="https://mistral.ai/news/announcing-mistral-7b/"><code>Mistral-7B-Instruct-v0.1</code></a> model.</p>
<p>Let’s start by looking at how this quantized model is different from the LLMs we ran in the previous lesson.</p>
<section id="quantized-models" class="level2">
<h2 class="anchored" data-anchor-id="quantized-models">Quantized models</h2>
<p>llama.cpp is a library that lets us easily run <em>quantized</em> LLMs. What does it mean for a model to be quantized?</p>
<p>Quantizing a model reduces the amount of memory it needs to run. This means we can fit a previously too-large model on less powerful machines, like a laptop or even a smaller GPU.</p>
<p>To be more specific, quantization reduces the number of bits that represent each of the model’s weights. For example, instead of using floats with 32 bits of precision, we can use 8-bit or 4-bit floats to cut down on the overall memory.</p>
<p>LLMs lose some of their accuracy and power when we quantize the weights. But, the performance drop is more than made up by the ability to run larger models on smaller machines.</p>
</section>
</section>
<section id="overview-of-llama.cpp" class="level1">
<h1>Overview of llama.cpp</h1>
<p>llama.cpp is designed to run quantized LLMs on a Mac. Despite its name, the project supports many other models beyond Llama and Llama-2. There are also <a href="https://github.com/abetlen/llama-cpp-python">python bindings</a> to make our lives even easier.</p>
<p>The picture below comes from project’s README, and shows low-level details about how the repo works and what it supports.</p>
<p><img src="llama_description.png" class="img-fluid"></p>
<p>The original project was hacked together in a <a href="https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022">single evening</a>, and has since become arguably the SOTA for deploying LLMs on CPUs. This is in large part thanks to the dedicated and helpful community behind it.</p>
<p>Below we can see the full list of models that llama.cpp supports as of writing.</p>
<p><img src="llama_model_support.png" class="img-fluid"></p>
<p>The benefits of llama.cpp go beyond its code or models. Folks are always collaborating in <a href="https://github.com/ggerganov/llama.cpp/pulls">Pull Requests</a> to bring in the latest, greatest advances from the flood of LLM progress. Tracking these PRs is a great way of keeping up to date with the field.</p>
<p>The community is also very open to hackers and new ideas: if something works and there’s proof, then it gets merged in.</p>
<p>Next, let’s use llama.cpp to run a quantized <code>Mistral-v0.1</code> model.</p>
</section>
<section id="running-mistral-v0.1-with-llama.cpp" class="level1">
<h1>Running Mistral-v0.1 with llama.cpp</h1>
<p>This section covers the following:<br>
1. Creating a virtual env for llama.cpp<br>
2. Installing the <code>llama.cpp</code> repo<br>
3. Downloading a quantized <code>Mistral-v0.1</code> model<br>
4. Running the model directly with <code>llama.cpp</code><br>
5. Running the model in a Jupyter Notebook</p>
<p>First, we create a mamba environment to keep our work isolated. Then we download and install the repo.</p>
<p>Next we download the actual Mistral model from the HuggingFace Model Hub.</p>
<p>Lastly, we run the Mistral model first with C++ and then in a Jupyter Notebook.</p>
<section id="building-llama.cpp" class="level3">
<h3 class="anchored" data-anchor-id="building-llama.cpp">Building llama.cpp</h3>
<p>Go ahead and create a new python3.11 mamba environment:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create an environment for llama.cpp</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> create <span class="at">-n</span> llama-cpp python=3.11</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This isn’t <em>strictly</em> necessary for llama.cpp since it uses C++, but we will need the env later for the python bindings. And in any case, it’s best practice to keep our projects in isolated environments.</p>
<p>Next, activate the new environment:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># activate the environment</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="ex">mamba</span> activate llama-cpp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We can now clone the repo.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># clone and move into the llama.cpp repo</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/ggerganov/llama.cpp</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> llama.cpp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After cloning, we can move inside and prepare it for the build.</p>
<p>There are two options to build llama.cpp:<br>
- <a href="https://www.gnu.org/software/make/">GNU Make</a><br>
- <a href="https://cmake.org/">CMake</a></p>
<p><code>make</code> works great on Linux, but I’ve had mixed results on Mac. For that reason, we’ll stick with <code>CMake</code> for now.</p>
<p>The best bet is grabbing the CMake installer from the <a href="https://cmake.org/download/">official site</a>. Download the appropriate one for your system and install it.</p>
<p>With CMake installed, we can now follow a standard build process. This might look familiar if you’ve installed other software libraries from source.</p>
<p>Start by creating and moving into the special <code>build/</code> folder:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a build directory and move into it</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> build</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> build</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then run the <code>cmake</code> command that prepares the build. Here is where we can also specify other, special build options. For example, on Mac we can pass the <code>LLAMA_METAL=1</code> flag to use the GPU, or on Linux we can pass the <code>LLAMA_CUBLAS=1</code> flag to use an NVIDIA GPU.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare the llama.cpp build with Mac hardware acceleration</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cmake</span> <span class="at">-DLLAMA_METAL</span><span class="op">=</span>1 ..</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># # or, use the line below on Linux/Window to build for NVIDIA GPUs</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># cmake -DLLAMA_CUBLAS=1 ..</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>With the setup files ready we can now build the project:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># build the accelerated llama.cpp project</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="fu">cmake</span> <span class="at">--build</span> . <span class="at">--config</span> Release </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once the build finishes, the binaries to run models will be inside of the <code>build/bin</code> folder. Inside this folder, the executable binary called <code>main</code> is how we’ll be calling llama.cpp to run LLMs.</p>
<p>Now, we are ready to download the Mistral model.</p>
</section>
<section id="downloading-mistral-v0.1" class="level2">
<h2 class="anchored" data-anchor-id="downloading-mistral-v0.1">Downloading Mistral-v0.1</h2>
<p>We will download the <code>Mistral-7B-Instruct-v0.1</code> model. What exactly does it do? We can find out by breaking down the name a bit:<br>
- <code>Mistral</code> is the name given by the developers, in this case the <a href="https://mistral.ai/">Mistral.ai team</a><br>
- <code>7B</code> means that the model has 7 billion parameters<br>
- <code>Instruct</code> means that it was trained to follow and complete user instructions<br>
- <code>v0.1</code> is the release version for this model</p>
<p>Follow the link below to see the model on the HuggingFace Model Hub.</p>
<blockquote class="blockquote">
<p>Download link for <a href="https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF"><code>Mistral-7B-Instruct-v0.1</code></a></p>
</blockquote>
<p>Once on the page, click on the <code>Files and version</code> tab near the top. Here you’ll see a big list of different quantized models.</p>
<p><img src="mistral_quantized.png" class="img-fluid"></p>
<p>The files shown here are variants of the same, base Mistral model that were quantized in different ways.</p>
<p>The names can be overwhelming. Let’s break them down a bit.</p>
<p>You’ll notice that each file ends with a format like this: <code>Q*_*.gguf</code>.</p>
<p>For example one model from the list is: <code>mistral-7B-Instruct-v0.1.Q4_K_S.gguf</code>. We already covered the first part of the name above.</p>
<p>The <code>Q4</code> part means that the model was quantized with 4-bits. The <code>K_S</code> part refers to the specific flavor of quantization that was used.</p>
<p>There is an unfortunate tradeoff between quantization and performance. The fewer bits we use, the smaller and faster the model will be at the code of performance. And the more bits we use, the better its performance but the slower the model.</p>
<p>In general, the <code>Q4</code> and <code>Q5</code> models offer a good balance between speed, performance, and size.</p>
<p>Let’s get back to running the model.</p>
<p>We choose the <code>Q5_K_M</code> model. It is not much larger than the <code>Q4</code> models and has a performance better enough to make it worthwhile.</p>
<p>Before grabbing this model, make sure the huggingface-hub CLI is installed. This will let us download models easily in the terminal.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install a tool to download HuggingFace models via the terminal</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install huggingface-hub</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Then move into the <code>models/</code> folder inside of the llama.cpp repo, and download the our chosen model as follows:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># download the Mistral Q5_K_M model</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="ex">huggingface-cli</span> download TheBloke/Mistral-7B-Instruct-v0.1-GGUF <span class="dt">\</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    mistral-7b-instruct-v0.1.Q5_K_M.gguf <span class="dt">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">--local-dir</span> . <span class="dt">\</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">--local-dir-use-symlinks</span> False</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Once the model is downloaded, we’ll use the binary created by the build process to run it.</p>
</section>
<section id="running-the-mistral-model" class="level2">
<h2 class="anchored" data-anchor-id="running-the-mistral-model">Running the Mistral model</h2>
<p>We’ll use the <code>main</code> binary inside of the <code>build/</code> folder from before to run the <code>Q5_K_M</code> model.</p>
<p>Run the following command to see the Mistral LLM in action! Here we prompt it to tell us how to build a website in 10 steps.</p>
<p>The <code>-m</code> flag points to the model weight we downloaded into the <code>models/</code> folder.</p>
<p>The <code>-p</code> flag is the prompt for the model to follow.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># from build/, run the official example to see Mistral-v0.1 in action</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="ex">./bin/main</span> <span class="at">-m</span> ../models/mistral-7b-instruct-v0.1.Q5_K_M.gguf <span class="dt">\</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">-p</span> <span class="st">"Building a website can be done in 10 simple steps:\nStep 1:"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Here’s a short snippet from my output after running the command:</p>
<pre><code>Building a website can be done in 10 simple steps:  

Step 1: Choose your website builder
Step 2: Select a template and customize it to suit your needs.
Step 3: Add content to your website such as text, images, and videos.
Step 4: Optimize your website for search engines using keywords and meta tags.
Step 5: Publish your website on the web server or hosting provider.
Step 6: Promote your website through social media, email marketing, paid advertising, and other channels.
Step 7: Monitor your website's analytics to see how users interact with it.
Step 8: Keep your website up-to-date by regularly updating content and fixing any bugs or errors.
Step 9: Consider adding e-commerce functionality to sell products or services online.
Step 10: Continuously improve your website's design, usability, and performance to enhance the user experience</code></pre>
<p>Congratulation! We have now:<br>
- Downloaded and built llama.cpp.<br>
- Downloaded a quantized Mistral-v0.1 model.<br>
- Ran the Mistral model on a sample input.</p>
<p>Everything so far was done in C++ via the terminal.</p>
<p>Next, let’s run the Mistral model inside a Jupyter Notebook with the llama.cpp python bindings. This will give us a preview into a fun way of augmenting your work with LLMs: coding alongside an Agent that you can talk to anytime by popping into a code cell.</p>
</section>
</section>
<section id="running-mistral-v0.1-with-python" class="level1">
<h1>Running Mistral-v0.1 with python</h1>
<p>Make sure to install the llama.cpp python bindings inside of the mamba virtual environment we created earlier.</p>
<p>The pair of pip commands below will install the bindings.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install the python bindings with Metal acceleration</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> uninstall llama-cpp-python <span class="at">-y</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="va">CMAKE_ARGS</span><span class="op">=</span><span class="st">"-DLLAMA_METAL=on"</span> <span class="ex">pip</span> install <span class="at">-U</span> llama-cpp-python <span class="at">--no-cache-dir</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The two commands above:<br>
- First uninstall older versions of the bindings, if any are found.<br>
- Then, it installs the bindings with Metal (Mac GPU) acceleration.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Make sure to change the <code>CMAKE_ARGS</code> to CUDA if running on Linux.</p>
</div>
</div>
<p>After installing the bindings, run the following code snippet in the notebook. This will tell us if the bindings are installed correctly.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check if we can import the llama.cpp python bindings </span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> llama_cpp <span class="im">import</span> Llama</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If the command above works, we can now run the Mistral-v0.1 model inside a Jupyter Notebook!</p>
<p>We can instantiate a <code>Llama</code> model object and point it to the weights we downloaded earlier. Make sure to change the paths to match your own.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># point the Llama class to the model weights we downloaded in the previous sections</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>work_dir <span class="op">=</span> <span class="st">"/Users/cck/repos/llama.cpp/"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>llm <span class="op">=</span> Llama(<span class="ss">f"</span><span class="sc">{</span>work_dir<span class="sc">}</span><span class="ss">/models/mistral-7b-instruct-v0.1.Q5_K_M.gguf"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s prompt it again to give us 10 steps for building a website.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># asking Mistral for help building a website</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">"Building a website can be done in 10 simple steps:</span><span class="ch">\n</span><span class="st">Step 1:"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> llm(prompt, max_tokens<span class="op">=</span><span class="dv">512</span>, echo<span class="op">=</span><span class="va">True</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s see what it said!</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-overflow-wrap code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># viewing the in-notebook Mistral generation</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>output[<span class="st">'choices'</span>][<span class="dv">0</span>][<span class="st">'text'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>"Building a website can be done in 10 simple steps:\nStep 1: Plan the website: Determine what you want to achieve with your website, who your audience is, and how much content you need.\nStep 2: Choose a platform: There are many platforms available for building a website, such as WordPress, Wix, Squarespace, or Weebly. Choose one that suits your needs and budget.\nStep 3: Choose a domain name: Your domain name should be easy to remember and relevant to your business or brand. Register it with a domain registrar such as GoDaddy or Namecheap.\nStep 4: Design your website: Create a layout and choose colors, fonts, and images that align with your brand and appeal to your audience. Use a template or hire a web designer if needed.\nStep 5: Add content: Write and upload the text, images, and videos that will make up your website's content. Keep it easy to read and navigate, with clear calls to action.\nStep 6: Optimize for search engines: Use keywords, meta descriptions, and alt tags to improve your website's visibility on search engines such as Google.\nStep 7: Add functionality: Install plugins or add code to your website to add features such as a contact form, e-commerce cart, or social media integration.\nStep 8: Test and launch: Test your website for any errors or bugs before launching it to the public. Make sure it works on all devices and browsers.\nStep 9: Promote your website: Use social media, email marketing, and other channels to drive traffic to your website and attract new visitors.\nStep 10: Monitor and update: Keep track of your website's performance with analytics tools such as Google Analytics. Make updates and improvements as needed to keep your website relevant and engaging for your audience."</code></pre>
</div>
</div>
<p>Congrats! We’ve now ran the <code>Mistral-7B-Instruct-v0.1</code> model with llama.cpp in both C++ and python.</p>
<p>The C++ version is ideal for a server or production application. And as for python version, we can now bootup a handy LLM assistant inside a Jupyter Notebook, and ask it questions as we code or develop.</p>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This notebook covered the llama.cpp library and how to use it to run LLMs. We then ran a <code>Mistral-7B-Instruct-v0.1</code> model with llama.cpp in both C++ and python.</p>
<p>The main goal here was to get you familiar with quantized models, which are the ones we’ll eventually be deploy on our local devices.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="enzokro/chaski_comments" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>