{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Lesson 4: Quantized LLMs with llama.cpp\"\n",
    "author: \"Chris Kroenke\"\n",
    "date: \"10/14/2023\"\n",
    "toc: true \n",
    "badges: true\n",
    "categories: [fractal, python, LLM]\n",
    "image: llama-cpp-logo.png\n",
    "jupyter: python3\n",
    "include-after-body:\n",
    "    text: <script type=\"text/javascript\" src=\"interactive.js\"></script>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using llama.cpp to run a quantized Gemma model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the fourth lesson. Here is what we have done so far:  \n",
    "\n",
    "- Lesson 1: Created a python environment for LLMs.  \n",
    "- Lesson 2: Set up a personal blog to track our progress.\n",
    "- Lesson 3: Ran our first LLM using the HuggingFace API.  \n",
    "\n",
    "In this notebook, we will run an LLM using the [`llama.cpp`](https://github.com/ggerganov/llama.cpp) library. We'll deploy a version of the powerful, recently released [`Gemma`](https://blog.google/technology/developers/gemma-open-models/) model.  \n",
    "\n",
    "llama.cpp is a library that lets us easily run *quantized* LLMs. What does it mean for a model to be quantized?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing a model reduces the amount of memory it takes up. This lets us run previously too-large models on less powerful hardware, like a laptop or a small, consumer GPU. \n",
    "\n",
    "Quantization works by reducing the number of bits that represent a model's weights. For example, instead of using floats with 32 bits of precision, we can use 8-bit or 4-bit floats to reduce the memory footprint by quite a bit.  \n",
    "\n",
    "LLMs do lose some accuracy and power when their weights are quantized. But, the drop in performance is more than made up for by the ability to run larger models on smaller machines. It's better to run *something*, than to not run anything at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "llama.cpp is a library focused on running quantized LLMs on Mac computers. Despite its name, the project supports many other models beyond Llama and Llama-2. It also has a set of [python bindings](https://github.com/abetlen/llama-cpp-python) to make our lives easier.  \n",
    "\n",
    "The picture below from project's README shows the low-level details about how the repo works and what it supports. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](llama_description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original project was hacked together in a [single evening](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022), and has since become arguably the SOTA for deploying LLMs on CPUs. This is in largely thanks to the helpful and dedicated community behind it.  \n",
    "\n",
    "Below we can see the full list of models that llama.cpp supports as of writing.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](llama_model_support.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benefits of llama.cpp stretch beyond its code and models. Folks are always collaborating in [Pull Requests](https://github.com/ggerganov/llama.cpp/pulls) to bring in the latest, greatest advances from the flood of LLM progress. In fact, tracking these PRs is a great way of keeping up to date with the larger LLM field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::callout-note\n",
    "The proof of the community's power is in the this notebook. Originally, it used a Mistral model. And even though Gemma was released very recently, there was a PR to run it with llama.cpp within a few days of release.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The community is also open to hackers and new approaches: if there is proof than an idea works, then it gets merged in.  \n",
    "\n",
    "Next, let's use llama.cpp to run a quantized `Gemma` model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gemma with llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section covers the following:  \n",
    "1. Create a virtual env for llama.cpp  \n",
    "2. Install the `llama.cpp` repo  \n",
    "3. Download a quantized `Gemma` model   \n",
    "4. Run the model directly with `llama.cpp`    \n",
    "5. Run the model in a Jupyter Notebook  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a mamba environment to keep our work isolated. Then we download and install the llama.cpp repo. Next we download the actual `gemma-7b-it` model from the HuggingFace Model Hub. Lastly, we run the `gemma-7b-it` model first with C++ and then in a Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by creating a new mamba environment for llama.cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# create an environment for llama.cpp\n",
    "mamba create -y -n llama-cpp python=3.11\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't *strictly* necessary for llama.cpp since it uses C++, but we will need the env later for the python bindings. And in any case, it's a good idea to keep our projects in isolated environments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, activate this new environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# activate the environment\n",
    "mamba activate llama-cpp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now clone the repo and move into the directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# clone and move into the llama.cpp repo\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cloning, we can move inside and prepare it for the build.  \n",
    "\n",
    "There are two options to build llama.cpp:  \n",
    "- [GNU Make](https://www.gnu.org/software/make/)  \n",
    "- [CMake](https://cmake.org/)    \n",
    "\n",
    "Previously I had some issues with `make` on Mac. However, again thanks to the great llama.cpp community, the issues have been fixed. Go ahead and build the llama.cpp project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# from the main llama.cpp directory, build it\n",
    "make\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! llama.cpp is now installed. We can now grab the quantized Gemma model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the [`gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) model. What exactly does it do? We can find out by breaking down the name a bit:  \n",
    "\n",
    "- `gemma` is the name given by the developers.  \n",
    "- `7B` means that the model has 7 billion parameters.     \n",
    "- `it` means that it was trained to follow and complete user instructions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link above takes you to the official, original model page on HuggingFace. I went ahead and quantized and uploaded a few models, as part of the lesson (check the Appendix for more details).\n",
    "\n",
    "> [My quantized Gemma models](https://huggingface.co/enzokro/gemma-7b-it-gguf/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once on the page, click on the `Files` tab near the top. Here you'll see a big list of different quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](enzokro_gemma_quants.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files shown here are variants of the same, base `gemma-7b-it` model that were quantized in different ways. Why are there two different files, and how are they different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking down the quantized gemma names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see how each quantized gemma model ends with a format like: `Q*_*.gguf`.  \n",
    "\n",
    "For example the first file above is: *gemma-7b-it-**Q4_K_M.gguf***. We already covered what the first part of the name means.\n",
    "\n",
    "The `Q4` part tells us that the model was quantized with 4-bits. The `K_M` part refers to the specific flavor of quantization that was used.  \n",
    "\n",
    "There is an unfortunate tradeoff between quantization and performance. The fewer bits we use, the smaller and faster the model will be at the code of performance. And the more bits we use, the better its performance but the slower and larger the model.  \n",
    "\n",
    "In general, the `Q4` and `Q5` models offer a good balance between speed, performance, and size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we stick with the `Q5_K_M` model. It is not much larger than the `Q4` model, and has a performance better enough to make it worthwhile. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before grabbing this model, we need to install the `huggingface-hub` command-line interface (CLI) is installed. This tool will let us point to and download any model on the HuggingFace hub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloaidng a quantized Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "# install a tool to download HuggingFace models via the terminal\n",
    "pip install huggingface-hub\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then move into `models/`  and create a `gemma-7b-it` folder. Having one folder for each family of models keeps the folder from getting cluttered. Once inside, download the Q5_K_M model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# from the llama.cpp directory, move into the models directory\n",
    "cd models\n",
    "\n",
    "# create and move into the gemma-7b-it directory\n",
    "mkdir gemma-7b-it\n",
    "cd gemma-7b-it\n",
    "\n",
    "# download the quantized gemma-7b-it model\n",
    "huggingface-cli download enzokro/gemma-7b-it-gguf \\\n",
    "    gemma-7b-it-Q5_K_M.gguf \\\n",
    "    --local-dir . \\\n",
    "    --local-dir-use-symlinks False\n",
    "\n",
    "    \n",
    "## other sample model downloads:\n",
    "\n",
    "# # download a quantized Mistral-7B model\n",
    "# huggingface-cli download TheBloke/Mistral-7B-Instruct-v0.2-GGUF \\\n",
    "#     mistral-7b-instruct-v0.2.Q4_K_S.gguf \\\n",
    "#     --local-dir . \\\n",
    "#     --local-dir-use-symlinks False\n",
    "\n",
    "# # download a Llama-2 model for chatting\n",
    "# huggingface-cli download TheBloke/Llama-2-7B-Chat-GGUF \\\n",
    "#     llama-2-7b-chat.Q4_K_M.gguf \\\n",
    "#     --local-dir . \\\n",
    "#     --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is downloaded, we can run it with the `main` binary from the llama.cpp repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::callout-note\n",
    "The binaries are created as part of the build process - make sure you've ran those before going forward.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Gemma model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get right to it. Start with the command below, which will immediately prompt and run the quantized gemma model. We're asking it to give us 10 simple steps for building a website. Then, we'll breakdown exactly what the command is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# run a simple example to see quantized Gemma in action\n",
    "./main -m models/gemma-7b-it/gemma-7b-it-Q5_K_M.gguf \\\n",
    "    -p \"Please provide 10 simple steps for building a website.\" \\\n",
    "    --in-prefix \"<start_of_turn>user\\n\" \\\n",
    "    --in-suffix \"<end_of_turn>\\n<start_of_turn>model\\n\" \\\n",
    "    -e \\\n",
    "    --temp 0 \\\n",
    "    --repeat-penalty 1.0 \\\n",
    "    --no-penalize-nl\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `-m` flag points to the quantized model weight we downloaded into the `models/gemma-7b-it` folder.   \n",
    "\n",
    "The `-p` flag is the prompt for the model to follow.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where things get interesting. Remember that Gemma was trained to follow instructions from a user. During training, it was shown a pair of `<PROMPT>` and `<RESPONSE>` strings. The `<PROMPT>` string is the user's input, and the `<RESPONSE>` string is the model's output.  \n",
    "\n",
    "That means the model expects to see the same `<PROMPT> | <RESPONSE>` format in its input. This is very similar to the text preprocessing steps we covered in Lesson 3. Making sure we follow the model's expected prompt format is the last text preprocessing step we *must* do.  \n",
    "\n",
    "Without this prompt structure, the model will technically still run. But we'd be throwing away a huge portion of the model's power.  \n",
    "\n",
    "For the Gemma models, the `<PROMPT>` must begin with the phrase `<start_of_turn>user\\n`. This tells the model to prepare for an instruction. And the `<RESPONSE>` string must begin with the phrase `<end_of_turn>\\n<start_of_turn>model\\n`. This tells the model to start following the request. \n",
    "\n",
    "We specify the first string by passing in the `--in-prefix` argument. And we specify the second by passing in the `--in-suffix` argument. Our actual prompt will be sandwiched between these two strings:  \n",
    "\n",
    "```\n",
    "<start_of_turn>user\n",
    "|| INSTRUCTION_HERE ||\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last three arguments are specific to the instruction model. They control the temperature, the repeat penalty, and the penalty for newlines. \n",
    "\n",
    "```\n",
    "--temp 0\n",
    "--repeat-penalty 1.0\n",
    "--no-penalize-nl\n",
    "```\n",
    "\n",
    "\n",
    "Think of them as sprinkles on top to get better model outputs. And for reference, they were suggested by [Georgi Gerganov](https://huggingface.co/google/gemma-7b-it/discussions/38#65d7b14adb51f7c160769fa1), the main author of llama.cpp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemma model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a the output I got after running the command:\n",
    "\n",
    "> Please provide 10 simple steps for building a website.\n",
    ">  \n",
    "> **1. Choose a Domain Name:** Select a domain name that is relevant to your website's content and easy to remember.  \n",
    "> **2. Choose a Web Host:** Select a web hosting service that meets your needs in terms of bandwidth, storage space, and reliability.  \n",
    "> **3. Select a Content Management System (CMS):** Choose a CMS platform that offers the features you need to manage your website content easily.  \n",
    "> **4. Design Your Website:** Create a visually appealing website design that is responsive to different devices.  \n",
    "> **5. Add Content:** Populate your website with high-quality content that is relevant to your target audience.  \n",
    "> **6. Optimize for Search Engines:** Use SEO techniques to optimize your website for search engines to improve its visibility.  \n",
    "> **7. Create a Mobile-Friendly Version:** Ensure your website is responsive and optimized for mobile devices.  \n",
    "> **8. Promote Your Website:** Use social media and other marketing channels to promote your website.  \n",
    "> **9. Track and Analyze:** Monitor your website's performance and analyze user behavior to identify areas for improvement.  \n",
    "> **10. Continuously Update:** Keep your website up-to-date with new content, updates, and security patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! We have now:  \n",
    "\n",
    "- Downloaded and built llama.cpp.   \n",
    "- Downloaded a quantized Gemma model.   \n",
    "- Ran the Gemma model on a sample input.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything so far was done in C++ via the terminal. \n",
    "\n",
    "Next, let's run the Gemma model inside a Jupyter Notebook with the llama.cpp python bindings. This will give us a preview into a fun way of augmenting your work with LLMs: coding alongside an Agent that you can talk to anytime by popping into a code cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Gemma with python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to install the llama.cpp python bindings inside of the mamba virtual environment we created earlier.  \n",
    "\n",
    "The pair of pip commands below will install the bindings.  \n",
    "\n",
    "```bash\n",
    "# install the python bindings with Metal acceleration\n",
    "pip uninstall llama-cpp-python -y\n",
    "CMAKE_ARGS=\"-DLLAMA_METAL=on\" pip install -U llama-cpp-python --no-cache-dir\n",
    "```  \n",
    "\n",
    "The two commands above:  \n",
    "- First uninstall older versions of the bindings, if any are found.    \n",
    "- Then, it installs the bindings with Metal (Mac GPU) acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::: callout-note\n",
    "Make sure to change the `CMAKE_ARGS` to CUDA if running on Linux.\n",
    "::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, let's install the ipykernel package. This will let us run the llama.cpp environment inside of a Jupyter Notebook.  \n",
    "\n",
    "```bash\n",
    "# install the ipykernel package to run in notebooks\n",
    "pip install ipykernel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After installing the bindings, run the following code snippet in the notebook. This will tell us if the bindings are installed correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if we can import the llama.cpp python bindings \n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the command above works, we can now run the Gemma model inside a Jupyter Notebook!   \n",
    "\n",
    "We can instantiate a `Llama` model object and point it to the weights we downloaded earlier. Make sure to change the paths to match your own. Here we point to the Q4 model to try something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output: false\n",
    "# point the Llama class to the model weights we downloaded in the previous sections\n",
    "work_dir = \"/Users/cck/repos/llama.cpp/\"\n",
    "llm = Llama(f\"{work_dir}/models/gemma-7b-it/gemma-7b-it-Q4_K_M.gguf\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prompt it again to give us 10 steps for building a website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|output: false\n",
    "# asking Gemma for help building a website\n",
    "prefix = \"<start_of_turn>user\" \n",
    "suffix = \"<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "prompt = \"Please provide 10 simple steps for building a website.\"\n",
    "\n",
    "full_prompt = f'''<start_of_turn>user\n",
    "{prompt}\n",
    "<end_of_turn>\n",
    "<start_of_turn>model\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the llm from the notebook\n",
    "output = llm(\n",
    "    full_prompt, \n",
    "    max_tokens=1000,\n",
    "    temperature=0.0,\n",
    "    repeat_penalty=1.0,\n",
    "    echo=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look inside `output` to see what the model said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-overflow: wrap \n",
    "# viewing the in-notebook gemma-7b-it generation\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! We've now ran the `gemma-7b-it` model with llama.cpp in both C++ and python. \n",
    "\n",
    "The C++ version is ideal for a server or production application. And as for python version, we can now bootup a handy LLM assistant inside a Jupyter Notebook, and ask it questions as we code or develop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covered the llama.cpp library and how to use it to run LLMs. We then ran a `gemma-7b-it` model with llama.cpp in both C++ and python.  \n",
    "\n",
    "The main goal here was to get you familiar with quantized models, which are the ones we'll eventually be deploy on our local devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 1: Quantizing the gemma-7b-it model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model we used in this notebook was already quantized. I quantized the official, full GGUF model from Google and uploaded it to my own HuggingFace account.  \n",
    "\n",
    "The steps below are how I quantized this model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we move into the `models/` folder and download the original model. I create a special `hf_models` folder to keep any models that came from the huggingface hub. Start from the base llama directory.\n",
    "\n",
    "```bash\n",
    "# move into the models/ and create a folder for huggingface models\n",
    "cd models/\n",
    "mkdir hf_models\n",
    "cd hf_models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# download the original model\n",
    "huggingface-cli download google/gemma-7b-it \\\n",
    "    gemma-7b-it.gguf \\\n",
    "    --local-dir . \\\n",
    "    --local-dir-use-symlinks False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this model, we can run llama.cpp's `quantize` command to compress it. I put this into a proper `gemma-7b-it` folder inside `models/` to keep things organized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# create a folder for gemma-7b-it\n",
    "mkdir models/gemma-7b-it\n",
    "```\n",
    "\n",
    "```bash\n",
    "# quantize the original model to the Q4_K_M format\n",
    "./quantize models/hf_models/gemma-7b-it.gguf models/gemma-7b-it/gemma-7b-it-Q5_K_M.gguf Q5_K_M\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
